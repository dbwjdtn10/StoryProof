"""
ì„ë² ë”© ë° ê²€ìƒ‰ ì—”ì§„ ëª¨ë“ˆ
BAAI/bge-m3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± ë° Pinecone ë²¡í„° ê²€ìƒ‰
"""

from typing import List, Dict

from backend.core.config import settings
from backend.db.session import SessionLocal
from backend.db.models import VectorDocument


class EmbeddingSearchEngine:
    """ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ (Pinecone ì—°ë™)"""
    
    def __init__(self):
        """
        BAAI/bge-m3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±
        Pineconeì„ ë²¡í„° ì €ì¥ì†Œë¡œ ì‚¬ìš©
        """
        try:
            from sentence_transformers import SentenceTransformer
            from pinecone import Pinecone
        except ImportError:
            raise ImportError("sentence-transformers, pinecone í•„ìš”: pip install sentence-transformers pinecone")
        
        print("ğŸ”„ BAAI/bge-m3 ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = SentenceTransformer('BAAI/bge-m3')
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # Pinecone ì´ˆê¸°í™”
        self.pc = Pinecone(api_key=settings.PINECONE_API_KEY)
        self.index_name = settings.PINECONE_INDEX_NAME
        
        # ì¸ë±ìŠ¤ í™•ì¸
        if self.index_name not in [idx.name for idx in self.pc.list_indexes()]:
            print(f"âš ï¸ Pinecone ì¸ë±ìŠ¤ '{self.index_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")
        
        self.index = self.pc.Index(self.index_name)
        print(f"âœ… Pinecone ì¸ë±ìŠ¤ ì—°ê²°: {self.index_name}")
    
    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        embedding = self.model.encode(text, normalize_embeddings=True)
        return embedding.tolist()
    
    def add_documents(self, documents: List[Dict], novel_id: int):
        """ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•˜ì—¬ Pineconeê³¼ DBì— ì €ì¥"""
        print(f"\nğŸ“¥ {len(documents)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        
        db = SessionLocal()
        vectors_to_upsert = []
        
        try:
            for i, doc in enumerate(documents):
                # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ìƒì„± (ìš”ì•½ + ì›ë¬¸ ì¼ë¶€)
                search_text = f"{doc.get('summary', '')} {doc.get('original_text', '')[:1000]}"
                
                # ì„ë² ë”© ìƒì„±
                embedding = self.embed_text(search_text)
                
                # ê³ ìœ  ID ìƒì„±
                vector_id = f"novel_{novel_id}_scene_{doc['scene_index']}"
                
                # Pinecone ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata = {
                    'novel_id': novel_id,
                    'scene_index': doc['scene_index'],
                    'summary': doc.get('summary', '')[:200],
                }
                
                vectors_to_upsert.append({
                    'id': vector_id,
                    'values': embedding,
                    'metadata': metadata
                })
                
                # DBì— ìƒì„¸ ì •ë³´ ì €ì¥ (VectorDocument)
                existing_doc = db.query(VectorDocument).filter(
                    VectorDocument.vector_id == vector_id
                ).first()
                
                if existing_doc:
                    existing_doc.chunk_text = doc.get('original_text', '')
                    existing_doc.metadata_json = doc
                else:
                    new_doc = VectorDocument(
                        novel_id=novel_id,
                        chapter_id=None,
                        vector_id=vector_id,
                        chunk_index=doc['scene_index'],
                        chunk_text=doc.get('original_text', ''),
                        metadata_json=doc
                    )
                    db.add(new_doc)
                
                if (i + 1) % 10 == 0:
                    print(f"  ì§„í–‰: {i + 1}/{len(documents)}")
            
            # Pinecone ì—…ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬)
            batch_size = 100
            for i in range(0, len(vectors_to_upsert), batch_size):
                batch = vectors_to_upsert[i:i + batch_size]
                self.index.upsert(vectors=batch)
                
            db.commit()
            print("âœ… Pinecone ì—…ë¡œë“œ ë° DB ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            db.rollback()
            print(f"âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e
        finally:
            db.close()
    
    def search(self, query: str, novel_id: int = None, top_k: int = 5) -> List[Dict]:
        """ì†Œì„¤ ë‚´ì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì”¬ ê²€ìƒ‰"""
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embed_text(query)
        
        # Pinecone ì¿¼ë¦¬ í•„í„°
        filter_dict = {}
        if novel_id:
            filter_dict['novel_id'] = novel_id
        
        # Pinecone ê²€ìƒ‰
        results = self.index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            filter=filter_dict if filter_dict else None
        )
        
        # ê²°ê³¼ ë§¤í•‘
        hits = []
        db = SessionLocal()
        try:
            for match in results.matches:
                vector_id = match.id
                score = match.score
                
                # DBì—ì„œ ì›ë³¸ ë°ì´í„° ì¡°íšŒ
                doc = db.query(VectorDocument).filter(
                    VectorDocument.vector_id == vector_id
                ).first()
                
                if doc:
                    scene_data = doc.metadata_json
                    hits.append({
                        'document': scene_data,
                        'similarity': score,
                        'vector_id': vector_id
                    })
                else:
                    # DBì— ì—†ì„ ê²½ìš° Pinecone ë©”íƒ€ë°ì´í„° ì‚¬ìš©
                    print(f"âš ï¸ DBì—ì„œ ë¬¸ì„œ {vector_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    hits.append({
                        'document': {
                            'scene_index': match.metadata.get('scene_index'),
                            'summary': match.metadata.get('summary'),
                            'characters': [],
                            'locations': [],
                            'original_text': f"[Warning: DB Sync Error]\n{match.metadata.get('summary', 'ë‚´ìš© ì—†ìŒ')}"
                        },
                        'similarity': score,
                        'vector_id': vector_id
                    })
        finally:
            db.close()
        
        return hits
