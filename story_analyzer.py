"""
ìŠ¤í† ë¦¬ ë¶„ì„ ì‹œìŠ¤í…œ
ì”¬ ì²­í‚¹ â†’ LLM êµ¬ì¡°í™” â†’ ì„ë² ë”© ê²€ìƒ‰ (Pinecone) â†’ ì‚¬ì „ ê¸°ëŠ¥ (PostgreSQL)
"""

import os
import json
import re
import uuid
import sys
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
import numpy as np

# Add parent directory to path if needed (e.g. running as script)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Backend Imports
from backend.core.config import settings
from backend.db.session import SessionLocal
from backend.db.models import Novel, Analysis, VectorDocument, AnalysisType, AnalysisStatus, User

# ============================================================================
# 1. ê¸°ì¡´ Parent Chunking í´ë˜ìŠ¤ë“¤ (ê·¸ëŒ€ë¡œ ìœ ì§€)
# ============================================================================

class DocumentLoader:
    """ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
    
    @staticmethod
    def load_txt(file_path: str) -> str:
        """TXT íŒŒì¼ ë¡œë“œ (ìë™ ì¸ì½”ë”© ê°ì§€)"""
        try:
            import chardet
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                result = chardet.detect(raw_data)
                detected_encoding = result['encoding']
                confidence = result['confidence']
                
                if confidence > 0.7 and detected_encoding:
                    try:
                        text = raw_data.decode(detected_encoding)
                        print(f"[OK] íŒŒì¼ ë¡œë“œ: {detected_encoding} (ì‹ ë¢°ë„: {confidence:.2f})")
                        return text
                    except Exception:
                        pass
        except ImportError:
            pass
        
        encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-16', 'latin-1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding, errors='strict') as f:
                    text = f.read()
                    print(f"[OK] íŒŒì¼ ë¡œë“œ: {encoding}")
                    return text
            except (UnicodeDecodeError, UnicodeError, LookupError):
                continue
        
        raise UnicodeDecodeError(
            'unknown', b'', 0, 1,
            f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸ì½”ë”©: {encodings}"
        )


class SceneChunker:
    """ì”¬ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• """
    
    LOCATION_KEYWORDS = [
        # ğŸ  ì‹¤ë‚´ / ì£¼ê±°
        'ë°©', 'ì§‘', 'ê±°ì‹¤', 'ì¹¨ì‹¤', 'ë¶€ì—Œ', 'ì£¼ë°©', 'ìš•ì‹¤', 'í™”ì¥ì‹¤',
        'í˜„ê´€', 'ë‹¤ë½', 'ì§€í•˜ì‹¤', 'ë² ë€ë‹¤', 'ë§ˆë‹¹', 'ì˜¥ìƒ',

        # ğŸ¢ ê±´ë¬¼ / ì‹œì„¤
        'ê±´ë¬¼', 'ì‚¬ë¬´ì‹¤', 'íšŒì‚¬', 'íšŒì˜ì‹¤', 'ê°•ë‹¹', 'ì—°êµ¬ì‹¤',
        'ë³‘ì›', 'ì‘ê¸‰ì‹¤', 'ìˆ˜ìˆ ì‹¤', 'ì•½êµ­',
        'í•™êµ', 'êµì‹¤', 'êµì •', 'ë„ì„œê´€',
        'ê²½ì°°ì„œ', 'ë²•ì›', 'ê°ì˜¥', 'êµ¬ì¹˜ì†Œ',
        'ì€í–‰', 'ìš°ì²´êµ­',

        # ğŸ½ ìƒì—… / ê³µê³µ ê³µê°„
        'ì¹´í˜', 'ì‹ë‹¹', 'ìˆ ì§‘', 'ë°”', 'í¬ì¥ë§ˆì°¨',
        'ìƒì ', 'ê°€ê²Œ', 'ì‹œì¥', 'ë§ˆíŠ¸', 'ë°±í™”ì ',
        'í˜¸í…”', 'ëª¨í…”', 'ì—¬ê´€', 'ìˆ™ì†Œ', 'ë¡œë¹„',

        # ğŸš‰ êµí†µ / ì´ë™
        'ê±°ë¦¬', 'ê³¨ëª©', 'ë„ë¡œ', 'êµì°¨ë¡œ',
        'ì—­', 'ì§€í•˜ì² ì—­', 'ì •ë¥˜ì¥',
        'ê³µí•­', 'í„°ë¯¸ë„', 'í•­êµ¬', 'ë¶€ë‘',
        'ì°¨ ì•ˆ', 'ì—´ì°¨ ì•ˆ', 'ë²„ìŠ¤ ì•ˆ',

        # ğŸŒ† ì§€ì—­ / í–‰ì • ë‹¨ìœ„
        'ë§ˆì„', 'ë™ë„¤', 'ë„ì‹œ', 'ì‹œë‚´', 'ì™¸ê³½',
        'ì§€ì—­', 'êµ¬ì—­', 'ì§€êµ¬',

        # ğŸŒ² ìì—° / ì•¼ì™¸
        'ê³µì›', 'ê´‘ì¥',
        'ìˆ²', 'ì‚°', 'ì–¸ë•', 'ê³„ê³¡',
        'ê°•', 'í˜¸ìˆ˜', 'ë°”ë‹¤', 'í•´ë³€',
        'ë“¤íŒ', 'ì´ˆì›', 'ì‚¬ë§‰', 'ë™êµ´',

        # ğŸ° ì„œì‚¬ / ì¥ë¥´ íŠ¹í™” (íŒíƒ€ì§€Â·ì‚¬ê·¹Â·ë¬´í˜‘)
        'ì„±', 'ì„±ë²½', 'ì„±ë¬¸', 'ê¶', 'ê¶ì „', 'ì™•ê¶',
        'íƒ‘', 'ì‹ ì „', 'ì‚¬ì›', 'ì œë‹¨',
        'ë§ˆë²•ì§„', 'ë˜ì „', 'ìœ ì ',
        'ë¬´ë¤', 'ë¬˜ì§€', 'íí—ˆ',
        'ê°ì”', 'ì£¼ë§‰', 'ì„œì›',
        'ì „ì¥', 'ì§„ì˜', 'ì•¼ì˜ì§€',

        # ğŸŒŒ ì¶”ìƒì Â·ê²½ê³„ ê³µê°„ (ì˜ë¯¸ ì „í™˜ìš©)
        'ì•ˆ', 'ë°–', 'ë‚´ë¶€', 'ì™¸ë¶€',
        'ê·¼ì²˜', 'ë§ì€í¸', 'ì €í¸', 'ê±´ë„ˆí¸'
]
    
    TIME_TRANSITIONS = [
        'ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ê·¸ í›„', 'ì´íŠ¿ë‚ ', 'ë©°ì¹  í›„',
        'ë‹¤ìŒ', 'ê·¸ë‚ ', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ìƒˆë²½', 'ì˜¤í›„',
        'í•œì°¸ í›„', 'ê³§', 'ì´ìœ½ê³ ', 'ê·¸ëŸ¬ì', 'ìˆœê°„'
    ]
    
    # ì±•í„° íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    CHAPTER_PATTERNS = [
        r'^\s*ì œ\s*\d+\s*[ì¥í™”íšŒ]',      # ì œ1ì¥, ì œ 1 í™”
        r'^\s*Chapter\s*\d+',          # Chapter 1
        r'^\s*\d+\.\s+',               # 1. ì œëª©
        r'^\s*í”„ë¡¤ë¡œê·¸',                # í”„ë¡¤ë¡œê·¸
        r'^\s*ì—í•„ë¡œê·¸',                # ì—í•„ë¡œê·¸
        r'^\s*Prologue',
        r'^\s*Epilogue',
        r'^\s*Open\s*$'                # Open (ê°€ë” ì‚¬ìš©ë¨)
    ]
    
    def __init__(self, threshold: int = 8, min_scene_sentences: int = 3, max_scene_sentences: int = 90):
        # ê¸°ë³¸ ì„ê³„ê°’ (ìë™ ê°ì§€ì— ì‹¤íŒ¨í–ˆì„ ë•Œì˜ ì•ˆì „ ì¥ì¹˜)
        self.default_threshold = threshold
        self.current_threshold = threshold
        self.mode = "scene" # 'scene' or 'chapter'
        self.min_scene_sentences = min_scene_sentences  # ìµœì†Œ ì”¬ ê¸¸ì´
        self.max_scene_sentences = max_scene_sentences  # ìµœëŒ€ ì”¬ ê¸¸ì´ (ì²­í¬ë‹¹ ì•½ 3,000ê¸€ì ëª©í‘œ)
        self.target_chunk_size = 3000  # ëª©í‘œ ì²­í¬ í¬ê¸° (ê¸€ì ìˆ˜)
    
    def contains_new_location(self, sentence: str) -> bool:
        return any(loc in sentence for loc in self.LOCATION_KEYWORDS)

    def is_chapter_header(self, sentence: str) -> bool:
        """ë¬¸ì¥ì´ ì±•í„° í—¤ë”ì¸ì§€ í™•ì¸"""
        sentence = sentence.strip()
        if len(sentence) > 60: 
            return False
            
        for pattern in self.CHAPTER_PATTERNS:
            if re.search(pattern, sentence, re.IGNORECASE):
                return True
        return False
        
    def detect_structure(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ëª¨ë“œ ê²°ì •"""
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì±•í„° í—¤ë” íŒ¨í„´ì´ ëª‡ ë²ˆì´ë‚˜ ë‚˜ì˜¤ëŠ”ì§€ ìƒ˜í”Œë§
        
        matches = 0
        lines = text.split('\n')
        sample_lines = lines[:3000] 
        
        for line in sample_lines:
            if self.is_chapter_header(line):
                matches += 1
        
        # ëª…í™•í•œ ì±•í„° êµ¬ì¡° ê°ì§€ (2ê°œ ì´ìƒ ì°¾ì•˜ì„ ë•Œë§Œ)
        if matches >= 2:
            print(f"ğŸ’¡ ëª…í™•í•œ ì±•í„° êµ¬ì¡° ê°ì§€ë¨ ({matches}ê°œ í—¤ë”). ì±•í„° ê¸°ë°˜ ë¶„í• ì„ ì ìš©í•©ë‹ˆë‹¤.")
            return "chapter"
        else:
            # êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ëŠ” ë™ì  ì„ê³„ê°’ìœ¼ë¡œ ê· í˜•ì¡íŒ ì²­í‚¹
            print(f"ğŸ’¡ êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ ê°ì§€. ë™ì  ì„ê³„ê°’ìœ¼ë¡œ ê· í˜•ì¡íŒ ì²­í‚¹ì„ ì ìš©í•©ë‹ˆë‹¤.")
            return "hybrid"  # ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ
    
    def calculate_dynamic_threshold(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ íŠ¹ì„±ì— ë”°ë¼ ë™ì  ì„ê³„ê°’ ê³„ì‚°
        ëª©í‘œ: ì†Œì„¤ í¬ê¸°ì™€ ê´€ê³„ì—†ì´ ê· í˜•ì¡íŒ ì²­í‚¹ ìœ ì§€ (ì²­í¬ë‹¹ ì•½ 700-800ê¸€ì)
        
        ì‹¤í—˜ ê²°ê³¼:
        - ì„ê³„ê°’ 15: ì²­í¬ë‹¹ ~550ê¸€ì (ì•¨ë¦¬ìŠ¤/ì§€í‚¬ ëª¨ë‘ ê¸°ì¤€)
        - ì„ê³„ê°’ 20: ì²­í¬ë‹¹ ~730-740ê¸€ì âœ“ (ê¶Œì¥)
        - ì„ê³„ê°’ 25: ì²­í¬ë‹¹ ~900-920ê¸€ì (í° ì†Œì„¤ìš©)
        
        ë¬¸ì¥ ê¸¸ì´ì™€ í…ìŠ¤íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ì¤‘ê°„ê°’ì„ ì°¾ì•„ ì ìš©
        """
        import statistics
        
        # ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
        sentences = re.split(r'([.!?]\s+)', text)
        merged_sentences = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                merged_sentences.append(sentences[i] + sentences[i + 1])
            else:
                merged_sentences.append(sentences[i])
        
        merged_sentences = [s.strip() for s in merged_sentences if s.strip()]
        
        # ê° ë¬¸ì¥ì˜ ê¸¸ì´ ì¸¡ì •
        sentence_lengths = [len(s) for s in merged_sentences]
        
        if not sentence_lengths:
            return 18  # ê¸°ë³¸ê°’ (ê· í˜•ì¡íŒ ì¤‘ê°„ê°’)
        
        # í†µê³„ ê³„ì‚°
        avg_sentence_length = statistics.mean(sentence_lengths)
        median_sentence_length = statistics.median(sentence_lengths)
        
        if median_sentence_length == 0:
            return 18
        
        # ì¤‘ê°„ê°’ ê¸°ì¤€ìœ¼ë¡œ ì„ê³„ê°’ ë™ì  ì¡°ì • (3,000ê¸€ì ëª©í‘œ)
        # ë¬¸ì¥ì´ ì§§ì„ìˆ˜ë¡ â†’ ì„ê³„ê°’ ë†’ê²Œ (ë” ë§ì€ ë¬¸ì¥ì„ ëª¨ìŒ)
        # ë¬¸ì¥ì´ ê¸¸ìˆ˜ë¡ â†’ ì„ê³„ê°’ ë‚®ê²Œ (ë” ì ê²Œ ëª¨ìŒ)
        
        # ëª©í‘œ: ì²­í¬ë‹¹ 3,000ê¸€ì
        # í•„ìš”í•œ ë¬¸ì¥ ê°œìˆ˜ ê³„ì‚°
        target_chunk_size = 3000
        needed_sentences = max(40, target_chunk_size / (median_sentence_length or 1))
        
        # ì„ê³„ê°’ì€ ë¬¸ì¥ ê°œìˆ˜ì— ë”°ë¼ ì„¤ì •
        # ë†’ì€ ë°°ìˆ˜ë¥¼ ì ìš©í•´ì„œ ëœ ë¶„í• ë˜ë„ë¡ í•¨ (ë” í° ì²­í¬)
        calculated_threshold = max(225, int(needed_sentences * 2.4))
        
        # ë²”ìœ„ ì œí•œ (225~340)
        calculated_threshold = min(calculated_threshold, 340)
        
        print(f"  ğŸ“Š ë™ì  ì„ê³„ê°’ ê³„ì‚° (3,000ê¸€ì ëª©í‘œ):")
        print(f"     - í‰ê·  ë¬¸ì¥ ê¸¸ì´: {avg_sentence_length:.0f}ê¸€ì")
        print(f"     - ì¤‘ì•™ê°’ ë¬¸ì¥ ê¸¸ì´: {median_sentence_length:.0f}ê¸€ì")
        print(f"     - í•„ìš” ë¬¸ì¥ ê°œìˆ˜: {needed_sentences:.0f}ê°œ")
        print(f"     - ì ìš© ì„ê³„ê°’: {calculated_threshold}")
        
        return calculated_threshold
    
    def split_into_scenes(self, text: str) -> List[str]:
        # 1. êµ¬ì¡° ê°ì§€ ë° ëª¨ë“œ ì„¤ì •
        self.mode = self.detect_structure(text)
        
        if self.mode == "chapter":
            # ì±•í„° ëª¨ë“œ: ì±•í„° í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ë˜, max_scene_sentencesë¡œ ì œí•œ
            self.current_threshold = 1000
        elif self.mode == "hybrid":
            # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: ë™ì  ì„ê³„ê°’ìœ¼ë¡œ ê· í˜•ì¡íŒ ì²­í‚¹
            self.current_threshold = self.calculate_dynamic_threshold(text)
        else:
            # ê¸°ë³¸ ì”¬ ëª¨ë“œ
            self.current_threshold = self.default_threshold
            
        sentences = re.split(r'([.!?]\s+)', text)
        
        merged_sentences = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):

                merged_sentences.append(sentences[i] + sentences[i + 1])
            else:
                merged_sentences.append(sentences[i])
        
        scenes = []
        current_scene = []
        score = 0
        sentence_count = 0
        prev_was_dialogue = False
        
        for sent in merged_sentences:
            if not sent.strip():
                continue
            
            # [ìˆ˜ì •] ì¤„ë°”ê¿ˆ ì •ê·œí™” (2ê°œ ì´ìƒì˜ ì¤„ë°”ê¿ˆì„ ë‹¨ì¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³µë°± ì œê±°)
            sent = re.sub(r'\n{2,}', '\n', sent)
            
            # [ê³µí†µ] ì±•í„°/í—¤ë” ê°ì§€ (ê°•ì œ ë¶„í• )
            if self.is_chapter_header(sent):
                if current_scene and sentence_count >= self.min_scene_sentences:
                    scenes.append(" ".join(current_scene))
                    current_scene = []
                    sentence_count = 0
                current_scene.append(sent)
                score = 0 
                continue

            # [ì ìˆ˜ ê³„ì‚°]
            # ëª…í™•í•œ ì”¬ êµ¬ë¶„ì
            if "***" in sent or "---" in sent or "###" in sent:
                score += 12
            
            # ì—°ì†ëœ ì¤„ë°”ê¿ˆ (ë¬¸ë‹¨ êµ¬ë¶„)
            if "\n\n" in sent or sent.count('\n') >= 2:
                score += 5
            
            # ì¥ì†Œ ë³€í™”
            if self.contains_new_location(sent):
                score += 4
            
            # ì‹œê°„ ì „í™˜
            if any(word in sent for word in self.TIME_TRANSITIONS):
                score += 3
            
            # ëŒ€í™” ì „í™˜ ê°ì§€ (ì¸ìš©ë¶€í˜¸ë¡œ ì‹œì‘)
            is_dialogue = sent.strip().startswith('"') or sent.strip().startswith("'")
            if is_dialogue != prev_was_dialogue and sentence_count > 0:
                score += 2
            prev_was_dialogue = is_dialogue
            
            # ëŒ€í™” ì¢…ë£Œ í›„ ì§€ë¬¸
            if re.search(r'["\']\s*[.!?]\s+[^"\']+', sent):
                score += 2
            
            current_scene.append(sent)
            sentence_count += 1
            
            # ë¶„í•  ì¡°ê±´:
            # 1. ì ìˆ˜ê°€ ì„ê³„ê°’ ë„ë‹¬ AND ìµœì†Œ ê¸¸ì´ ë§Œì¡±
            # 2. ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼
            should_split = False
            
            if score >= self.current_threshold and sentence_count >= self.min_scene_sentences:
                should_split = True
            
            if sentence_count >= self.max_scene_sentences:
                should_split = True
            
            if should_split:
                scenes.append(" ".join(current_scene))
                current_scene = []
                score = 0
                sentence_count = 0
                prev_was_dialogue = False
        
        # ë§ˆì§€ë§‰ ì”¬ ì²˜ë¦¬ (ìµœì†Œ ê¸¸ì´ ì²´í¬)
        if current_scene:
            if sentence_count >= self.min_scene_sentences or not scenes:
                scenes.append(" ".join(current_scene))
            else:
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ì´ì „ ì”¬ì— ë³‘í•©
                if scenes:
                    scenes[-1] += " " + " ".join(current_scene)
                else:
                    scenes.append(" ".join(current_scene))
        
        print(f"âœ‚ï¸ ì´ {len(scenes)}ê°œì˜ ì”¬ìœ¼ë¡œ ë¶„í• ë¨ (ëª¨ë“œ: {self.mode})")
        return scenes


# ============================================================================
# 2. LLM êµ¬ì¡°í™” ì‹œìŠ¤í…œ (Gemini)
# ============================================================================

@dataclass
class Character:
    """ì¸ë¬¼ ì •ë³´"""
    name: str
    aliases: List[str]  # ë³„ì¹­, ë‹¤ë¥¸ í˜¸ì¹­
    description: str
    first_appearance: int  # ì”¬ ë²ˆí˜¸
    traits: List[str]  # ì„±ê²©, íŠ¹ì§•


@dataclass
class Item:
    """ì•„ì´í…œ/ì†Œí’ˆ ì •ë³´"""
    name: str
    description: str
    first_appearance: int
    significance: str  # ì¤‘ìš”ë„/ì—­í• 


@dataclass
class Location:
    """ì¥ì†Œ ì •ë³´"""
    name: str
    description: str
    scenes: List[int]  # ë“±ì¥í•œ ì”¬ ë²ˆí˜¸ë“¤


@dataclass
class Event:
    """ì‚¬ê±´/ì´ë²¤íŠ¸ ì •ë³´"""
    summary: str
    scene_index: int
    characters_involved: List[str]
    significance: str


@dataclass
class StructuredScene:
    """êµ¬ì¡°í™”ëœ ì”¬"""
    scene_index: int
    original_text: str
    summary: str
    characters: List[str]
    locations: List[str]
    items: List[str]
    key_events: List[str]
    mood: str  # ë¶„ìœ„ê¸°
    time_period: Optional[str]  # ì‹œê°„ëŒ€


class GeminiStructurer:
    """Geminië¥¼ ì‚¬ìš©í•œ ì”¬ êµ¬ì¡°í™”"""
    
    def __init__(self, api_key: str):
        try:
            from google import genai
            from google.api_core import retry
        except ImportError:
            # ì„¤ì¹˜í•´ì•¼ í•  íŒ¨í‚¤ì§€ ì´ë¦„ë„ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤ (google-generativeai -> google-genai)
            raise ImportError("Gemini API í•„ìš”: pip install google-genai")
        
        # Use settings if api_key is not passed
        if not api_key:
             api_key = settings.GOOGLE_API_KEY
        
        # [í•µì‹¬ ìˆ˜ì •] configure ì‚­ì œ -> Client ê°ì²´ ìƒì„±ìœ¼ë¡œ ë³€ê²½
        # [í•µì‹¬ ìˆ˜ì •] configure ì‚­ì œ -> Client ê°ì²´ ìƒì„±ìœ¼ë¡œ ë³€ê²½
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ì œê±°í•˜ì—¬ ê¸°ë³¸ê°’ ì‚¬ìš© (600ì´ msë¡œ í•´ì„ë˜ì–´ 1ì´ˆ ë¯¸ë§Œ ì—ëŸ¬ ë°œìƒ ì¶”ì •)
        self.client = genai.Client(api_key=api_key)
        
        # [í•µì‹¬ ìˆ˜ì •] ëª¨ë¸ ì´ë¦„(ID)ì„ ì €ì¥í•´ ë‘¡ë‹ˆë‹¤.
        self.model_name = 'gemini-2.5-flash'
        
        # Retry Configuration
        self.retry_policy = {
            "retry": retry.Retry(predicate=retry.if_transient_error, initial=1.0, multiplier=2.0, maximum=60.0, timeout=300.0)
        }
        
        self.system_prompt = """ë‹¹ì‹ ì€ ì†Œì„¤/ìŠ¤í† ë¦¬ì˜ ì”¬ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì”¬ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”:

{
  "summary": "ì”¬ì˜ í•µì‹¬ ìš”ì•½ (2-3 ë¬¸ì¥)",
  "characters": ["ë“±ì¥í•˜ëŠ” ì¸ë¬¼ ì´ë¦„ë“¤"],
  "locations": ["ë“±ì¥í•˜ëŠ” ì¥ì†Œë“¤"],
  "items": ["ì¤‘ìš”í•œ ì•„ì´í…œ/ì†Œí’ˆë“¤"],
  "key_events": ["ì£¼ìš” ì‚¬ê±´/í–‰ë™ë“¤"],
  "mood": "ë¶„ìœ„ê¸° (ì˜ˆ: ê¸´ì¥ê°, í‰ì˜¨, ìŠ¬í””, ìœ ì¾Œ ë“±)",
  "time_period": "ì‹œê°„ëŒ€ ì •ë³´ (ìˆë‹¤ë©´)"
}

**ì¤‘ìš” ê·œì¹™:**
- ì •í™•íˆ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”
- ì—†ëŠ” ì •ë³´ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸([]) ë˜ëŠ” nullë¡œ í‘œì‹œ
- ì¸ë¬¼ ì´ë¦„ì€ ì¼ê´€ì„± ìˆê²Œ í‘œê¸° (ë³„ì¹­ë„ í†µì¼)
"""

    def _generate_with_retry(self, prompt: str):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ìƒì„± í•¨ìˆ˜"""
        try:
            # google-genai SDK 0.2+ style configuration
            # configëŠ” 'GenerateContentConfig' ê°ì²´ í˜¹ì€ dictì—¬ì•¼ í•˜ëŠ”ë°
            # timeoutì€ config ë‚´ë¶€ê°€ ì•„ë‹ˆë¼ http_options (ë˜ëŠ” ìœ ì‚¬ ì˜µì…˜)ìœ¼ë¡œ ì²˜ë¦¬ë˜ê±°ë‚˜
            # types.GenerateContentConfig(http_options={'timeout': 120}) í˜•íƒœì—¬ì•¼ í•¨.
            # í•˜ì§€ë§Œ ìµœì‹  ë²„ì „ì—ì„œëŠ” config ë‚´ë¶€ì— http_optionsë¥¼ ë„£ëŠ” ê²ƒì´ ì¼ë°˜ì ì„.
            # ERROR: Extra inputs are not permitted [type=extra_forbidden, input_value=120, input_type=int]
            
            # ì˜¬ë°”ë¥¸ ì„¤ì •: config ë‚´ë¶€ì— http_options ì‚¬ìš©
            from google.genai import types
            
            config = types.GenerateContentConfig(
                http_options={'timeout': 120000} # ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì¼ ìˆ˜ ë„ ìˆê³  ì´ˆ ë‹¨ìœ„ì¼ ìˆ˜ë„ ìˆìŒ. ë³´í†µ SDKëŠ” ì´ˆ ë‹¨ìœ„ì§€ë§Œ 120ìœ¼ë¡œ ì„¤ì •í•´ì„œ ì—ëŸ¬ë‚¬ìœ¼ë‹ˆ 
                # ì•„ê¹Œ ì—ëŸ¬ëŠ” timeout í•„ë“œ ìì²´ê°€ í—ˆìš©ë˜ì§€ ì•Šì•˜ìŒ.
            )
            
            # google-genai SDK ìµœì‹  ë³€ê²½ ì‚¬í•­:
            # Client.models.generate_content(..., config=...)
            # configì— timeout í•„ë“œê°€ ì—†ë‹¤ë©´, http_optionsë¥¼ ì¨ì•¼ í•¨.
            
            # ë‹¤ì‹œ ì‹œë„: config ë”•ì…”ë„ˆë¦¬ì— http_options ì¶”ê°€
            # (SDK ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°€ì¥ ì•ˆì „í•œ ë°©ë²• ì‹œë„)
            
            # Pydantic ì—ëŸ¬ê°€ ë‚¬ë‹¤ëŠ” ê±´ GenerateContentConfig ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨ì„.
            # GenerateContentConfigì—ëŠ” timeout í•„ë“œê°€ ì—†ìŒ.
            # ë³´í†µ client ë ˆë²¨ì´ë‚˜ í˜¸ì¶œ ì‹œì ì— http_optionsë¥¼ ì¤˜ì•¼ í•¨.
            
            response = self.client.models.generate_content(
                model=self.model_name, 
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type='application/json' # JSON ì‘ë‹µ ê°•ì œ (ì˜µì…˜)
                )
            )
            # íƒ€ì„ì•„ì›ƒì€ ì—¬ê¸°ì„œ í•´ê²°í•˜ê¸°ë³´ë‹¤, client ìƒì„±ì‹œë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í•´ì•¼í•  ìˆ˜ ìˆìŒ.
            # ì¼ë‹¨ ì—ëŸ¬ë‚˜ëŠ” timeout íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•˜ê³  ì‹¤í–‰.
            # (ë¦¬ì†ŒìŠ¤ ë¬¸ì œë¡œ ì¸í•œ íƒ€ì„ì•„ì›ƒì€ ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ ì»¤ë²„)
            return response
            return response
        except Exception as e:
            print(f"âš ï¸ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì¬ì‹œë„ ì‹¤íŒ¨): {e}")
            raise e

    def structure_scene(self, scene_text: str, scene_index: int) -> StructuredScene:
        """ë‹¨ì¼ ì”¬ êµ¬ì¡°í™” ë¶„ì„"""
        prompt = f"""{self.system_prompt}

ë‹¤ìŒ ì”¬ì„ ë¶„ì„í•˜ì„¸ìš”:

{scene_text}
"""
        try:
            response = self._generate_with_retry(prompt)
            json_text = response.text.strip()
            
            # Markdown code block ì œê±°
            if json_text.startswith("```"):
                json_text = re.sub(r'^```json?\s*|\s*```$', '', json_text, flags=re.MULTILINE)
            
            data = json.loads(json_text)
            
            return StructuredScene(
                scene_index=scene_index,
                original_text=scene_text,
                summary=data.get('summary', ''),
                characters=data.get('characters', []),
                locations=data.get('locations', []),
                items=data.get('items', []),
                key_events=data.get('key_events', []),
                mood=data.get('mood', ''),
                time_period=data.get('time_period')
            )
            
        except Exception as e:
            print(f"âš ï¸ ì”¬ {scene_index} êµ¬ì¡°í™” ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê°ì²´ ë°˜í™˜
            return StructuredScene(
                scene_index=scene_index,
                original_text=scene_text,
                summary="ë¶„ì„ ì‹¤íŒ¨",
                characters=[],
                locations=[],
                items=[],
                key_events=[],
                mood="",
                time_period=None
            )
            
    def extract_global_entities(self, structured_scenes: List[StructuredScene], custom_system_prompt: Optional[str] = None) -> Dict:
        """ì „ì²´ ì”¬ì—ì„œ ë“±ì¥í•˜ëŠ” ì—”í‹°í‹° í†µí•© ë¶„ì„ (ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì§€ì›)"""
        
        # ëª¨ë“  ì”¬ ì •ë³´ ìˆ˜ì§‘ (ì›ë³¸ í…ìŠ¤íŠ¸ ì œì™¸í•˜ì—¬ í† í° ì ˆì•½ for prompt)
        scenes_summary = []
        full_scenes_data = [] # ë°˜í™˜ìš© ì „ì²´ ë°ì´í„° (text í¬í•¨)

        for scene in structured_scenes:
            scene_data = asdict(scene)
            full_scenes_data.append(scene_data.copy()) # ì›ë³¸ ë³´ì¡´

            if 'original_text' in scene_data:
                del scene_data['original_text'] # í”„ë¡¬í”„íŠ¸ìš©ì—ì„œëŠ” ì œê±°
            scenes_summary.append(scene_data)
            
        all_info = {
            "scenes": scenes_summary
        }
        
        if custom_system_prompt:
            # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            print("ğŸ¨ ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
            prompt = f"""{custom_system_prompt}

ë‹¤ìŒì€ ì†Œì„¤ì˜ ì”¬ ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ„ í”„ë¡¬í”„íŠ¸ì˜ ì§€ì‹œì‚¬í•­ì„ ìˆ˜í–‰í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{json.dumps(all_info, ensure_ascii=False, indent=2)}
"""
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ ë°”ì´ë¸” êµ¬ì¡°)
            prompt = f"""{self.system_prompt}

ë‹¤ìŒì€ ì—¬ëŸ¬ ì”¬ì˜ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì „ì²´ ìŠ¤í† ë¦¬ì—ì„œ ë“±ì¥í•˜ëŠ” ì£¼ìš” ì—”í‹°í‹°ë“¤ì„ í†µí•©í•˜ì—¬ ì •ë¦¬í•˜ì„¸ìš”:

{json.dumps(all_info, ensure_ascii=False, indent=2)}

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "characters": [
    {{
      "name": "ì¸ë¬¼ ì´ë¦„",
      "aliases": ["ë³„ì¹­1", "ë³„ì¹­2"],
      "description": "ì¸ë¬¼ ì„¤ëª…",
      "first_appearance": ì²«_ë“±ì¥_ì”¬_ë²ˆí˜¸,
      "traits": ["íŠ¹ì§•1", "íŠ¹ì§•2"]
    }}
  ],
  "items": [
    {{
      "name": "ì•„ì´í…œ ì´ë¦„",
      "description": "ì„¤ëª…",
      "first_appearance": ì²«_ë“±ì¥_ì”¬_ë²ˆí˜¸,
      "significance": "ìŠ¤í† ë¦¬ìƒ ì˜ë¯¸"
    }}
  ],
  "locations": [
    {{
      "name": "ì¥ì†Œ ì´ë¦„",
      "description": "ì¥ì†Œ ì„¤ëª…",
      "scenes": [ë“±ì¥í•œ_ì”¬_ë²ˆí˜¸ë“¤]
    }}
  ],
  ],
  "key_events": [
    {{
      "summary": "í•µì‹¬ ì‚¬ê±´ ë‚´ìš©",
      "scene_index": ì”¬_ë²ˆí˜¸,
      "importance": "ìƒ/ì¤‘/í•˜"
    }}
  ]
}}
"""
        
        try:
            response = self._generate_with_retry(prompt)
            json_text = response.text.strip()
            
            if json_text.startswith("```"):
                json_text = re.sub(r'^```json?\s*|\s*```$', '', json_text, flags=re.MULTILINE)
            
            result = json.loads(json_text)
            
            result = json.loads(json_text)
            
            # [ìˆ˜ì •] ì”¬ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê²°ê³¼ì— í¬í•¨
            result['scenes'] = full_scenes_data

            # [ì¶”ê°€] ìºë¦­í„°ë³„ ë“±ì¥ ì”¬(appearances) ê³„ì‚° ë° ë³´ê°•
            if 'characters' in result:
                for char in result['characters']:
                    char_name = char.get('name', '')
                    char_aliases = char.get('aliases', [])
                    appearances = []

                    for scene in full_scenes_data:
                        scene_chars = scene.get('characters', [])
                        # í•´ë‹¹ ì”¬ì˜ ë“±ì¥ì¸ë¬¼ ëª©ë¡ì— ì´ë¦„ì´ë‚˜ ë³„ì¹­ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        is_appeared = False
                        if char_name in scene_chars:
                            is_appeared = True
                        else:
                            for alias in char_aliases:
                                if alias in scene_chars:
                                    is_appeared = True
                                    break
                        
                        if is_appeared:
                            appearances.append(scene['scene_index'])
                    
                    char['appearances'] = appearances
                    char['appearance_count'] = len(appearances)

            # [ì¶”ê°€] ì•„ì´í…œë³„ ë“±ì¥ ì”¬(appearances) ê³„ì‚° ë° ë³´ê°•
            if 'items' in result:
                for item in result['items']:
                    item_name = item.get('name', '')
                    appearances = []

                    for scene in full_scenes_data:
                        scene_items = scene.get('items', [])
                        # í•´ë‹¹ ì”¬ì˜ ì•„ì´í…œ ëª©ë¡ì— ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        if item_name in scene_items:
                            appearances.append(scene['scene_index'])
                    
                    item['appearances'] = appearances
                    item['appearance_count'] = len(appearances)

            # [ì¶”ê°€] ì¥ì†Œë³„ ë“±ì¥ ì”¬(scenes) ê³„ì‚° ë° ë³´ê°•
            if 'locations' in result:
                for loc in result['locations']:
                    loc_name = loc.get('name', '')
                    related_scenes = []

                    for scene in full_scenes_data:
                        scene_locs = scene.get('locations', [])
                        # í•´ë‹¹ ì”¬ì˜ ì¥ì†Œ ëª©ë¡ì— ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        if loc_name in scene_locs:
                            related_scenes.append(scene['scene_index'])
                    
                    # LLMì´ ì¶”ì¶œí•œ scenes ë¦¬ìŠ¤íŠ¸ê°€ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, ì •í™•ë„ë¥¼ ìœ„í•´ ê³„ì‚°ëœ ê°’ìœ¼ë¡œ ë®ì–´ì“°ê±°ë‚˜ ë³´ì™„
                    # ì—¬ê¸°ì„œëŠ” ê³„ì‚°ëœ ê°’ìœ¼ë¡œ ë®ì–´ì”ë‹ˆë‹¤.
                    loc['scenes'] = related_scenes
                    loc['appearance_count'] = len(related_scenes)
            
            return result
        
        except Exception as e:
            print(f"âš ï¸  ì „ì—­ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (ë™ì  êµ¬ì¡°ì´ë¯€ë¡œ í‚¤ë¥¼ ë¯¸ë¦¬ ì•Œ ìˆ˜ ì—†ìŒ)
            # ì‹¤íŒ¨í•˜ë”ë¼ë„ ì”¬ ì •ë³´ëŠ” ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
            return {"scenes": full_scenes_data}


# ============================================================================
# 3. ì„ë² ë”© ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ (Bge-m3 + Pinecone + Postgres)
# ============================================================================

class EmbeddingSearchEngine:
    """ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ (Pinecone ì—°ë™)"""
    
    def __init__(self):
        """
        BAAI/bge-m3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±
        Pineconeì„ ë²¡í„° ì €ì¥ì†Œë¡œ ì‚¬ìš©
        """
        try:
            from sentence_transformers import SentenceTransformer
            from pinecone import Pinecone
        except ImportError:
            raise ImportError("sentence-transformers, pinecone-client í•„ìš”: pip install sentence-transformers pinecone-client")
        
        print("ğŸ”„ BAAI/bge-m3 ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = SentenceTransformer('BAAI/bge-m3')
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # Pinecone ì´ˆê¸°í™”
        self.pc = Pinecone(api_key=settings.PINECONE_API_KEY)
        self.index_name = settings.PINECONE_INDEX_NAME
        
        # ì¸ë±ìŠ¤ í™•ì¸ (ì—†ìœ¼ë©´ ìƒì„±ì€ í•˜ì§€ ì•ŠìŒ, ì—ëŸ¬ ë°œìƒ)
        if self.index_name not in [idx.name for idx in self.pc.list_indexes()]:
             print(f"âš ï¸ Pinecone ì¸ë±ìŠ¤ '{self.index_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")
             # raise ValueError(f"Pinecone index '{self.index_name}' not found.")
        
        self.index = self.pc.Index(self.index_name)
        print(f"âœ… Pinecone ì¸ë±ìŠ¤ ì—°ê²°: {self.index_name}")
    
    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        embedding = self.model.encode(text, normalize_embeddings=True)
        return embedding.tolist()
    
    def add_documents(self, documents: List[Dict], novel_id: int):
        """ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•˜ì—¬ Pineconeê³¼ DBì— ì €ì¥"""
        print(f"\nğŸ“¥ {len(documents)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        
        db = SessionLocal()
        vectors_to_upsert = []
        
        try:
            for i, doc in enumerate(documents):
                # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ìƒì„± (ìš”ì•½ + ì›ë¬¸ ì¼ë¶€)
                # ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
                search_text = f"{doc.get('summary', '')} {doc.get('original_text', '')[:1000]}"
                
                # ì„ë² ë”© ìƒì„±
                embedding = self.embed_text(search_text)
                
                # ê³ ìœ  ID ìƒì„± (UUID ë˜ëŠ” novel_id_scene_index í˜•ì‹)
                vector_id = f"novel_{novel_id}_scene_{doc['scene_index']}"
                
                # Pinecone ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (í•„í„°ë§ ë° ê°„ë‹¨í•œ ì •ë³´ í‘œì‹œìš©)
                metadata = {
                    'novel_id': novel_id,
                    'scene_index': doc['scene_index'],
                    'summary': doc.get('summary', '')[:200],  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œí•œ
                    # 'type': 'scene' # ë‚˜ì¤‘ì— ì±•í„°ë‚˜ ë‹¤ë¥¸ ë‹¨ìœ„ê°€ ìƒê¸°ë©´ êµ¬ë¶„
                }
                
                vectors_to_upsert.append({
                    'id': vector_id,
                    'values': embedding,
                    'metadata': metadata
                })
                
                # DBì— ìƒì„¸ ì •ë³´ ì €ì¥ (VectorDocument)
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                existing_doc = db.query(VectorDocument).filter(
                    VectorDocument.vector_id == vector_id
                ).first()
                
                if existing_doc:
                    existing_doc.chunk_text = doc.get('original_text', '')
                    existing_doc.metadata_json = doc # ì „ì²´ êµ¬ì¡°í™” ì •ë³´ ì €ì¥
                else:
                    new_doc = VectorDocument(
                        novel_id=novel_id,
                        chapter_id=None, # í˜„ì¬ëŠ” ì”¬ ë‹¨ìœ„ì´ë¯€ë¡œ ì±•í„° ì •ë³´ê°€ ëª…ì‹œì ìœ¼ë¡œ ì—†ìœ¼ë©´ None
                        vector_id=vector_id,
                        chunk_index=doc['scene_index'],
                        chunk_text=doc.get('original_text', ''),
                        metadata_json=doc
                    )
                    db.add(new_doc)
                
                if (i + 1) % 10 == 0:
                    print(f"  ì§„í–‰: {i + 1}/{len(documents)}")
            
            # Pinecone ì—…ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬ ê¶Œì¥í•˜ì§€ë§Œ ì—¬ê¸°ì„  í•œë°©ì—)
            # Pinecone limit per request is usually 100 vectors, stick to safe batching if needed.
            batch_size = 100
            for i in range(0, len(vectors_to_upsert), batch_size):
                batch = vectors_to_upsert[i:i + batch_size]
                self.index.upsert(vectors=batch)
                
            db.commit()
            print("âœ… Pinecone ì—…ë¡œë“œ ë° DB ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            db.rollback()
            print(f"âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e
        finally:
            db.close()
    
    def search(self, query: str, novel_id: int = None, top_k: int = 5) -> List[Dict]:
        """ì†Œì„¤ ë‚´ì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì”¬ ê²€ìƒ‰"""
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embed_text(query)
        
        # Pinecone ì¿¼ë¦¬ í•„í„°
        filter_dict = {}
        if novel_id:
            filter_dict['novel_id'] = novel_id # Pinecone metadata filter uses float usually, but int works if stored as number
        
        # Pinecone ê²€ìƒ‰
        results = self.index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            filter=filter_dict if filter_dict else None
        )
        
        # ê²°ê³¼ ë§¤í•‘
        hits = []
        db = SessionLocal()
        try:
            for match in results.matches:
                vector_id = match.id
                score = match.score
                
                # DBì—ì„œ ì›ë³¸ ë°ì´í„° ì¡°íšŒ (ì „ì²´ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´)
                doc = db.query(VectorDocument).filter(VectorDocument.vector_id == vector_id).first()
                
                if doc:
                    # JSON ë©”íƒ€ë°ì´í„°ì—ì„œ êµ¬ì¡°í™”ëœ ì”¬ ì •ë³´ ë³µì›
                    scene_data = doc.metadata_json
                    hits.append({
                        'document': scene_data,
                        'similarity': score,
                        'vector_id': vector_id
                    })
                else:
                    # DBì— ì—†ì„ ê²½ìš° Pinecone ë©”íƒ€ë°ì´í„°ë¼ë„ ì‚¬ìš©
                    print(f"âš ï¸ DBì—ì„œ ë¬¸ì„œ {vector_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    hits.append({
                        'document': {
                            'scene_index': match.metadata.get('scene_index'),
                            'summary': match.metadata.get('summary'),
                            'characters': [],
                            'locations': [],
                            'original_text': "(DB ì¡°íšŒ ì‹¤íŒ¨)"
                        },
                        'similarity': score,
                        'vector_id': vector_id
                    })
        finally:
            db.close()
        
        return hits


# ============================================================================
# 4. ìŠ¤í† ë¦¬ë³´ë“œ ë° ì‚¬ì „ ì‹œìŠ¤í…œ (DB ì—°ë™)
# ============================================================================

class StoryboardSystem:
    """í†µí•© ìŠ¤í† ë¦¬ë³´ë“œ + ì‚¬ì „ ì‹œìŠ¤í…œ"""
    
    def __init__(self, gemini_api_key: str): # Key is used for Gemini
        self.structurer = GeminiStructurer(gemini_api_key)
        self.search_engine = EmbeddingSearchEngine() # Pinecone key is in settings
        
        self.current_novel_id: Optional[int] = None
        self.structured_scenes: List[StructuredScene] = []
        self.entities: Dict = {}
    
    def export_storyboard(self, filename: str = "storyboard.txt"):
        """ìŠ¤í† ë¦¬ë³´ë“œë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if not self.current_novel_id:
            print("âŒ ì„ íƒëœ ì†Œì„¤ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        db = SessionLocal()
        try:
            docs = db.query(VectorDocument).filter(
                VectorDocument.novel_id == self.current_novel_id
            ).order_by(VectorDocument.chunk_index).all()
            
            if not docs:
                print("ğŸ“­ ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"=== ìŠ¤í† ë¦¬ë³´ë“œ (ID: {self.current_novel_id}) ===\n\n")
                
                for doc in docs:
                    meta = doc.metadata_json or {}
                    summary = meta.get('summary', 'ìš”ì•½ ì—†ìŒ')
                    chars = ", ".join(meta.get('characters', []))
                    
                    f.write(f"[Scene {doc.chunk_index}]\n")
                    f.write(f"ìš”ì•½: {summary}\n")
                    if chars:
                        f.write(f"ë“±ì¥ì¸ë¬¼: {chars}\n")
                    
                    locs = ", ".join(meta.get('locations', []))
                    if locs:
                        f.write(f"ì¥ì†Œ: {locs}\n")
                        
                    f.write("-" * 50 + "\n\n")
            
            print(f"âœ… ìŠ¤í† ë¦¬ë³´ë“œê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        finally:
            db.close()

    def get_or_create_novel(self, title: str) -> int:
        """ì†Œì„¤ DB ë“±ë¡ ë˜ëŠ” ì¡°íšŒ"""
        db = SessionLocal()
        try:
            # Check if exists
            novel = db.query(Novel).filter(Novel.title == title).first()
            if not novel:
                # ì„ì‹œë¡œ 1ë²ˆ ìœ ì €(admin)ì—ê²Œ í• ë‹¹í•˜ê² ìŠµë‹ˆë‹¤.
                # ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ë¡œê·¸ì¸í•œ ìœ ì € ì •ë³´ë¥¼ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.
                user = db.query(User).first()
                if not user:
                    # Create dummy user if not exists (for standalone test)
                    user = User(email="admin@example.com", username="admin", hashed_password="hashed_password")
                    db.add(user)
                    db.commit()
                    db.refresh(user)
                
                novel = Novel(
                    title=title,
                    author_id=user.id,
                    description="Analyzed by Story Analyzer"
                )
                db.add(novel)
                db.commit()
                db.refresh(novel)
                print(f"ğŸ†• ìƒˆ ì†Œì„¤ ë“±ë¡: {title} (ID: {novel.id})")
            else:
                print(f"â„¹ï¸ ê¸°ì¡´ ì†Œì„¤ ë¡œë“œ: {title} (ID: {novel.id})")
            
            return novel.id
        finally:
            db.close()

    def load_entities_from_db(self, novel_id: int):
        """DBì—ì„œ ì—”í‹°í‹°(ì‚¬ì „) ì •ë³´ ë¡œë“œ"""
        db = SessionLocal()
        try:
            # OVERALL íƒ€ì…ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ìŒ
            analysis = db.query(Analysis).filter(
                Analysis.novel_id == novel_id,
                Analysis.analysis_type == AnalysisType.OVERALL
            ).first()
            
            if analysis and analysis.result:
                self.entities = analysis.result
                print("âœ… DBì—ì„œ ê¸°ì¡´ ì—”í‹°í‹° ì‚¬ì „ ë¡œë“œ ì™„ë£Œ")
            else:
                self.entities = {}
                print("â„¹ï¸ ê¸°ì¡´ ì—”í‹°í‹° ì‚¬ì „ì´ ì—†ìŠµë‹ˆë‹¤.")
                
        finally:
            db.close()

    def save_entities_to_db(self, novel_id: int, entities: Dict):
        """ì—”í‹°í‹°(ì‚¬ì „) ì •ë³´ë¥¼ DBì— ì €ì¥"""
        db = SessionLocal()
        try:
            # ê¸°ì¡´ ë ˆì½”ë“œ í™•ì¸
            analysis = db.query(Analysis).filter(
                Analysis.novel_id == novel_id,
                Analysis.analysis_type == AnalysisType.OVERALL
            ).first()
            
            if not analysis:
                analysis = Analysis(
                    novel_id=novel_id,
                    analysis_type=AnalysisType.OVERALL,
                    status=AnalysisStatus.COMPLETED,
                    result=entities
                )
                db.add(analysis)
            else:
                analysis.result = entities
                analysis.updated_at = db.func.now()
            
            db.commit()
            print("ğŸ’¾ ì—”í‹°í‹° ì‚¬ì „ DB ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            db.rollback()
            print(f"âŒ ì—”í‹°í‹° ì €ì¥ ì‹¤íŒ¨: {e}")
        finally:
            db.close()

    def process_story(self, file_path: str, scene_threshold: int = 8):
        """ìŠ¤í† ë¦¬ íŒŒì¼ ì „ì²´ ì²˜ë¦¬ (scene_threshold: ì”¬ ë¶„í•  ì„ê³„ê°’, ë‚®ì„ìˆ˜ë¡ ë” ì˜ê²Œ ë¶„í• )"""
        
        filename = os.path.basename(file_path)
        novel_title = os.path.splitext(filename)[0]
        
        print("=" * 70)
        print(f"ğŸ¬ ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„± ì‹œì‘: {novel_title}")
        print("=" * 70)
        
        # DB ì†Œì„¤ ID í™•ë³´
        self.current_novel_id = self.get_or_create_novel(novel_title)
        
        # 1. íŒŒì¼ ë¡œë“œ ë° ì”¬ ë¶„í• 
        print("\n[1/4] íŒŒì¼ ë¡œë“œ ë° ì”¬ ë¶„í• ")
        loader = DocumentLoader()
        text = loader.load_txt(file_path)
        
        chunker = SceneChunker(threshold=scene_threshold)
        scenes = chunker.split_into_scenes(text)
        print(f"âœ… {len(scenes)}ê°œ ì”¬ ìƒì„±")
        
        # 2. ê° ì”¬ êµ¬ì¡°í™”
        print(f"\n[2/4] ì”¬ë³„ êµ¬ì¡°í™” (Gemini ë¶„ì„)")
        self.structured_scenes = []
        for i, scene in enumerate(scenes):
            structured = self.structurer.structure_scene(scene, i)
            self.structured_scenes.append(structured)
            print(f"  ì”¬ {i+1}/{len(scenes)} ì™„ë£Œ")
        
        # 3. ì „ì—­ ì—”í‹°í‹° ì¶”ì¶œ
        print(f"\n[3/4] ì „ì—­ ì—”í‹°í‹° ì¶”ì¶œ (ì¸ë¬¼, ì•„ì´í…œ, ì¥ì†Œ)")
        self.entities = self.structurer.extract_global_entities(self.structured_scenes)
        print(f"âœ… ì¸ë¬¼: {len(self.entities.get('characters', []))}ëª…")
        print(f"âœ… ì•„ì´í…œ: {len(self.entities.get('items', []))}ê°œ")
        print(f"âœ… ì¥ì†Œ: {len(self.entities.get('locations', []))}ê³³")
        
        # DB ì €ì¥ (ì—”í‹°í‹°)
        self.save_entities_to_db(self.current_novel_id, self.entities)
        
        # 4. ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± (Pinecone + DB)
        print(f"\n[4/4] ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± (Pinecone & DB)")
        documents = [asdict(scene) for scene in self.structured_scenes]
        self.search_engine.add_documents(documents, self.current_novel_id)
        
        print("\n" + "=" * 70)
        print("âœ… ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„± ë° ì €ì¥ ì™„ë£Œ!")
        print("=" * 70)
    
    def search_scenes(self, query: str, top_k: int = 5):
        """ì”¬ ê²€ìƒ‰ (Pinecone ì‚¬ìš©)"""
        if not self.current_novel_id:
             print("âŒ ì„ íƒëœ ì†Œì„¤ì´ ì—†ìŠµë‹ˆë‹¤.")
             return []
        
        print(f"\nğŸ” ê²€ìƒ‰: '{query}'")
        results = self.search_engine.search(query, self.current_novel_id, top_k)
        
        print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {len(results)}ê°œ):\n")
        for i, result in enumerate(results):
            doc = result['document']
            sim = result['similarity']
            
            print(f"[{i+1}] ì”¬ {doc.get('scene_index', '?')} (ìœ ì‚¬ë„: {sim:.3f})")
            print(f"    ìš”ì•½: {doc.get('summary', 'ë‚´ìš© ì—†ìŒ')}")
            # print(f"    ì¸ë¬¼: {', '.join(doc.get('characters', []))}") # Optional
            print()
        
        return results
    
    def lookup_character(self, name: str) -> Optional[Dict]:
        """ì¸ë¬¼ ì‚¬ì „ ì¡°íšŒ"""
        if not self.entities and self.current_novel_id:
             self.load_entities_from_db(self.current_novel_id)

        for char in self.entities.get('characters', []):
            if char['name'] == name or name in char.get('aliases', []):
                return char
        return None
    
    def lookup_item(self, name: str) -> Optional[Dict]:
        """ì•„ì´í…œ ì‚¬ì „ ì¡°íšŒ"""
        if not self.entities and self.current_novel_id:
             self.load_entities_from_db(self.current_novel_id)

        for item in self.entities.get('items', []):
            if item['name'] == name:
                return item
        return None
    
    def lookup_location(self, name: str) -> Optional[Dict]:
        """ì¥ì†Œ ì‚¬ì „ ì¡°íšŒ"""
        if not self.entities and self.current_novel_id:
             self.load_entities_from_db(self.current_novel_id)

        for loc in self.entities.get('locations', []):
            if loc['name'] == name:
                return loc
        return None
    
    def print_dictionary(self):
        """ì „ì²´ ì‚¬ì „ ì¶œë ¥"""
        if not self.entities and self.current_novel_id:
             self.load_entities_from_db(self.current_novel_id)

        print("\n" + "=" * 70)
        print("ğŸ“– ìŠ¤í† ë¦¬ ì‚¬ì „")
        print("=" * 70)
        
        # ì¸ë¬¼
        print("\nğŸ‘¥ ì¸ë¬¼:")
        for char in self.entities.get('characters', []):
            print(f"\n  â€¢ {char['name']}")
            if char.get('aliases'):
                print(f"    ë³„ì¹­: {', '.join(char['aliases'])}")
            print(f"    ì„¤ëª…: {char.get('description', 'ì—†ìŒ')}")
            print(f"    ì²« ë“±ì¥: ì”¬ {char.get('first_appearance', '?')}")
            if char.get('traits'):
                print(f"    íŠ¹ì§•: {', '.join(char['traits'])}")
        
        # ì•„ì´í…œ
        print("\nğŸ“¦ ì•„ì´í…œ:")
        for item in self.entities.get('items', []):
            print(f"\n  â€¢ {item['name']}")
            print(f"    ì„¤ëª…: {item.get('description', 'ì—†ìŒ')}")
            print(f"    ì²« ë“±ì¥: ì”¬ {item.get('first_appearance', '?')}")
            print(f"    ì˜ë¯¸: {item.get('significance', 'ì—†ìŒ')}")
        
        # ì¥ì†Œ
        print("\nğŸ—ºï¸  ì¥ì†Œ:")
        for loc in self.entities.get('locations', []):
            print(f"\n  â€¢ {loc['name']}")
            print(f"    ì„¤ëª…: {loc.get('description', 'ì—†ìŒ')}")
            scenes = loc.get('scenes', [])
            if scenes:
                print(f"    ë“±ì¥ ì”¬: {', '.join(map(str, scenes))}")
        
        print("\n" + "=" * 70)


# ============================================================================
# 5. ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    
    # 1. ì„¤ì • í™•ì¸
    api_key = settings.GOOGLE_API_KEY
    if not api_key:
        print("âŒ GOOGLE_API_KEY/GEMINI_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
        
    print(f"ğŸ”§ í™˜ê²½: {settings.ENVIRONMENT}")
    print(f"ğŸŒ² Pinecone: {settings.PINECONE_INDEX_NAME}")
    print(f"ğŸ›¢ï¸ Database: {settings.DATABASE_URL}")
    print("-" * 50)
    
    storyboard = StoryboardSystem(api_key)
    
    # ì˜ˆì‹œ: í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
    import glob
    import argparse
    
    parser = argparse.ArgumentParser(description="Story Analyzer")
    parser.add_argument("file", nargs="?", help="Path to the text file to analyze")
    args = parser.parse_args()
    
    input_file = None
    
    if args.file:
        if os.path.exists(args.file):
            input_file = args.file
        else:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.file}")
            return
    else:
        txt_files = glob.glob("*.txt")
        
        if not txt_files:
            print("âŒ í˜„ì¬ ë””ë ‰í† ë¦¬ì— .txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ì‚¬ìš©ë²•: python story_analyzer.py [íŒŒì¼ê²½ë¡œ]")
            print("ë˜ëŠ” í˜„ì¬ í´ë”ì— .txt íŒŒì¼ì„ ë³µì‚¬í•´ì£¼ì„¸ìš”.")
            return
        
        print("ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼:")
        for i, f in enumerate(txt_files):
            print(f"[{i+1}] {f}")
            
        choice = input("\níŒŒì¼ ë²ˆí˜¸ ì„ íƒ (ì—”í„°: ì²«ë²ˆì§¸ íŒŒì¼): ").strip()
        idx = 0
        if choice and choice.isdigit():
            idx = int(choice) - 1
            
        if idx < 0 or idx >= len(txt_files):
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return
            
        input_file = txt_files[idx]
    
    print(f"ğŸ“„ ì„ íƒëœ íŒŒì¼: {input_file}")
    
    # Processing Option
    print("\nì‘ì—… ì„ íƒ:")
    print(" [1] ìƒˆë¡œ ë¶„ì„ ë° DB ì €ì¥ (ì‹œê°„ ì†Œìš”ë¨)")
    print(" [2] ê¸°ì¡´ DB ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°")
    mode = input("ì„ íƒ (ê¸°ë³¸: 2): ").strip()
    
    if mode == "1":
        storyboard.process_story(input_file, scene_threshold=8)
    else:
        # Load logic
        filename = os.path.basename(input_file)
        novel_title = os.path.splitext(filename)[0]
        novel_id = storyboard.get_or_create_novel(novel_title)
        storyboard.current_novel_id = novel_id
        storyboard.load_entities_from_db(novel_id)
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: Novel ID {novel_id}")
    
    # 3. ì‚¬ì „ ì¶œë ¥
    storyboard.print_dictionary()
    
    # 4. ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
    print("\n" + "=" * 70)
    print("ğŸ’¬ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ")
    print("=" * 70)
    print("ëª…ë ¹ì–´:")
    print("  search [ì¿¼ë¦¬]    - ì”¬ ê²€ìƒ‰")
    print("  export          - ìŠ¤í† ë¦¬ë³´ë“œ txt ë‚´ë³´ë‚´ê¸°")
    print("  char [ì´ë¦„]      - ì¸ë¬¼ ì¡°íšŒ")
    print("  item [ì´ë¦„]      - ì•„ì´í…œ ì¡°íšŒ")
    print("  loc [ì´ë¦„]       - ì¥ì†Œ ì¡°íšŒ")
    print("  dict            - ì „ì²´ ì‚¬ì „ ë³´ê¸°")
    print("  quit            - ì¢…ë£Œ")
    print()
    
    while True:
        try:
            cmd = input(">>> ").strip()
            
            if not cmd:
                continue
            
            if cmd == "quit":
                break
            
            elif cmd == "export":
                storyboard.export_storyboard()
            
            elif cmd == "dict":
                storyboard.print_dictionary()
            
            elif cmd.startswith("search "):
                query = cmd[7:]
                storyboard.search_scenes(query, top_k=5)
            
            elif cmd.startswith("char "):
                name = cmd[5:]
                char = storyboard.lookup_character(name)
                if char:
                    print(f"\nğŸ‘¤ {char['name']}")
                    print(f"   {char.get('description', '')}")
                else:
                    print(f"âŒ '{name}' ì¸ë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            elif cmd.startswith("item "):
                name = cmd[5:]
                item = storyboard.lookup_item(name)
                if item:
                    print(f"\nğŸ“¦ {item['name']}")
                    print(f"   {item.get('description', '')}")
                else:
                    print(f"âŒ '{name}' ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            elif cmd.startswith("loc "):
                name = cmd[4:]
                loc = storyboard.lookup_location(name)
                if loc:
                    print(f"\nğŸ—ºï¸  {loc['name']}")
                    print(f"   {loc.get('description', '')}")
                else:
                    print(f"âŒ '{name}' ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            else:
                print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.")
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
