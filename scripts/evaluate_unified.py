"""
RAG & Agent í†µí•© í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
==============================
ìƒì„±ëœ ë°ì´í„°ì…‹(eval_dataset.json)ì„ ê¸°ë°˜ìœ¼ë¡œ
16ê°œ ì „ì²´ ì†Œì„¤ì— ëŒ€í•´ RAG ì •í™•ë„ì™€ Agent ì¼ê´€ì„±ì„ í•œ ë²ˆì— í‰ê°€í•©ë‹ˆë‹¤.

ê¸°ëŠ¥:
1. RAG íŒŒì´í”„ë¼ì¸ í‰ê°€ (4ê°œ ì§€í‘œ)
2. Agent ì¼ê´€ì„± ê²€ì‚¬ í‰ê°€ (3ê°œ ì§€í‘œ + ì •í™•ë„)
3. í†µí•© ê²°ê³¼ ì €ì¥ (rag_eval_results.json, agent_eval_results.json)

ì‚¬ìš©ë²•:
    python scripts/evaluate_unified.py
    python scripts/evaluate_unified.py --max-samples 10
"""

import os
import sys
import json
import time
import asyncio
import argparse
import numpy as np
from typing import Dict, List, Optional
from tqdm import tqdm
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from google import genai
from dotenv import load_dotenv
from backend.db.session import SessionLocal
from backend.db.models import Novel, Chapter
from backend.services.chatbot_service import ChatbotService
from backend.services.analysis.agent import StoryConsistencyAgent

load_dotenv()

# ===== ì„¤ì • =====
RAG_OUTPUT_FILE = "rag_eval_results.json"
AGENT_OUTPUT_FILE = "agent_eval_results.json"

# ===== í”„ë¡¬í”„íŠ¸ (RAG) =====
CONTEXT_RELEVANCE_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[ì§ˆë¬¸]: {question}
[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸]:
{context}
ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
í‰ê°€ ê¸°ì¤€:
- 1ì : ì™„ì „íˆ ë¬´ê´€í•œ ë‚´ìš©
- 2ì : ì•½ê°„ì˜ ê´€ë ¨ì„±ì€ ìˆì§€ë§Œ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ë¶€ì¡±
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ ìˆìœ¼ë‚˜ í•µì‹¬ ì •ë³´ ë¶€ì¡±
- 4ì : ëŒ€ë¶€ë¶„ ê´€ë ¨ ìˆê³  ë‹µë³€ì— ì¶©ë¶„í•œ ì •ë³´ í¬í•¨
- 5ì : ì§ˆë¬¸ì— ì •í™•íˆ ëŒ€ì‘í•˜ëŠ” ë†’ì€ ê´€ë ¨ì„±
JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

FAITHFULNESS_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ ì¶©ì‹¤ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[ì§ˆë¬¸]: {question}
[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸]:
{context}
[ìƒì„±ëœ ë‹µë³€]:
{answer}
ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê·¼ê±°í•˜ì—¬ ì‘ì„±ë˜ì—ˆëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í–ˆë‹¤ë©´(í™˜ê°/hallucination) ê°ì í•©ë‹ˆë‹¤.
í‰ê°€ ê¸°ì¤€:
- 1ì : ë‹µë³€ì˜ ëŒ€ë¶€ë¶„ì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ (ì‹¬ê°í•œ í™˜ê°)
- 2ì : ë§ì€ ë¶€ë¶„ì´ ì»¨í…ìŠ¤íŠ¸ì™€ ë¬´ê´€í•˜ê±°ë‚˜ ì§€ì–´ë‚¸ ë‚´ìš©
- 3ì : ì¼ë¶€ í™˜ê°ì´ ìˆìœ¼ë‚˜ í•µì‹¬ì€ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜
- 4ì : ê±°ì˜ ëª¨ë‘ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°, ì‚¬ì†Œí•œ ì¶”ë¡ ë§Œ í¬í•¨
- 5ì : ì™„ë²½íˆ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê·¼ê±°í•œ ë‹µë³€
JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

ANSWER_RELEVANCE_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[ì§ˆë¬¸]: {question}
[ìƒì„±ëœ ë‹µë³€]:
{answer}
ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì ì ˆíˆ ëŒ€ë‹µí•˜ëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
í‰ê°€ ê¸°ì¤€:
- 1ì : ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ë‹µë³€
- 2ì : ì§ˆë¬¸ì˜ ì£¼ì œëŠ” ë§ì§€ë§Œ í•µì‹¬ì„ ë¹—ë‚˜ê°
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ë‹µë³€í•˜ë‚˜ ë¶ˆì™„ì „
- 4ì : ëŒ€ì²´ë¡œ ì ì ˆí•œ ë‹µë³€ì´ë‚˜ ì•½ê°„ì˜ ë³´ì™„ í•„ìš”
- 5ì : ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì™„ë²½í•˜ê²Œ ëŒ€ë‹µ
JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

ANSWER_CORRECTNESS_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[ì§ˆë¬¸]: {question}
[ì •ë‹µ (Ground Truth)]:
{ground_truth}
[ìƒì„±ëœ ë‹µë³€]:
{answer}
ìƒì„±ëœ ë‹µë³€ì´ ì •ë‹µê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
í‰ê°€ ê¸°ì¤€:
- 1ì : ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš©
- 2ì : ì•½ê°„ì˜ ê´€ë ¨ì„±ë§Œ ìˆê³  í•µì‹¬ì´ ë‹¤ë¦„
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ë§ì§€ë§Œ ì¤‘ìš”í•œ ë¶€ë¶„ ëˆ„ë½ ë˜ëŠ” ì˜¤ë¥˜
- 4ì : í•µì‹¬ ì˜ë¯¸ëŠ” ì¼ì¹˜í•˜ë‚˜ ì„¸ë¶€ ì°¨ì´ ì¡´ì¬
- 5ì : ì •ë‹µê³¼ ì™„ì „íˆ ì¼ì¹˜
JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

# ===== í”„ë¡¬í”„íŠ¸ (Agent) =====
TOOL_USE_ACCURACY_PROMPT = """ë‹¹ì‹ ì€ AI Agentì˜ ë„êµ¬ í™œìš© í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[í…ŒìŠ¤íŠ¸ ì…ë ¥ ë¬¸ì¥]:
{input_text}
[Agentê°€ ê²€ìƒ‰í•œ ì»¨í…ìŠ¤íŠ¸]:
{retrieved_context}
[ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…]:
{explanation}
Agentê°€ ì„¤ì • ì¼ê´€ì„± ê²€ì‚¬ë¥¼ ìœ„í•´ ì ì ˆí•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í–ˆëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
í‰ê°€ ê¸°ì¤€:
- 1ì : ì™„ì „íˆ ë¬´ê´€í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰
- 2ì : ì•½ê°„ ê´€ë ¨ ìˆì§€ë§Œ íŒë‹¨ì— ë¶€ì¡±
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ ìˆëŠ” ì»¨í…ìŠ¤íŠ¸
- 4ì : ëŒ€ë¶€ë¶„ ê´€ë ¨ ìˆê³  íŒë‹¨ì— ì¶©ë¶„í•œ ì •ë³´
- 5ì : íŒë‹¨ì— ì™„ë²½íˆ ì í•©í•œ ë†’ì€ ê´€ë ¨ì„±ì˜ ì»¨í…ìŠ¤íŠ¸
JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

REASONING_QUALITY_PROMPT = """ë‹¹ì‹ ì€ AI Agentì˜ ì¶”ë¡  í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[í…ŒìŠ¤íŠ¸ ì…ë ¥ ë¬¸ì¥]:
{input_text}
[ì˜ˆìƒ ê²°ê³¼]: {expected_status}
[ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•]: {scenario_type}
[ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…]: {explanation}
[Agentì˜ ì‹¤ì œ ì‘ë‹µ]:
{agent_response}
Agentì˜ íŒë‹¨ ë¡œì§ì´ í•©ë¦¬ì ì¸ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
í‰ê°€ ê¸°ì¤€:
- 1ì : ì™„ì „íˆ ì˜ëª»ëœ íŒë‹¨, ë…¼ë¦¬ì  ì˜¤ë¥˜
- 2ì : íŒë‹¨ì´ ë¶€ì •í™•í•˜ê³  ê·¼ê±°ê°€ ì•½í•¨
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ë§ëŠ” íŒë‹¨ì´ë‚˜ ë…¼ë¦¬ì— í—ˆì 
- 4ì : ëŒ€ì²´ë¡œ í•©ë¦¬ì  íŒë‹¨, ì‚¬ì†Œí•œ ê°œì„  ì—¬ì§€
- 5ì : ì™„ë²½íˆ í•©ë¦¬ì ì´ê³  ê·¼ê±°ê°€ ëª…í™•í•œ íŒë‹¨
JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

OUTPUT_COMPLETENESS_PROMPT = """ë‹¹ì‹ ì€ AI Agentì˜ ì¶œë ¥ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[Agentì˜ ì‘ë‹µ]:
{agent_response}
[ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•]: {scenario_type}
Agentì˜ ì‘ë‹µì´ ì™„ì „í•œì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
ì™„ì „í•œ ì‘ë‹µì˜ ì¡°ê±´: status í•„ë“œ, quote(íŒŒê´´ì‹œ), description, suggestion ë“± í¬í•¨ ì—¬ë¶€.
í‰ê°€ ê¸°ì¤€:
- 1ì : í•„ìˆ˜ í•„ë“œ ëŒ€ë¶€ë¶„ ëˆ„ë½
- 2ì : ì¼ë¶€ í•„ë“œë§Œ ì¡´ì¬
- 3ì : ê¸°ë³¸ êµ¬ì¡°ëŠ” ìˆìœ¼ë‚˜ ì„¸ë¶€ ì‚¬í•­ ë¶€ì¡±
- 4ì : ëŒ€ë¶€ë¶„ì˜ í•„ë“œ ì¡´ì¬
- 5ì : ëª¨ë“  í•„ìˆ˜ í•„ë“œê°€ ì™„ë²½íˆ í¬í•¨ë¨
JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""


def get_gemini_client():
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return genai.Client(api_key=api_key)


def judge_metric(client, prompt: str, max_retries: int = 3) -> Dict:
    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config={'response_mime_type': 'application/json', 'temperature': 0.1}
            )
            result = json.loads(response.text)
            score = int(result.get("score", 0))
            if 1 <= score <= 5:
                return {"score": score, "reason": result.get("reason", "")}
        except Exception as e:
            time.sleep(1)
    return {"score": 0, "reason": "ì±„ì  ì‹¤íŒ¨"}


def resolve_novel_id(novel_filename: str) -> Optional[int]:
    """íŒŒì¼ëª…ìœ¼ë¡œ novel_id ì¡°íšŒ (ê°•í™”ëœ ë§¤ì¹­ ì „ëµ)"""
    FILENAME_KR_MAP = {
        "alice": "ì•¨ë¦¬ìŠ¤", "jane": "ì œì¸", "sherlock": "ì…œë¡",
        "frankenstein": "í”„ë‘ì¼„", "jekyll": "ì§€í‚¬", "gatsby": "ê°œì¸ ë¹„",
        "pride": "ì˜¤ë§Œ", "dracula": "ë“œë¼í˜ë¼", "oz": "ì˜¤ì¦ˆ",
        "peterpan": "í”¼í„°íŒ¬", "treasure": "ë³´ë¬¼", "tomsawyer": "í†°",
        "mobydick": "ëª¨ë¹„ë”•", "timemachine": "íƒ€ì„ë¨¸ì‹ ",
        "warofworlds": "ìš°ì£¼ì „ìŸ", "karamazov": "ì¹´ë¼ë§ˆì¡°í”„",
    }
    
    db = SessionLocal()
    try:
        clean_name = novel_filename.replace('.txt', '')
        parts = clean_name.split('_')
        keywords = [p.lower() for p in parts if len(p) > 2 and p != "KR"]
        
        # ì „ëµ 1: ì˜ë¬¸ â†’ í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in keywords:
            kr_keyword = FILENAME_KR_MAP.get(keyword)
            if kr_keyword:
                novel = db.query(Novel).filter(Novel.title.ilike(f"%{kr_keyword}%")).first()
                if novel: return novel.id
        
        # ì „ëµ 2: ì˜ë¬¸ í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ ë§¤ì¹­
        for keyword in keywords:
            novel = db.query(Novel).filter(Novel.title.ilike(f"%{keyword}%")).first()
            if novel: return novel.id
            
        # ì „ëµ 3: í¬í•¨ ê²€ìƒ‰
        novel = db.query(Novel).filter(Novel.title.contains(clean_name)).first()
        if novel: return novel.id
        
        return None
    finally:
        db.close()


# ===== RAG Logic =====
def run_rag_pipeline(question: str, novel_filename: str) -> Dict:
    service = ChatbotService()
    novel_id = resolve_novel_id(novel_filename)
    
    top_chunks = service.hybrid_search(
        question=question,
        novel_id=novel_id,
        novel_filter=novel_filename if novel_id is None else None
    )
    
    if not top_chunks:
        top_chunks = service.find_similar_chunks(
            question=question, novel_id=novel_id, top_k=5,
            novel_filter=novel_filename if novel_id is None else None
        )
    
    if not top_chunks:
        return {"answer": "ë‚´ìš© ì—†ìŒ", "context": "", "chunks": [], "found_context": False}
    
    context_texts = [f"[Context {i+1}] {c['text']}" for i, c in enumerate(top_chunks)]
    context = "\n\n".join(context_texts)
    answer = service.generate_answer(question, context)
    
    return {
        "answer": answer, "context": context,
        "chunks": top_chunks, "found_context": True, "novel_id_resolved": novel_id
    }


def evaluate_rag(client, dataset: List[Dict]) -> List[Dict]:
    results = []
    print(f"\nğŸ“Š RAG í‰ê°€ ì‹œì‘ ({len(dataset)}ê°œ)")
    for qa in tqdm(dataset):
        rag_result = run_rag_pipeline(qa["question"], qa.get("novel_filename", ""))
        
        entry = {
            "question": qa["question"],
            "ground_truth": qa["answer"],
            "novel_filename": qa.get("novel_filename", ""),
            "category": qa.get("category", "unknown"),
            "rag_answer": rag_result["answer"],
            "metrics": {}
        }
        
        if rag_result["found_context"]:
            entry["context_found"] = True
            entry["context_preview"] = rag_result["context"][:300]
            
            # Metric Evaluation
            m = {}
            m["context_relevance"] = judge_metric(client, CONTEXT_RELEVANCE_PROMPT.format(
                question=qa["question"], context=rag_result["context"]))
            m["faithfulness"] = judge_metric(client, FAITHFULNESS_PROMPT.format(
                question=qa["question"], context=rag_result["context"], answer=rag_result["answer"]))
            m["answer_relevance"] = judge_metric(client, ANSWER_RELEVANCE_PROMPT.format(
                question=qa["question"], answer=rag_result["answer"]))
            m["answer_correctness"] = judge_metric(client, ANSWER_CORRECTNESS_PROMPT.format(
                question=qa["question"], ground_truth=qa["answer"], answer=rag_result["answer"]))
            entry["metrics"] = m
        else:
            entry["context_found"] = False
            zero = {"score": 0, "reason": "No context"}
            entry["metrics"] = {k: zero for k in ["context_relevance", "faithfulness", "answer_relevance", "answer_correctness"]}
            
        results.append(entry)
        time.sleep(0.5)
    return results


# ===== Agent Logic =====
def run_agent_check(input_text: str, novel_filename: str) -> Dict:
    api_key = os.getenv("GOOGLE_API_KEY")
    agent = StoryConsistencyAgent(api_key=api_key)
    novel_id = resolve_novel_id(novel_filename)
    
    if not novel_id:
        return {"status": "ì˜¤ë¥˜", "results": [], "context_used": "", "raw_response": {"error": "Novel not found"}}
    
    try:
        result = asyncio.run(agent.check_consistency(novel_id, input_text))
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(agent.check_consistency(novel_id, input_text))
        loop.close()
        
    search_results = agent.search_engine.search(input_text, novel_id=novel_id, top_k=5)
    context_str = "\n---\n".join([hit['document'].get('original_text', '')[:300] for hit in search_results])
    
    return {
        "status": result.get("status", "ì•Œ ìˆ˜ ì—†ìŒ"),
        "results": result.get("results", []),
        "context_used": context_str,
        "raw_response": result
    }


def evaluate_agent(client, dataset: List[Dict]) -> List[Dict]:
    results = []
    print(f"\nğŸ¤– Agent í‰ê°€ ì‹œì‘ ({len(dataset)}ê°œ)")
    for scenario in tqdm(dataset):
        agent_result = run_agent_check(scenario["input_text"], scenario.get("novel_filename", ""))
        
        agent_resp_str = json.dumps(agent_result["raw_response"], ensure_ascii=False)
        m = {}
        
        # LLM Metrics
        m["tool_use_accuracy"] = judge_metric(client, TOOL_USE_ACCURACY_PROMPT.format(
            input_text=scenario["input_text"], retrieved_context=agent_result["context_used"][:2000],
            explanation=scenario.get("explanation", "")))
        
        m["reasoning_quality"] = judge_metric(client, REASONING_QUALITY_PROMPT.format(
            input_text=scenario["input_text"], expected_status=scenario["expected_status"],
            scenario_type=scenario["scenario_type"], explanation=scenario.get("explanation", ""),
            agent_response=agent_resp_str[:2000]))
            
        m["output_completeness"] = judge_metric(client, OUTPUT_COMPLETENESS_PROMPT.format(
            agent_response=agent_resp_str[:2000], scenario_type=scenario["scenario_type"]))
            
        # Accuracy
        ex_st = scenario["expected_status"]
        ac_st = agent_result["status"]
        correct = False
        if "íŒŒê´´" in ex_st and "íŒŒê´´" in ac_st: correct = True
        elif "ì¼ì¹˜" in ex_st and ("ì¼ì¹˜" in ac_st or "ì¼ê´€" in ac_st): correct = True
        elif ex_st == ac_st: correct = True
        
        m["accuracy"] = {"correct": correct, "expected": ex_st, "actual": ac_st}
        
        results.append({
            "input_text": scenario["input_text"],
            "novel_filename": scenario.get("novel_filename", ""),
            "scenario_type": scenario["scenario_type"],
            "metrics": m,
            "agent_results_count": len(agent_result["results"]),
            "expected_status": ex_st,
            "actual_status": ac_st
        })
        time.sleep(0.5)
    return results


def compute_rag_summary(results):
    summary = {}
    for m in ["context_relevance", "faithfulness", "answer_relevance", "answer_correctness"]:
        scores = [r["metrics"][m]["score"] for r in results if r["metrics"][m]["score"] > 0]
        summary[m] = {
            "mean": round(sum(scores)/len(scores), 2) if scores else 0,
            "count": len(scores)
        }
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    by_category = {}
    for r in results:
        c = r.get("category", "unknown")
        if c not in by_category: by_category[c] = {k:[] for k in ["context_relevance", "faithfulness", "answer_relevance", "answer_correctness"]}
        for k in ["context_relevance", "faithfulness", "answer_relevance", "answer_correctness"]:
            s = r["metrics"][k]["score"]
            if s > 0: by_category[c][k].append(s)
            
    for c in by_category:
        for k in by_category[c]:
            vals = by_category[c][k]
            by_category[c][k] = round(sum(vals)/len(vals), 2) if vals else 0
    summary["by_category"] = by_category

    # ì†Œì„¤ë³„ í†µê³„
    by_novel = {}
    for r in results:
        n = r["novel_filename"]
        if n not in by_novel: by_novel[n] = {k:[] for k in ["context_relevance", "faithfulness", "answer_relevance", "answer_correctness"]}
        for k in ["context_relevance", "faithfulness", "answer_relevance", "answer_correctness"]:
            s = r["metrics"][k]["score"]
            if s > 0: by_novel[n][k].append(s)
            
    for n in by_novel:
        for k in by_novel[n]:
            vals = by_novel[n][k]
            by_novel[n][k] = round(sum(vals)/len(vals), 2) if vals else 0
    summary["by_novel"] = by_novel
            
    return summary


def compute_agent_summary(results):
    summary = {}
    for m in ["tool_use_accuracy", "reasoning_quality", "output_completeness"]:
        scores = [r["metrics"][m]["score"] for r in results if r["metrics"][m]["score"] > 0]
        summary[m] = {"mean": round(sum(scores)/len(scores), 2) if scores else 0}
        
    correct = sum(1 for r in results if r["metrics"]["accuracy"]["correct"])
    summary["accuracy"] = {"rate": round(correct/len(results), 4) if results else 0, "correct": correct, "total": len(results)}
    
    # ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•ë³„ í†µê³„
    by_scenario_type = {}
    for r in results:
        t = r["scenario_type"]
        if t not in by_scenario_type: by_scenario_type[t] = {"correct": 0, "total": 0}
        by_scenario_type[t]["total"] += 1
        if r["metrics"]["accuracy"]["correct"]: by_scenario_type[t]["correct"] += 1
        
    for t in by_scenario_type:
        by_scenario_type[t]["accuracy_rate"] = round(by_scenario_type[t]["correct"]/by_scenario_type[t]["total"], 4)
        
    summary["by_scenario_type"] = by_scenario_type
    
    by_novel = {}
    for r in results:
        n = r["novel_filename"]
        if n not in by_novel: by_novel[n] = {"correct": 0, "total": 0}
        by_novel[n]["total"] += 1
        if r["metrics"]["accuracy"]["correct"]: by_novel[n]["correct"] += 1
        
    for n in by_novel:
        by_novel[n]["accuracy_rate"] = round(by_novel[n]["correct"]/by_novel[n]["total"], 4)
        
    summary["by_novel"] = by_novel
    return summary


def main():
    parser = argparse.ArgumentParser(description="RAG & Agent í†µí•© í‰ê°€")
    parser.add_argument("--dataset", type=str, default="eval_dataset.json")
    parser.add_argument("--max-samples", type=int, default=None)
    args = parser.parse_args()
    
    if not os.path.exists(args.dataset):
        print(f"âŒ ë°ì´í„°ì…‹ '{args.dataset}'ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    with open(args.dataset, 'r', encoding='utf-8') as f:
        ds = json.load(f)
        
    client = get_gemini_client()
    
    # RAG Evaluation
    rag_data = ds.get("rag_eval", [])
    if args.max_samples: rag_data = rag_data[:args.max_samples]
    
    rag_results = evaluate_rag(client, rag_data)
    rag_summary = compute_rag_summary(rag_results)
    
    with open(RAG_OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump({"metadata": {"total_samples": len(rag_results)}, "summary": rag_summary, "details": rag_results}, f, ensure_ascii=False, indent=2)
        
    # Agent Evaluation
    agent_data = ds.get("agent_eval", [])
    if args.max_samples: agent_data = agent_data[:args.max_samples]
    
    agent_results = evaluate_agent(client, agent_data)
    agent_summary = compute_agent_summary(agent_results)
    
    with open(AGENT_OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump({"metadata": {"total_samples": len(agent_results)}, "summary": agent_summary, "details": agent_results}, f, ensure_ascii=False, indent=2)
        
    print(f"\nâœ… í†µí•© í‰ê°€ ì™„ë£Œ!")
    print(f"   RAG ê²°ê³¼: {RAG_OUTPUT_FILE}")
    print(f"   Agent ê²°ê³¼: {AGENT_OUTPUT_FILE}")
    print(f"   ì´ì œ 'python scripts/metrics_dashboard.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
