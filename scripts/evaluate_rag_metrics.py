"""
RAG ì§€í‘œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (LLM-as-a-Judge)
=========================================
ChatbotServiceì˜ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³ , Geminiê°€ 4ê°€ì§€ ì§€í‘œë¡œ ì±„ì í•©ë‹ˆë‹¤.

ì§€í‘œ:
1. Context Relevance (ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±) â€” ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ê´€ë ¨ ìˆëŠ”ê°€
2. Faithfulness (ì¶©ì‹¤ë„) â€” ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê·¼ê±°í•˜ëŠ”ê°€ (í™˜ê° ì—†ëŠ”ê°€)
3. Answer Relevance (ë‹µë³€ ê´€ë ¨ì„±) â€” ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ë‹µí•˜ëŠ”ê°€
4. Answer Correctness (ë‹µë³€ ì •í™•ë„) â€” ë‹µë³€ì´ ì •ë‹µê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ê°€

ì‚¬ìš©ë²•:
    python scripts/evaluate_rag_metrics.py --dataset eval_dataset.json
    python scripts/evaluate_rag_metrics.py --dataset eval_dataset.json --max-samples 5
"""

import os
import sys
import json
import time
import argparse
from typing import Dict, List, Optional
from tqdm import tqdm
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from google import genai
from dotenv import load_dotenv
from backend.core.config import settings

load_dotenv()

# ===== ì„¤ì • =====
OUTPUT_FILE = "rag_eval_results.json"


# ===== LLM-as-a-Judge í”„ë¡¬í”„íŠ¸ =====
CONTEXT_RELEVANCE_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]: {question}

[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸]:
{context}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- 1ì : ì™„ì „íˆ ë¬´ê´€í•œ ë‚´ìš©
- 2ì : ì•½ê°„ì˜ ê´€ë ¨ì„±ì€ ìˆì§€ë§Œ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ë¶€ì¡±
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ ìˆìœ¼ë‚˜ í•µì‹¬ ì •ë³´ ë¶€ì¡±
- 4ì : ëŒ€ë¶€ë¶„ ê´€ë ¨ ìˆê³  ë‹µë³€ì— ì¶©ë¶„í•œ ì •ë³´ í¬í•¨
- 5ì : ì§ˆë¬¸ì— ì •í™•íˆ ëŒ€ì‘í•˜ëŠ” ë†’ì€ ê´€ë ¨ì„±

JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

FAITHFULNESS_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ ì¶©ì‹¤ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]: {question}

[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸]:
{context}

[ìƒì„±ëœ ë‹µë³€]:
{answer}

ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê·¼ê±°í•˜ì—¬ ì‘ì„±ë˜ì—ˆëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í–ˆë‹¤ë©´(í™˜ê°/hallucination) ê°ì í•©ë‹ˆë‹¤.

í‰ê°€ ê¸°ì¤€:
- 1ì : ë‹µë³€ì˜ ëŒ€ë¶€ë¶„ì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ (ì‹¬ê°í•œ í™˜ê°)
- 2ì : ë§ì€ ë¶€ë¶„ì´ ì»¨í…ìŠ¤íŠ¸ì™€ ë¬´ê´€í•˜ê±°ë‚˜ ì§€ì–´ë‚¸ ë‚´ìš©
- 3ì : ì¼ë¶€ í™˜ê°ì´ ìˆìœ¼ë‚˜ í•µì‹¬ì€ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜
- 4ì : ê±°ì˜ ëª¨ë‘ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°, ì‚¬ì†Œí•œ ì¶”ë¡ ë§Œ í¬í•¨
- 5ì : ì™„ë²½íˆ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê·¼ê±°í•œ ë‹µë³€

JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

ANSWER_RELEVANCE_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]: {question}

[ìƒì„±ëœ ë‹µë³€]:
{answer}

ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì ì ˆíˆ ëŒ€ë‹µí•˜ëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- 1ì : ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ë‹µë³€
- 2ì : ì§ˆë¬¸ì˜ ì£¼ì œëŠ” ë§ì§€ë§Œ í•µì‹¬ì„ ë¹—ë‚˜ê°
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ë‹µë³€í•˜ë‚˜ ë¶ˆì™„ì „
- 4ì : ëŒ€ì²´ë¡œ ì ì ˆí•œ ë‹µë³€ì´ë‚˜ ì•½ê°„ì˜ ë³´ì™„ í•„ìš”
- 5ì : ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì™„ë²½í•˜ê²Œ ëŒ€ë‹µ

JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

ANSWER_CORRECTNESS_PROMPT = """ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]: {question}

[ì •ë‹µ (Ground Truth)]:
{ground_truth}

[ìƒì„±ëœ ë‹µë³€]:
{answer}

ìƒì„±ëœ ë‹µë³€ì´ ì •ë‹µê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
í‘œí˜„ ë°©ì‹ì´ ë‹¬ë¼ë„ í•µì‹¬ ì˜ë¯¸ê°€ ê°™ìœ¼ë©´ ë†’ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

í‰ê°€ ê¸°ì¤€:
- 1ì : ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš©
- 2ì : ì•½ê°„ì˜ ê´€ë ¨ì„±ë§Œ ìˆê³  í•µì‹¬ì´ ë‹¤ë¦„
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ë§ì§€ë§Œ ì¤‘ìš”í•œ ë¶€ë¶„ ëˆ„ë½ ë˜ëŠ” ì˜¤ë¥˜
- 4ì : í•µì‹¬ ì˜ë¯¸ëŠ” ì¼ì¹˜í•˜ë‚˜ ì„¸ë¶€ ì°¨ì´ ì¡´ì¬
- 5ì : ì •ë‹µê³¼ ì™„ì „íˆ ì¼ì¹˜ (í‘œí˜„ ì°¨ì´ëŠ” ë¬´ê´€)

JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""


def get_gemini_client():
    """Gemini API í´ë¼ì´ì–¸íŠ¸"""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return genai.Client(api_key=api_key)


def judge_metric(client, prompt: str, max_retries: int = 3) -> Dict:
    """
    Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ì§€í‘œ ì±„ì 
    
    Returns:
        {"score": int, "reason": str}
    """
    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config={
                    'response_mime_type': 'application/json',
                    'temperature': 0.1  # ì¼ê´€ëœ ì±„ì ì„ ìœ„í•´ ë‚®ì€ temperature
                }
            )
            result = json.loads(response.text)
            # ì ìˆ˜ ìœ íš¨ì„± ê²€ì‚¬
            score = int(result.get("score", 0))
            if 1 <= score <= 5:
                return {"score": score, "reason": result.get("reason", "")}
            else:
                print(f"  âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì ìˆ˜ ({score}), ì¬ì‹œë„...")
        except Exception as e:
            print(f"  âš ï¸ ì±„ì  ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
            time.sleep(1)
    
    return {"score": 0, "reason": "ì±„ì  ì‹¤íŒ¨"}


def resolve_novel_id(novel_filename: str) -> Optional[int]:
    """
    novel_filename(ì˜ˆ: "KR_fantasy_alice.txt")ìœ¼ë¡œ DBì˜ novel_idë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ì¡°íšŒ ì „ëµ (ìˆœì°¨ ì‹œë„):
    1. Novel.titleì— íŒŒì¼ëª… í‚¤ì›Œë“œ ë§¤ì¹­ (alice â†’ "ì•¨ë¦¬ìŠ¤", sherlock â†’ "ì…œë¡" ë“±)
    2. Novel.titleì— íŒŒì¼ëª… ì „ì²´ í¬í•¨ ì—¬ë¶€
    3. Chapter.titleì— íŒŒì¼ëª… í‚¤ì›Œë“œ ë§¤ì¹­
    """
    from backend.db.session import SessionLocal
    from backend.db.models import Novel, Chapter
    
    # íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ì˜ë¬¸ í‚¤ì›Œë“œ â†’ í•œêµ­ì–´ ë§¤í•‘
    FILENAME_KR_MAP = {
        "alice": "ì•¨ë¦¬ìŠ¤",
        "jane": "ì œì¸",
        "sherlock": "ì…œë¡",
        "frankenstein": "í”„ë‘ì¼„",
        "jekyll": "ì§€í‚¬",
        "gatsby": "ê°œì¸ ë¹„",
        "pride": "ì˜¤ë§Œ",
        "dracula": "ë“œë¼í˜ë¼",
        "oz": "ì˜¤ì¦ˆ",
        "peterpan": "í”¼í„°íŒ¬",
        "treasure": "ë³´ë¬¼",
        "tomsawyer": "í†°",
        "mobydick": "ëª¨ë¹„ë”•",
        "timemachine": "íƒ€ì„ë¨¸ì‹ ",
        "warofworlds": "ìš°ì£¼ì „ìŸ",
        "karamazov": "ì¹´ë¼ë§ˆì¡°í”„",
    }
    
    db = SessionLocal()
    try:
        clean_name = novel_filename.replace('.txt', '')
        
        # íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ: "KR_fantasy_alice" â†’ ["fantasy", "alice"]
        parts = clean_name.split('_')
        keywords = [p.lower() for p in parts if len(p) > 2 and p != "KR"]
        
        # ì „ëµ 1: ì˜ë¬¸ í‚¤ì›Œë“œ â†’ í•œêµ­ì–´ ë³€í™˜ í›„ Novel.title ë§¤ì¹­
        for keyword in keywords:
            kr_keyword = FILENAME_KR_MAP.get(keyword)
            if kr_keyword:
                novel = db.query(Novel).filter(
                    Novel.title.ilike(f"%{kr_keyword}%")
                ).first()
                if novel:
                    print(f"  [Novel ID] ì „ëµ 1 ì„±ê³µ: '{keyword}'â†’'{kr_keyword}' â†’ title='{novel.title}' â†’ novel_id={novel.id}")
                    return novel.id
        
        # ì „ëµ 2: ì˜ë¬¸ í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ Novel.title ê²€ìƒ‰ (ì˜ë¬¸ ì œëª©ì¼ ê²½ìš°)
        for keyword in keywords:
            novel = db.query(Novel).filter(
                Novel.title.ilike(f"%{keyword}%")
            ).first()
            if novel:
                print(f"  [Novel ID] ì „ëµ 2 ì„±ê³µ: í‚¤ì›Œë“œ '{keyword}' â†’ title='{novel.title}' â†’ novel_id={novel.id}")
                return novel.id
        
        # ì „ëµ 3: Novel.titleì— íŒŒì¼ëª… ì „ì²´ í¬í•¨ ê²€ìƒ‰
        novel = db.query(Novel).filter(
            Novel.title.contains(clean_name)
        ).first()
        if novel:
            print(f"  [Novel ID] ì „ëµ 3 ì„±ê³µ: ì§ì ‘ ë§¤ì¹­ â†’ novel_id={novel.id}")
            return novel.id
        
        # ì‹¤íŒ¨: ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        all_novels = db.query(Novel).all()
        print(f"  âš ï¸ [Novel ID] ë§¤ì¹­ ì‹¤íŒ¨! '{novel_filename}'ì— ëŒ€ì‘í•˜ëŠ” ì†Œì„¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"  ğŸ“‹ DBì— ë“±ë¡ëœ ì†Œì„¤ ëª©ë¡:")
        for n in all_novels[:10]:
            print(f"     - id={n.id}, title='{n.title}'")
        
        return None
        
    finally:
        db.close()


def run_rag_pipeline(question: str, novel_filename: str) -> Dict:
    """
    ChatbotServiceì˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Returns:
        {"answer": str, "context": str, "chunks": list, "found_context": bool}
    """
    from backend.services.chatbot_service import ChatbotService
    
    service = ChatbotService()
    
    # novel_filenameìœ¼ë¡œ novel_id ì¡°íšŒ (ê°•í™”ëœ ë§¤ì¹­)
    novel_id = resolve_novel_id(novel_filename)
    
    if novel_id is None:
        print(f"  âš ï¸ novel_idë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. novel_filterë¡œ í´ë°±í•©ë‹ˆë‹¤.")
    else:
        print(f"  âœ… novel_id={novel_id} í™•ì¸ë¨")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
    top_chunks = service.hybrid_search(
        question=question,
        novel_id=novel_id,
        # novel_idê°€ Noneì´ë©´ novel_filterë¡œ Pinecone ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì‹œë„
        novel_filter=novel_filename if novel_id is None else None
    )
    
    if not top_chunks:
        # í´ë°±: ì›ë³¸ ì¿¼ë¦¬ë¡œ ì§ì ‘ ê²€ìƒ‰
        top_chunks = service.find_similar_chunks(
            question=question,
            novel_id=novel_id,
            top_k=5,
            novel_filter=novel_filename if novel_id is None else None
        )
    
    if not top_chunks:
        return {
            "answer": "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "context": "",
            "chunks": [],
            "found_context": False
        }
    
    # ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
    context_texts = []
    for i, chunk in enumerate(top_chunks):
        header = f"[Context {i+1}]"
        if chunk.get('scene_index') is not None:
            header += f" Scene {chunk['scene_index']}"
        if chunk.get('summary'):
            header += f" ({chunk['summary']})"
        context_texts.append(f"{header}\n{chunk['text']}")
    
    context = "\n\n".join(context_texts)
    
    # ë‹µë³€ ìƒì„±
    answer = service.generate_answer(question, context)
    
    return {
        "answer": answer,
        "context": context,
        "chunks": [{"text": c.get("text", ""), "similarity": c.get("similarity", 0)} for c in top_chunks],
        "found_context": True,
        "novel_id_resolved": novel_id
    }


def evaluate_single_qa(client, qa: Dict, rag_result: Dict) -> Dict:
    """ë‹¨ì¼ QA ìŒì— ëŒ€í•´ 4ê°€ì§€ ì§€í‘œ ëª¨ë‘ í‰ê°€"""
    question = qa["question"]
    ground_truth = qa["answer"]
    context = rag_result["context"]
    answer = rag_result["answer"]
    
    metrics = {}
    
    # 1. Context Relevance
    prompt = CONTEXT_RELEVANCE_PROMPT.format(question=question, context=context)
    metrics["context_relevance"] = judge_metric(client, prompt)
    
    # 2. Faithfulness
    prompt = FAITHFULNESS_PROMPT.format(question=question, context=context, answer=answer)
    metrics["faithfulness"] = judge_metric(client, prompt)
    
    # 3. Answer Relevance
    prompt = ANSWER_RELEVANCE_PROMPT.format(question=question, answer=answer)
    metrics["answer_relevance"] = judge_metric(client, prompt)
    
    # 4. Answer Correctness
    prompt = ANSWER_CORRECTNESS_PROMPT.format(
        question=question, ground_truth=ground_truth, answer=answer
    )
    metrics["answer_correctness"] = judge_metric(client, prompt)
    
    return metrics


def compute_summary(results: List[Dict]) -> Dict:
    """ì „ì²´ ê²°ê³¼ì—ì„œ ìš”ì•½ í†µê³„ ê³„ì‚°"""
    metric_names = ["context_relevance", "faithfulness", "answer_relevance", "answer_correctness"]
    summary = {}
    
    for metric in metric_names:
        scores = [r["metrics"][metric]["score"] for r in results if r["metrics"][metric]["score"] > 0]
        if scores:
            summary[metric] = {
                "mean": round(sum(scores) / len(scores), 2),
                "min": min(scores),
                "max": max(scores),
                "count": len(scores)
            }
        else:
            summary[metric] = {"mean": 0, "min": 0, "max": 0, "count": 0}
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    category_stats = {}
    for r in results:
        cat = r.get("category", "unknown")
        if cat not in category_stats:
            category_stats[cat] = {m: [] for m in metric_names}
        for m in metric_names:
            score = r["metrics"][m]["score"]
            if score > 0:
                category_stats[cat][m].append(score)
    
    for cat, scores_dict in category_stats.items():
        for m in metric_names:
            scores = scores_dict[m]
            category_stats[cat][m] = round(sum(scores) / len(scores), 2) if scores else 0
    
    summary["by_category"] = category_stats
    
    # ì†Œì„¤ë³„ í†µê³„
    novel_stats = {}
    for r in results:
        novel = r.get("novel_filename", "unknown")
        if novel not in novel_stats:
            novel_stats[novel] = {m: [] for m in metric_names}
        for m in metric_names:
            score = r["metrics"][m]["score"]
            if score > 0:
                novel_stats[novel][m].append(score)
    
    for novel, scores_dict in novel_stats.items():
        for m in metric_names:
            scores = scores_dict[m]
            novel_stats[novel][m] = round(sum(scores) / len(scores), 2) if scores else 0
    
    summary["by_novel"] = novel_stats
    
    return summary


def main():
    parser = argparse.ArgumentParser(description="RAG ì§€í‘œ í‰ê°€ (LLM-as-a-Judge)")
    parser.add_argument("--dataset", type=str, default="eval_dataset.json", help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--max-samples", type=int, default=None, help="ìµœëŒ€ í‰ê°€ ìƒ˜í”Œ ìˆ˜")
    parser.add_argument("--output", type=str, default=OUTPUT_FILE, help="ê²°ê³¼ ì¶œë ¥ ê²½ë¡œ")
    args = parser.parse_args()
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    if not os.path.exists(args.dataset):
        print(f"âŒ ë°ì´í„°ì…‹ íŒŒì¼ ì—†ìŒ: {args.dataset}")
        print("   ë¨¼ì € python scripts/generate_eval_dataset.py ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    with open(args.dataset, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    rag_eval = dataset.get("rag_eval", [])
    if args.max_samples:
        rag_eval = rag_eval[:args.max_samples]
    
    print(f"ğŸ“Š RAG ì§€í‘œ í‰ê°€ (LLM-as-a-Judge)")
    print(f"   ë°ì´í„°ì…‹: {args.dataset}")
    print(f"   í‰ê°€ ìƒ˜í”Œ: {len(rag_eval)}ê°œ")
    print(f"   ì¶œë ¥: {args.output}")
    print()
    
    client = get_gemini_client()
    results = []
    
    for i, qa in enumerate(tqdm(rag_eval, desc="RAG í‰ê°€ ì¤‘")):
        print(f"\n[{i+1}/{len(rag_eval)}] Q: {qa['question'][:60]}...")
        
        # 1. RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        rag_result = run_rag_pipeline(qa["question"], qa.get("novel_filename", ""))
        
        if not rag_result["found_context"]:
            print(f"  âš ï¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            result_entry = {
                "question": qa["question"],
                "ground_truth": qa["answer"],
                "novel_filename": qa.get("novel_filename", ""),
                "category": qa.get("category", "unknown"),
                "rag_answer": rag_result["answer"],
                "context_found": False,
                "metrics": {
                    "context_relevance": {"score": 0, "reason": "ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"},
                    "faithfulness": {"score": 0, "reason": "ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"},
                    "answer_relevance": {"score": 0, "reason": "ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"},
                    "answer_correctness": {"score": 0, "reason": "ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"},
                }
            }
        else:
            # 2. LLM-as-a-Judge ì±„ì 
            metrics = evaluate_single_qa(client, qa, rag_result)
            
            result_entry = {
                "question": qa["question"],
                "ground_truth": qa["answer"],
                "novel_filename": qa.get("novel_filename", ""),
                "category": qa.get("category", "unknown"),
                "rag_answer": rag_result["answer"],
                "context_found": True,
                "context_preview": rag_result["context"][:500],
                "num_chunks": len(rag_result["chunks"]),
                "top_similarity": rag_result["chunks"][0]["similarity"] if rag_result["chunks"] else 0,
                "metrics": metrics
            }
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            scores = {k: v["score"] for k, v in metrics.items()}
            print(f"  â†’ ì ìˆ˜: {scores}")
        
        results.append(result_entry)
        
        # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
        time.sleep(0.5)
    
    # ìš”ì•½ í†µê³„ ê³„ì‚°
    summary = compute_summary(results)
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    output_data = {
        "metadata": {
            "evaluated_at": datetime.now().isoformat(),
            "total_samples": len(results),
            "dataset_source": args.dataset
        },
        "summary": summary,
        "details": results
    }
    
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ“Š RAG í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    for metric_name in ["context_relevance", "faithfulness", "answer_relevance", "answer_correctness"]:
        stats = summary.get(metric_name, {})
        mean = stats.get("mean", 0)
        bar = "â–ˆ" * int(mean) + "â–‘" * (5 - int(mean))
        print(f"  {metric_name:25s}: {bar} {mean}/5.0")
    
    print(f"\nâœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: {args.output}")


if __name__ == "__main__":
    main()
