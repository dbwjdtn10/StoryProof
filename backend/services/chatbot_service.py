"""
ì±—ë´‡ ì„œë¹„ìŠ¤ ëª¨ë“ˆ
==============
RAG (Retrieval-Augmented Generation) ê¸°ë°˜ ì†Œì„¤ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (Pinecone + BGE-M3 ì„ë² ë”©)
2. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„± (Google Gemini)
3. ì†Œì„¤ë³„ í•„í„°ë§ ì§€ì›

ë™ì‘ íë¦„:
ì‚¬ìš©ì ì§ˆë¬¸ â†’ ì„ë² ë”© ë³€í™˜ â†’ Pinecone ê²€ìƒ‰ â†’ ìœ ì‚¬ ì”¬ ì¶”ì¶œ â†’ Gemini ë‹µë³€ ìƒì„±
"""

from typing import Dict, List, Optional

from google import genai
from backend.core.config import settings
from backend.db.session import SessionLocal
from backend.db.models import Novel
from backend.services.analysis import EmbeddingSearchEngine


class ChatbotService:
    """
    RAG ê¸°ë°˜ ì±—ë´‡ ì„œë¹„ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì†Œì„¤ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ Pineconeì—ì„œ ìœ ì‚¬í•œ ì”¬ì„ ê²€ìƒ‰í•˜ê³ ,
    ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ Gemini LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Attributes:
        engine (EmbeddingSearchEngine): Pinecone ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ ì—”ì§„
        client (genai.Client): Google Gemini API í´ë¼ì´ì–¸íŠ¸
        DEFAULT_ALPHA (float): ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ë ˆê±°ì‹œ, í˜„ì¬ ë¯¸ì‚¬ìš©)
        DEFAULT_SIMILARITY_THRESHOLD (float): ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
    """
    
    # ê¸°ë³¸ ì„¤ì •ê°’ (í´ë˜ìŠ¤ ìƒìˆ˜)
    DEFAULT_ALPHA = 0.297  # ë ˆê±°ì‹œ íŒŒë¼ë¯¸í„° (Pineconeì—ì„œëŠ” ë¯¸ì‚¬ìš©)
    DEFAULT_SIMILARITY_THRESHOLD = 0.5  # ìœ ì‚¬ë„ ì„ê³„ê°’ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ë¥¼ ìœ„í•´ ìƒí–¥)

    def __init__(self):
        """
        ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        ì´ˆê¸°í™” ê³¼ì •:
        1. EmbeddingSearchEngine ë¡œë“œ (Pinecone ì—°ê²° + BGE-M3 ëª¨ë¸ ë¡œë“œ)
        2. Google Gemini API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        
        Note:
            - í™˜ê²½ ë³€ìˆ˜ GOOGLE_API_KEY, PINECONE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤
            - ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ì„œë¹„ìŠ¤ëŠ” ìƒì„±ë˜ì§€ë§Œ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤
        """
        # Step 1: ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        # EmbeddingSearchEngineì€ Pinecone ì—°ê²° ë° BGE-M3 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤
        if EmbeddingSearchEngine:
            try:
                self.engine = EmbeddingSearchEngine()
                print("âœ… ChatbotService: EmbeddingSearchEngine loaded")
            except Exception as e:
                print(f"âŒ ChatbotService: Failed to load EmbeddingSearchEngine: {e}")
                self.engine = None
        else:
            self.engine = None
        
        # Step 2: Google Gemini API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        # ë‹µë³€ ìƒì„±ì„ ìœ„í•œ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if settings.GOOGLE_API_KEY:
            self.client = genai.Client(api_key=settings.GOOGLE_API_KEY)
        else:
            self.client = None
            print("Warning: GOOGLE_API_KEY not set. LLM functionality will be disabled.")
    
    def find_similar_chunks(
        self,
        question: str,
        top_k: int = 5,
        alpha: float = DEFAULT_ALPHA,
        similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD,
        novel_id: Optional[int] = None,
        chapter_id: Optional[int] = None,
        novel_filter: Optional[str] = None
    ) -> List[Dict]:
        """
        ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì”¬(ì²­í¬)ì„ Pineconeì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        # ê²€ìƒ‰ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if not self.engine:
            return []
            
        # Step 1: novel_filterë¡œ ì†Œì„¤ ID ì¡°íšŒ (novel_idê°€ ì§ì ‘ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)
        if novel_id is None and novel_filter:
            db = SessionLocal()
            try:
                # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±° (ì˜ˆ: "alice.txt" â†’ "alice")
                search_term = novel_filter.replace('.txt', '')
                
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œëª©ìœ¼ë¡œ ì†Œì„¤ ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                novel = db.query(Novel).filter(Novel.title.ilike(f"%{search_term}%")).first()
                if novel:
                    novel_id = novel.id
                    print(f"ğŸ” Chatbot: Resolved novel_filter '{novel_filter}' to ID {novel_id} ({novel.title})")
                else:
                    print(f"âš ï¸ Chatbot: novel_filter '{novel_filter}' not found in DB")
            finally:
                db.close()
        elif novel_id:
            print(f"ğŸ” Chatbot: Using direct novel_id {novel_id}")
        
        # Step 2: Pinecone ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        try:
            results = self.engine.search(query=question, novel_id=novel_id, chapter_id=chapter_id, top_k=top_k)
            print(f"ğŸ” Chatbot: Found {len(results)} results (Novel: {novel_id}, Chapter Context: {chapter_id})")
            
            # Step 3: ê²°ê³¼ í¬ë§· ë³€í™˜ ë° í•„í„°ë§
            formatted_results = []
            for res in results:
                similarity = res['similarity']
                doc = res['document']
                scene_idx = doc.get('scene_index', '?')
                
                # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ì œì™¸
                if similarity < similarity_threshold:
                    print(f"  - [DROP] Scene {scene_idx}: similarity {similarity:.4f} < {similarity_threshold}")
                    continue
                
                print(f"  - [KEEP] Scene {scene_idx}: similarity {similarity:.4f}")
                    
                formatted_results.append({
                    'text': doc.get('original_text', ''),
                    'scene_index': doc.get('scene_index'),
                    'chapter_id': res.get('chapter_id'),
                    'summary': doc.get('summary'),
                    'novel_id': novel_id,
                    'similarity': similarity,
                    'original_similarity': similarity
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"Error during search: {e}")
            return []
    
    def generate_answer(self, question: str, context: str) -> str:
        """
        Google Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        RAG (Retrieval-Augmented Generation) ë°©ì‹:
        1. ê²€ìƒ‰ëœ ì”¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
        2. Geminiê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ ìƒì„±
        3. ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ LLMì˜ ì§€ì‹ í™œìš©
        
        í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:
        - ì—­í•  ì •ì˜: ì†Œì„¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸
        - ë‹µë³€ í˜•ì‹: [í•µì‹¬ ìš”ì•½] + [ìƒì„¸ ì„¤ëª…] 2ë‹¨ êµ¬ì¡°
        - ì»¨í…ìŠ¤íŠ¸: ê²€ìƒ‰ëœ ì”¬ë“¤ (ìµœëŒ€ 3,500ì)
        
        Args:
            question (str): ì‚¬ìš©ì ì§ˆë¬¸
            context (str): ê²€ìƒ‰ëœ ì”¬ë“¤ì˜ í…ìŠ¤íŠ¸ (ì—¬ëŸ¬ ì”¬ì´ ê²°í•©ëœ í˜•íƒœ)
            
        Returns:
            str: ìƒì„±ëœ ë‹µë³€ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
                 í˜•ì‹: [í•µì‹¬ ìš”ì•½]\n...\n\n[ìƒì„¸ ì„¤ëª…]\n...
                 
        Example:
            >>> service = ChatbotService()
            >>> context = "ì•¨ë¦¬ìŠ¤ëŠ” í† ë¼ë¥¼ ë”°ë¼ êµ¬ë©ìœ¼ë¡œ ë–¨ì–´ì¡Œë‹¤..."
            >>> answer = service.generate_answer(
            ...     question="ì•¨ë¦¬ìŠ¤ëŠ” ì–´ë””ë¡œ ë–¨ì–´ì¡Œë‚˜ìš”?",
            ...     context=context
            ... )
            >>> print(answer)
        """
        # Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not self.client:
            return "LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        # - ì»¨í…ìŠ¤íŠ¸ëŠ” 3,500ìë¡œ ì œí•œ (Gemini í† í° ì œí•œ ê³ ë ¤)
        # - ë‹µë³€ í˜•ì‹ì„ ëª…í™•íˆ ì§€ì •í•˜ì—¬ ì¼ê´€ëœ ì¶œë ¥ ìœ ë„
        # - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€: ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì§€ì‹ ì‚¬ìš© ê¸ˆì§€
        prompt = f"""ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ì¤‘ìš” ê·œì¹™ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€]
1. **ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ë§¥ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.**
2. **ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì™¸ë¶€ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
3. **ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ê²½ìš° "ì œê³µëœ ë¬¸ë§¥ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.**
4. **ê° ì£¼ì¥ì—ëŠ” ì–´ëŠ Contextì—ì„œ ê°€ì ¸ì™”ëŠ”ì§€ [Context N] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”.**

[ë‹µë³€ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”. ë‘ ì„¹ì…˜ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ì„ ë‘ì„¸ìš”.

[í•µì‹¬ ìš”ì•½]
(ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ì¶œì²˜ë¥¼ í‘œì‹œ) [Context N]

[ìƒì„¸ ì„¤ëª…]
(ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ê·¼ê±°. ê° ë¬¸ì¥ë§ˆë‹¤ ì¶œì²˜ í‘œì‹œ) [Context N]

ë¬¸ë§¥:
{context[:3500]}
        
ì§ˆë¬¸: {question}
        
ë‹µë³€:"""
        
        # Gemini API í˜¸ì¶œ
        # í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ë¥¼ ìœ„í•œ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •:
        # - temperature=0.1: ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± ë° ì‚¬ì‹¤ì„± í–¥ìƒ
        # - top_p=0.8: í™•ë¥  ë¶„í¬ ì œí•œìœ¼ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ì¦ê°€
        # - top_k=20: í›„ë³´ í† í° ì œí•œ
        try:
            response = self.client.models.generate_content(
                model=settings.GEMINI_CHAT_MODEL,  # ì˜ˆ: "gemini-2.5-flash"
                contents=prompt,
                config={
                    'temperature': 0.1,  # ë‚®ì€ temperatureë¡œ í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì†Œ
                    'top_p': 0.8,        # í™•ë¥  ë¶„í¬ ì œí•œ
                    'top_k': 20,         # í›„ë³´ í† í° ì œí•œ
                    'max_output_tokens': 1024
                }
            )
            return response.text
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def augment_query(self, question: str) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ í™•ì¥í•©ë‹ˆë‹¤.
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ í‚¤ì›Œë“œ, ë™ì˜ì–´, êµ¬ì²´ì ì¸ í‘œí˜„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        ì¤‘ìš”: ì›ë³¸ ì§ˆë¬¸ì„ ë°˜ë“œì‹œ ìœ ì§€í•˜ê³ , ì¶”ê°€ í‚¤ì›Œë“œë§Œ ë§ë¶™ì…ë‹ˆë‹¤.
        """
        if not self.client:
            return question

        prompt = f"""ë‹¹ì‹ ì€ ì†Œì„¤ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ í™•ì¥í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

[ì¤‘ìš” ì›ì¹™]
1. **ì›ë³¸ ì§ˆë¬¸ì„ ì ˆëŒ€ ì¶•ì•½í•˜ê±°ë‚˜ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”**
2. **ì›ë³¸ ì§ˆë¬¸ ë’¤ì— ê´€ë ¨ í‚¤ì›Œë“œë§Œ ì¶”ê°€í•˜ì„¸ìš”**
3. **ì§ˆë¬¸ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ë³´ì¡´í•˜ì„¸ìš”**

[ì‹œê°„ì  ìˆœì„œ í‚¤ì›Œë“œ ì¸ì‹]
- "ì²˜ìŒ", "ì²«", "ìµœì´ˆ" â†’ ì´ˆë°˜ ì”¬, ë“±ì¥, ì‹œì‘, ì²« ë²ˆì§¸, Scene 1-5, ì²« ë§Œë‚¨, ì²« ë“±ì¥
- "ë§ˆì§€ë§‰", "ìµœí›„", "ë" â†’ í›„ë°˜ ì”¬, ì¢…ë£Œ, ë§ˆë¬´ë¦¬, ë§ˆì§€ë§‰ Scene, ìµœí›„, ì¢…ê²°
- "ê°€ì¥", "ì œì¼", "ìµœê³ " â†’ ê°•ì¡°, ê·¹ë‹¨, ìµœìƒê¸‰, ê°€ì¥ ì¤‘ìš”í•œ
- "ìœ ì¼í•œ", "ë‹¨ í•˜ë‚˜" â†’ ë…íŠ¹, íŠ¹ë³„, ì˜¤ì§, ìœ ì¼

[í™•ì¥ ê·œì¹™]
1. **ì›ë³¸ ì§ˆë¬¸ ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€**
2. **ì‹œê°„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê´€ë ¨ í‘œí˜„ ì¶”ê°€** (ì˜ˆ: "ì²˜ìŒ" â†’ "ì²« ë²ˆì§¸, ìµœì´ˆ, ì´ˆë°˜, ì‹œì‘")
3. **í•µì‹¬ ì—”í‹°í‹°(ì¸ë¬¼ëª…, ì¥ì†Œëª…) ìœ ì§€**
4. **ë™ì˜ì–´ ë° ê´€ë ¨ í‘œí˜„ ì¶”ê°€**
5. **ì”¬ ìœ„ì¹˜ íŒíŠ¸ ì¶”ê°€** (ì‹œê°„ ì¡°ê±´ì´ ìˆëŠ” ê²½ìš°)

[ì¶œë ¥ í˜•ì‹]
ì›ë³¸ ì§ˆë¬¸ + ì¶”ê°€ í‚¤ì›Œë“œ (í•œ ì¤„, ì„¤ëª… ì—†ì´, ë”°ì˜´í‘œ ì—†ì´)

[ì˜ˆì‹œ]
ì…ë ¥: "ì•¨ë¦¬ìŠ¤ê°€ ì²˜ìŒ ë§Œë‚œ ì¸ë¬¼"
ì¶œë ¥: ì•¨ë¦¬ìŠ¤ê°€ ì²˜ìŒ ë§Œë‚œ ì¸ë¬¼ ì²« ë²ˆì§¸ ìµœì´ˆ ë“±ì¥ ì´ˆë°˜ ì‹œì‘ Scene 1 Scene 2 Scene 3 ì²« ë“±ì¥ì¸ë¬¼ ì²« ë§Œë‚¨

ì…ë ¥: "ë„ë¡œì‹œê°€ ì²˜ìŒë§Œë‚œ ì¸ë¬¼ì´ ëˆ„êµ¬ì–‘?"
ì¶œë ¥: ë„ë¡œì‹œê°€ ì²˜ìŒë§Œë‚œ ì¸ë¬¼ì´ ëˆ„êµ¬ì–‘ ë„ë¡œì‹œ ì²˜ìŒ ë§Œë‚œ ì¸ë¬¼ ì²« ë²ˆì§¸ ìµœì´ˆ ë“±ì¥ ì´ˆë°˜ ì‹œì‘ Scene 1 Scene 2 Scene 3 ì²« ë“±ì¥ì¸ë¬¼ ì²« ë§Œë‚¨

ì…ë ¥: "ì•¨ë¦¬ìŠ¤ê°€ ë–¨ì–´ì§„ ê³³"
ì¶œë ¥: ì•¨ë¦¬ìŠ¤ê°€ ë–¨ì–´ì§„ ê³³ ì¥ì†Œ ìœ„ì¹˜ í† ë¼êµ´ êµ¬ë© ë‚™í•˜ ë–¨ì–´ì§

ì…ë ¥: "í† ë¼ê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ë§"
ì¶œë ¥: í† ë¼ê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ë§ ìµœí›„ ì¢…ë£Œ ë í›„ë°˜ ë§ˆë¬´ë¦¬ ë§ˆì§€ë§‰ ëŒ€ì‚¬ ë§ˆì§€ë§‰ Scene

ì…ë ¥: "ê°€ì¥ ì¤‘ìš”í•œ ì‚¬ê±´"
ì¶œë ¥: ê°€ì¥ ì¤‘ìš”í•œ ì‚¬ê±´ ìµœê³  í•µì‹¬ ì£¼ìš” ê²°ì •ì  ì¤‘ëŒ€í•œ ì „í™˜ì  í´ë¼ì´ë§¥ìŠ¤

ì¶œë ¥:"""
        try:
            response = self.client.models.generate_content(
                model=settings.GEMINI_CHAT_MODEL,
                contents=prompt,
                config={
                    'temperature': 0.2,  # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± ìœ ì§€
                    'max_output_tokens': 200  # ì¶©ë¶„í•œ í‚¤ì›Œë“œ ìƒì„±
                }
            )
            augmented = response.text.strip()
            
            # ë”°ì˜´í‘œ ì œê±° (Geminiê°€ ê°€ë” ë”°ì˜´í‘œë¡œ ê°ì‹¸ëŠ” ê²½ìš° ëŒ€ë¹„)
            augmented = augmented.strip('"').strip("'")
            
            print(f"ğŸ§¬ Query Augmented: '{question}' -> '{augmented}'")
            return augmented
        except Exception as e:
            print(f"âš ï¸ Query Augmentation Failed: {e}")
            return question

    def ask(
        self,
        question: str,
        alpha: float = DEFAULT_ALPHA,
        similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD,
        novel_id: Optional[int] = None,
        chapter_id: Optional[int] = None,
        novel_filter: Optional[str] = None
    ) -> Dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        
        ê²€ìƒ‰ ì „ëµ:
        1. [ê°œì„ ] í•­ìƒ ì¿¼ë¦¬ í™•ì¥ ì ìš© (LLMìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ê°€)
        2. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ 1ì°¨ ê²€ìƒ‰
        3. ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¿¼ë¦¬ë¡œ 2ì°¨ ê²€ìƒ‰ (í´ë°±)
        
        ì˜ˆìƒ íš¨ê³¼: ìœ ì‚¬ë„ +8~15% í–¥ìƒ
        """
        # 1. [ê°œì„ ] í•­ìƒ ì¿¼ë¦¬ í™•ì¥ ì ìš©
        print(f"ğŸ” ì›ë³¸ ì§ˆë¬¸: '{question}'")
        augmented_query = self.augment_query(question)
        
        # í™•ì¥ì´ ì‹¤ì œë¡œ ì¼ì–´ë‚¬ëŠ”ì§€ í™•ì¸
        if augmented_query != question:
            print(f"âœ¨ í™•ì¥ëœ ì§ˆë¬¸: '{augmented_query}'")
            search_query = augmented_query
        else:
            print("â„¹ï¸ ì¿¼ë¦¬ í™•ì¥ ì‹¤íŒ¨ ë˜ëŠ” ë¶ˆí•„ìš”, ì›ë³¸ ì‚¬ìš©")
            search_query = question
        
        # 2. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ 1ì°¨ ê²€ìƒ‰
        top_chunks = self.find_similar_chunks(
            question=search_query,
            top_k=5,
            alpha=alpha,
            similarity_threshold=similarity_threshold,
            novel_id=novel_id,
            chapter_id=chapter_id,
            novel_filter=novel_filter
        )
        
        # 3. ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¿¼ë¦¬ë¡œ 2ì°¨ ê²€ìƒ‰ (í´ë°±)
        if not top_chunks and search_query != question:
            print("âš ï¸ í™•ì¥ ì¿¼ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ë¡œ ì¬ì‹œë„...")
            top_chunks = self.find_similar_chunks(
                question=question,  # ì›ë³¸ ì¿¼ë¦¬
                top_k=5,
                alpha=alpha,
                similarity_threshold=similarity_threshold,
                novel_id=novel_id,
                chapter_id=chapter_id,
                novel_filter=novel_filter
            )

        # 3. ì—¬ì „íˆ ìœ ì‚¬í•œ ìŠ¤í† ë¦¬ë³´ë“œê°€ ì—†ëŠ” ê²½ìš°
        if not top_chunks:
            error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            if not self.engine:
                error_msg += " (ê²€ìƒ‰ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"
            
            return {
                "answer": error_msg,
                "source": None,
                "similarity": 0.0,
                "found_context": False
            }
        
        # 4. ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ìƒìœ„ ì²­í¬ í…ìŠ¤íŠ¸ ê²°í•©)
        context_texts = []
        for i, chunk in enumerate(top_chunks):
            # ì”¬ ë²ˆí˜¸ë‚˜ ìš”ì•½ì´ ìˆìœ¼ë©´ í¬í•¨
            header = f"[Context {i+1}]"
            if chunk.get('scene_index') is not None:
                header += f" Scene {chunk['scene_index']}"
            if chunk.get('summary'):
                header += f" (Summary: {chunk['summary']})"
                
            context_texts.append(f"{header}\n{chunk['text']}")
        
        context = "\n\n".join(context_texts)
        
        # 4. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer = self.generate_answer(question, context)
        
        # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì •ë³´
        best_chunk = top_chunks[0]
        
        # novel title ê°€ì ¸ì˜¤ê¸°
        novel_title = "Unknown Novel"
        if best_chunk.get('novel_id'):
            db = SessionLocal()
            try:
                novel = db.query(Novel).filter(Novel.id == best_chunk['novel_id']).first()
                if novel:
                    novel_title = novel.title
            finally:
                db.close()

        return {
            "answer": answer,
            "source": {
                "filename": novel_title,
                "chapter_id": best_chunk.get('chapter_id'),
                "scene_index": best_chunk.get('scene_index'),
                "summary": best_chunk.get('summary'),
                "total_scenes": len(top_chunks)
            },
            "similarity": best_chunk.get('similarity', 0.0), # similarity might be missing in some cases if not careful
            "found_context": True
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_chatbot_service = None


def get_chatbot_service() -> ChatbotService:
    """
    ì±—ë´‡ ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Returns:
        ChatbotService: ì±—ë´‡ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    """
    global _chatbot_service
    if _chatbot_service is None:
        _chatbot_service = ChatbotService()
    return _chatbot_service

