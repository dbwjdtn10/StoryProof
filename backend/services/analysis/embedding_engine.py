"""
ì„ë² ë”© ë° ê²€ìƒ‰ ì—”ì§„ ëª¨ë“ˆ
BAAI/bge-m3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± ë° Pinecone ë²¡í„° ê²€ìƒ‰
"""


import re
from typing import List, Dict, Any, Optional
import numpy as np
from rank_bm25 import BM25Okapi

from backend.core.config import settings
from backend.db.session import SessionLocal
from backend.db.models import VectorDocument

# ì „ì—­ ëª¨ë¸ ë° ì¸ë±ìŠ¤ ìºì‹œ (ì‹±ê¸€í†¤)
_global_model = None
_global_reranker = None
_global_kiwi = None
_global_bm25_map = {}  # novel_id -> BM25Okapi
_global_corpus_indices_map = {}  # novel_id -> doc_id list


class EmbeddingSearchEngine:
    """ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ (Dual Model + Parent-Child Indexing) + Hybrid Search"""
    
    def __init__(self):
        """
        ì´ˆê¸°í™”: ëª¨ë¸ì€ ì§€ì—° ë¡œë”© ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•  ë•Œ ë¡œë“œí•©ë‹ˆë‹¤.
        ë‹¨ì¼ ëª¨ë¸(dragonkue/multilingual-e5-small-ko)ë¡œ í†µí•© ìš´ì˜í•©ë‹ˆë‹¤.
        """
        self.model = None
        self.reranker = None
        self.kiwi = None
        self.pc = None
        self.index = None
        self.bm25 = None
        self.corpus_indices = None # BM25ìš© ì¸ë±ìŠ¤ ë§¤í•‘ (index -> doc_id)
        
        # ëª¨ë¸ ì´ë¦„ ì„¤ì • (í•œêµ­ì–´/ë‹¤êµ­ì–´ í†µí•© ì²˜ë¦¬)
        self.model_name = settings.KOREAN_EMBEDDING_MODEL
        
        # Pinecone ì„¤ì •
        self.pinecone_api_key = settings.PINECONE_API_KEY
        self.index_name = settings.PINECONE_INDEX_NAME
        
        # ì²­í‚¹ ì„¤ì •
        self.child_chunk_size = settings.CHILD_CHUNK_SIZE
        self.child_chunk_overlap = settings.CHILD_CHUNK_OVERLAP

        # ì´ˆê¸° Pinecone ì—°ê²° (ì¸ë±ìŠ¤ í™•ì¸ìš©)
        self._init_pinecone()
        
        # BM25ëŠ” ê²€ìƒ‰ ì‹œ novel_id ê¸°ì¤€ìœ¼ë¡œ lazy loading í•¨
        self.bm25_map = _global_bm25_map
        self.corpus_indices_map = _global_corpus_indices_map

    def _init_pinecone(self):
        """Pinecone í´ë¼ì´ì–¸íŠ¸ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        import sys
        import os
        try:
            # ëŸ°íƒ€ì„ ì§„ë‹¨ ì •ë³´ ì¶œë ¥ (ê°œë°œ ì‹œì—ë§Œ ìœ ìš©)
            try:
                import pinecone
                # print(f"[DEBUG] Pinecone Module: {getattr(pinecone, '__file__', 'Unknown')}")
                # print(f"[DEBUG] Pinecone Version: {getattr(pinecone, '__version__', 'Unknown')}")
            except:
                pass

            from pinecone import Pinecone
            self.pc = Pinecone(api_key=self.pinecone_api_key)
            
            # ì¸ë±ìŠ¤ í™•ì¸
            available_indexes = []
            try:
                # v3.0.0+ ë°©ì‹
                available_indexes = [idx.name for idx in self.pc.list_indexes()]
            except AttributeError:
                # v2.x í•˜ìœ„ í˜¸í™˜ì„± (list_indexesê°€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
                available_indexes = self.pc.list_indexes()

            if self.index_name not in available_indexes:
                print(f"[Warning] Pinecone ì¸ë±ìŠ¤ '{self.index_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"[Index List] ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤: {available_indexes}")
            else:
                self.index = self.pc.Index(self.index_name)
                print(f"[Success] Pinecone ì¸ë±ìŠ¤ ì—°ê²°: {self.index_name}")
        except Exception as e:
            error_msg = str(e)
            print(f"[Error] Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg}")
            # ë§Œì•½ íŒ¨í‚¤ì§€ ëª…ì¹­ ë³€ê²½ ê´€ë ¨ ì˜¤ë¥˜ë¼ë©´ ë” ëª…í™•í•œ í•´ê²° ê°€ì´ë“œ ì¶œë ¥
            if "renamed" in error_msg.lower():
                print("ğŸ’¡ í•´ê²° ë°©ë²•: í„°ë¯¸ë„ì—ì„œ 'pip uninstall pinecone-client pinecone' í›„ 'pip install pinecone'ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                print(f"í˜„ì¬ Python: {sys.executable}")
    
    def _init_bm25(self, novel_id: int):
        """
        íŠ¹ì • ì†Œì„¤(novel_id)ì˜ BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” (Global Singleton Map ì‚¬ìš©)
        """
        global _global_bm25_map, _global_corpus_indices_map
        
        if novel_id in _global_bm25_map:
            return
            
        print(f"[Info] Building BM25 Index for Novel {novel_id} (with Kiwi)...")
        kiwi = self._get_kiwi()
        db = SessionLocal()
        try:
            # í•´ë‹¹ ì†Œì„¤ì˜ Parent Scene í…ìŠ¤íŠ¸ë§Œ ë¡œë“œ
            docs = db.query(VectorDocument).filter(VectorDocument.novel_id == novel_id).all()
            
            corpus = []
            corpus_indices = []
            
            for doc in docs:
                text = doc.chunk_text
                if not text: continue
                
                # Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì ìš©
                tokens = [t.form for t in kiwi.tokenize(text)]
                corpus.append(tokens)
                corpus_indices.append(doc.vector_id)
            
            if corpus:
                bm25 = BM25Okapi(corpus)
                _global_bm25_map[novel_id] = bm25
                _global_corpus_indices_map[novel_id] = corpus_indices
                print(f"[Success] BM25 Index built for Novel {novel_id} with {len(corpus)} documents")
            else:
                print(f"[Warning] No documents found for BM25 (Novel {novel_id})")
                
        except Exception as e:
            print(f"[Error] Failed to build BM25 Index for Novel {novel_id}: {e}")
        finally:
            db.close()

    def _get_model(self):
        """ëª¨ë¸ ë¡œë“œ ë° ë°˜í™˜ (Lazy Loading & Singleton)"""
        from sentence_transformers import SentenceTransformer
        global _global_model
        
        if _global_model is None:
            print(f"[Info] ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            _global_model = SentenceTransformer(self.model_name)
            print(f"[Success] ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name}")
            
        self.model = _global_model
        return self.model

    def _get_reranker(self):
        """Reranker ë¡œë“œ ë° ë°˜í™˜ (Lazy Loading & Singleton)"""
        # ì„¤ì •ì—ì„œ Reranker ë¹„í™œì„±í™” ì‹œ None ë°˜í™˜
        if not settings.ENABLE_RERANKER:
            return None

        from sentence_transformers import CrossEncoder
        global _global_reranker
        
        # ì„¤ì •ëœ Reranker ëª¨ë¸ (ì—†ìœ¼ë©´ BAAI/bge-reranker-v2-m3 ì‚¬ìš©)
        reranker_name = getattr(settings, 'RERANKER_MODEL', "BAAI/bge-reranker-v2-m3")

        if _global_reranker is None:
            print(f"[Info] Reranker ë¡œë”© ì‹œì‘: {reranker_name}")
            _global_reranker = CrossEncoder(reranker_name, max_length=512)
            print(f"[Success] Reranker ë¡œë”© ì™„ë£Œ: {reranker_name}")
            
        self.reranker = _global_reranker
        return self.reranker

    def _get_kiwi(self):
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ë° ë°˜í™˜ (Lazy Loading & Singleton)"""
        from kiwipiepy import Kiwi
        global _global_kiwi
        
        if _global_kiwi is None:
            print(f"[Info] Kiwi Tokenizer ë¡œë”© ì‹œì‘...")
            _global_kiwi = Kiwi()
            print(f"[Success] Kiwi Tokenizer ë¡œë”© ì™„ë£Œ")
            
        self.kiwi = _global_kiwi
        return self.kiwi

    def warmup(self):
        """
        ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì²« ìš”ì²­ ì§€ì—°ì„ ë°©ì§€í•©ë‹ˆë‹¤.
        """
        print("[Warmup] EmbeddingSearchEngine: Preloading models...")
        try:
            self._get_model()    # SentenceTransformer ë¡œë“œ
            self._get_reranker() # CrossEncoder ë¡œë“œ
            self._get_kiwi()     # Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ
            print("[Warmup] EmbeddingSearchEngine: All models loaded successfully.")
        except Exception as e:
            print(f"[Error] EmbeddingSearchEngine Warmup Failed: {e}")

    def _split_into_child_chunks(self, text: str) -> List[str]:
        """Parent Sceneì„ ì§€ì •ëœ í¬ê¸°ì˜ Child Chunkë¡œ ë¶„í•  (Sliding Window)"""
        chunks = []
        if not text:
            return chunks
            
        step = self.child_chunk_size - self.child_chunk_overlap
        if step <= 0:
            step = 1
            
        for i in range(0, len(text), step):
            chunk = text[i:i + self.child_chunk_size]
            if len(chunk) < 50: # ë„ˆë¬´ ì§§ì€ ìíˆ¬ë¦¬ëŠ” ì œì™¸ (ì˜µì…˜)
                continue
            chunks.append(chunk)
            
        # í…ìŠ¤íŠ¸ê°€ ì§§ì•„ì„œ ì²­í¬ê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ì¶”ê°€
        if not chunks and text:
            chunks.append(text)
            
        return chunks

    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        model = self._get_model()
        embedding = model.encode(text, normalize_embeddings=True)
        return embedding.tolist()

    def add_documents(self, documents: List[Dict], novel_id: int, chapter_id: int):
        """
        Parent-Child Indexing ì „ëµ:
        1. DBì—ëŠ” Parent Scene ì „ì²´ ì €ì¥ (Bible/Viewìš©)
        2. Pineconeì—ëŠ” Child Chunk ì €ì¥ (Searchìš©)
        """
        print(f"\nğŸ“¥ {len(documents)}ê°œ ì”¬(Parent) ì²˜ë¦¬ ì¤‘... (Parent-Child Strategy)")
        
        db = SessionLocal()
        vectors_to_upsert = []
        
        try:
            # 0. í•´ë‹¹ ì±•í„°ì˜ ê¸°ì¡´ VectorDocument(Parent) ë° Child ë°ì´í„° ì‚­ì œ (ì´ˆê¸°í™”)
            # Pineconeì—ì„œë„ ì‚­ì œí•´ì•¼ í•˜ì§€ë§Œ, ID ê¸°ë°˜ ë®ì–´ì“°ê¸°ê°€ ìš°ì„ ì´ë¯€ë¡œ ì¼ë‹¨ DBë¶€í„° ì •ë¦¬
            # ë§Œì•½ ì”¬ ê°œìˆ˜ê°€ ì¤„ì–´ë“¤ ê²½ìš°ë¥¼ ìœ„í•´ ê¸°ì¡´ ì±•í„° ë°ì´í„° ì‚­ì œ
            db.query(VectorDocument).filter(
                VectorDocument.novel_id == novel_id,
                VectorDocument.chapter_id == chapter_id
            ).delete()
            db.commit()

            for doc in documents:
                scene_index = doc['scene_index']
                original_text = doc.get('original_text', '')
                summary = doc.get('summary', '') 
                
                # 1. DBì— Parent Scene ì €ì¥
                # ê³ ìœ  ID ìƒì„± (Parentìš©) - Chapter ID í¬í•¨í•˜ì—¬ ì¶©ëŒ ë°©ì§€
                parent_vector_id = f"novel_{novel_id}_chap_{chapter_id}_scene_{scene_index}"
                
                new_doc = VectorDocument(
                    novel_id=novel_id,
                    chapter_id=chapter_id,
                    vector_id=parent_vector_id,
                    chunk_index=scene_index,
                    chunk_text=original_text,
                    metadata_json=doc
                )
                db.add(new_doc)

                # 2. Child Chunk ìƒì„± (Pinecone ìš©)
                # ... (rest of the logic remains similar but uses new parent_vector_id)
                
                # 2. Child Chunk ìƒì„± ë° ì„ë² ë”© ì¤€ë¹„
                # [ê°œì„ ] ìš”ì•½ + ë³¸ë¬¸ ê²°í•©ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
                # ìš”ì•½ì—ëŠ” í•µì‹¬ í‚¤ì›Œë“œê°€ ì••ì¶•ë˜ì–´ ìˆì–´ ì§ˆë¬¸ê³¼ì˜ ì–´íœ˜ ë§¤ì¹­ í™•ë¥  ì¦ê°€
                # í…ŒìŠ¤íŠ¸ ê²°ê³¼: í‰ê·  ìœ ì‚¬ë„ +2.8~5.9% í–¥ìƒ
                
                # ìš”ì•½ì´ ìˆìœ¼ë©´ ìš”ì•½ì„ ì•ì— ì¶”ê°€ (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)
                if summary:
                    combined_text = f"[ìš”ì•½] {summary}\n\n{original_text}"
                else:
                    combined_text = original_text
                
                child_chunks = self._split_into_child_chunks(combined_text)
                
                for i, chunk_text in enumerate(child_chunks):
                    # Child Chunk ì„ë² ë”©
                    embedding = self.embed_text(chunk_text)
                    
                    # Child Vector ID
                    child_id = f"{parent_vector_id}_chunk_{i}"
                    
                    # Metadata (Parent ì¶”ì ìš©)
                    metadata = {
                        'novel_id': novel_id,
                        'chapter_id': chapter_id,
                        'scene_index': scene_index,
                        'type': 'child', # êµ¬ë¶„ì
                        'text': chunk_text, # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•˜ì´ë¼ì´íŠ¸ ë§¤ì¹­ìš©ìœ¼ë¡œ ì €ì¥
                        'chunk_index': i
                    }
                    
                    vectors_to_upsert.append({
                        'id': child_id,
                        'values': embedding,
                        'metadata': metadata
                    })
 
                if (scene_index + 1) % 5 == 0:
                    print(f"  Parent ì”¬ ì²˜ë¦¬ ì¤‘: {scene_index + 1}/{len(documents)}")
            
            # Pinecone ì—…ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬)
            if vectors_to_upsert:
                # ì¸ë±ìŠ¤ ì—°ê²° í™•ì¸ ë° ì¬ì‹œë„
                if self.index is None:
                    print("[Warning] Pinecone ì¸ë±ìŠ¤ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¬ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                    self._init_pinecone()
                    
                if self.index is None:
                    raise RuntimeError(f"Pinecone ì¸ë±ìŠ¤ '{self.index_name}'ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

                batch_size = 100
                print(f"[Action] ì´ {len(vectors_to_upsert)}ê°œì˜ Child Chunkë¥¼ Pineconeì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
                
                for i in range(0, len(vectors_to_upsert), batch_size):
                    batch = vectors_to_upsert[i:i + batch_size]
                    self.index.upsert(vectors=batch)
            
            db.commit()
            print("[Success] Pinecone ì—…ë¡œë“œ ë° DB ì €ì¥ ì™„ë£Œ")
            
            # BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ë¬¸ì„œ ì¶”ê°€ ì‹œ í•´ë‹¹ ì†Œì„¤ ì¸ë±ìŠ¤ ì‚­ì œ ìœ ë„)
            if novel_id in _global_bm25_map:
                del _global_bm25_map[novel_id]
                del _global_corpus_indices_map[novel_id]
            self._init_bm25(novel_id)
            
        except Exception as e:
            db.rollback()
            print(f"[Error] ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e
        finally:
            db.close()
    
    def search(
        self, 
        query: str, 
        novel_id: Optional[int] = None, 
        chapter_id: Optional[int] = None, 
        exclude_chapter_id: Optional[int] = None, 
        top_k: int = 5,
        alpha: float = 0.7, # 0.83 vs 0.7 ë¹„êµ ê²°ê³¼, ì‚¬ìš©ì ì œì•ˆê°’ì¸ 0.7ì„ ê¸°ë³¸ìœ¼ë¡œ ì±„íƒ (í‚¤ì›Œë“œ ë¹„ì¤‘ ê°•í™”)
        keywords: Optional[List[str]] = None,
        original_query: Optional[str] = None
    ):
        """
        True Hybrid Search (Union of Dense + Sparse)
        
        Args:
            query (str): ê²€ìƒ‰ ì§ˆë¬¸ (í™•ì¥ëœ ì¿¼ë¦¬ì¼ ìˆ˜ ìˆìŒ)
            novel_id (int): í•„í„°ë§í•  ì†Œì„¤ ID
            chapter_id (int): í•„í„°ë§í•  íšŒì°¨ ID
            exclude_chapter_id (int): ì œì™¸í•  íšŒì°¨ ID
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            alpha (float): ë°€ì§‘ ê²€ìƒ‰(Vector) ê°€ì¤‘ì¹˜ (0.0 ~ 1.0)
            keywords (List[str]): ëª…ì‹œì  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            original_query (str): ì›ë³¸ ì§ˆë¬¸ (ë¦¬ë­ì»¤ì—ì„œ ë…¸ì´ì¦ˆ ì—†ëŠ” ê²€ìƒ‰ì„ ìœ„í•´ ì‚¬ìš©)
        """
        # (Step 1-3 logic remains similar but updated)
        
        # --- 1. Dense Search (Pinecone) ---
        query_embedding = self.embed_text(query)
        
        filter_dict = {}
        if novel_id:
            filter_dict['novel_id'] = novel_id
        if chapter_id:
            filter_dict['chapter_id'] = chapter_id
        elif exclude_chapter_id:
            filter_dict['chapter_id'] = {"$ne": exclude_chapter_id}
        
        dense_results = self.index.query(
            vector=query_embedding,
            top_k=top_k * 10,
            include_metadata=True,
            filter=filter_dict if filter_dict else None
        )
        
        dense_matches = {m.id: m for m in dense_results.matches}
        
        # --- 2. Sparse Search (BM25) ---
        sparse_scores_dict = {}
        sparse_top_parents = []
        
        if novel_id:
            self._init_bm25(novel_id)
            bm25 = _global_bm25_map.get(novel_id)
            corpus_indices = _global_corpus_indices_map.get(novel_id)
            
            if bm25 and corpus_indices:
                if keywords:
                    tokenized_query = keywords
                else:
                    kiwi = self._get_kiwi()
                    tokenized_query = [t.form for t in kiwi.tokenize(query)]
            
                sparse_scores = bm25.get_scores(tokenized_query)
                
                if len(sparse_scores) > 0:
                    max_s = np.max(sparse_scores)
                    min_s = np.min(sparse_scores)
                    if max_s > min_s:
                        normalized_scores = (sparse_scores - min_s) / (max_s - min_s)
                    else:
                        normalized_scores = np.zeros_like(sparse_scores)
                    
                    for idx, norm_score in enumerate(normalized_scores):
                        parent_id = corpus_indices[idx]
                        sparse_scores_dict[parent_id] = float(norm_score)
                        if norm_score > 0:
                            sparse_top_parents.append((parent_id, norm_score))
                    
                    sparse_top_parents.sort(key=lambda x: x[1], reverse=True)
                    sparse_top_parents = sparse_top_parents[:top_k * 10]
        
        # --- 3. Union & Hybrid Scoring ---
        candidate_child_ids = set(dense_matches.keys())
        sparse_parent_ids_to_fetch = set()
        for p_id, _ in sparse_top_parents:
            found = any(c_id.startswith(p_id) for c_id in candidate_child_ids)
            if not found:
                sparse_parent_ids_to_fetch.add(p_id)
        
        if sparse_parent_ids_to_fetch:
            print(f"[Hybrid] Fetching {len(sparse_parent_ids_to_fetch)} sparse candidates from Pinecone...")
            for p_id in sparse_parent_ids_to_fetch:
                try:
                    parts = p_id.split('_')
                    s_idx = int(parts[parts.index('scene')+1])
                    c_id_filter = int(parts[parts.index('chap')+1])
                    
                    temp_res = self.index.query(
                        vector=query_embedding,
                        top_k=3, 
                        filter={"scene_index": s_idx, "novel_id": novel_id, "chapter_id": c_id_filter},
                        include_metadata=True
                    )
                    for t_match in temp_res.matches:
                        if t_match.id not in dense_matches:
                            dense_matches[t_match.id] = t_match
                except (ValueError, IndexError):
                    continue

        # --- 3. Result Merging & Scoring ---
        combined_candidates = self._merge_results(
            dense_matches=dense_matches,
            sparse_scores_dict=sparse_scores_dict,
            dense_weight=alpha,
            sparse_weight=(1.0 - alpha)
        )
        
        # --- 4. Reranking (Cross-Encoder) ---
        rerank_candidates = combined_candidates[:top_k * 10]
        final_results = []
        
        # ë¦¬ë­í‚¹ì—ëŠ” ì›ë³¸ ì§ˆë¬¸(original_query)ì„ ì‚¬ìš©í•˜ì—¬ ë…¸ì´ì¦ˆ ê°ì†Œ
        rank_query = original_query or query
        
        try:
            reranker = self._get_reranker()
            
            # Rerankerê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰
            if reranker:
                pairs = [[rank_query, m.metadata.get('text', '')] for m in rerank_candidates]
                
                if pairs:
                    # activation_fct=nn.Sigmoid() used internally if requested, 
                    # but we'll do it manually to ensure 0-1 range.
                    logits = reranker.predict(pairs)
                    
                    # Sigmoid function for normalization
                    def sigmoid(x):
                        return 1 / (1 + np.exp(-x))
                    
                    scores = sigmoid(logits)
                    
                    for i, match in enumerate(rerank_candidates):
                        match.score = float(scores[i])
                        final_results.append(match)
                    final_results.sort(key=lambda x: x.score, reverse=True)
                else:
                    final_results = rerank_candidates
            else:
                # Reranker ë¹„í™œì„±í™” ì‹œ Hybrid Score ê·¸ëŒ€ë¡œ ì‚¬ìš©
                # ë‹¨, Hybrid ScoreëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„(0-1)ì™€ BM25(0-1 ì •ê·œí™”)ì˜ ì¡°í•©ì´ë¯€ë¡œ
                # ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ë¬´ë°©í•˜ì§€ë§Œ, Reranker ì ìˆ˜ì™€ í˜¸í™˜ì„±ì„ ìœ„í•´ ìŠ¤ì¼€ì¼ë§ ê³ ë ¤ ê°€ëŠ¥
                # í˜„ì¬ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš© (Hybrid Score ìì²´ê°€ ì‹ ë¢°ë„ ì§€í‘œ)
                final_results = rerank_candidates
                
        except Exception as e:
            print(f"[Warning] Reranker failed: {e}. Fallback to Hybrid scores.")
            final_results = rerank_candidates

        # --- 5. Result Formatting & Parent Aggregation ---
        seen_keys = set()
        hits = []
        db = SessionLocal()
        
        try:
            for match in final_results:
                scene_index = int(match.metadata.get('scene_index'))
                match_chapter_id = match.metadata.get('chapter_id') or chapter_id
                
                key = (match_chapter_id, scene_index)
                if key in seen_keys: continue
                seen_keys.add(key)
                
                parent_vector_id = f"novel_{match.metadata.get('novel_id')}_chap_{match_chapter_id}_scene_{scene_index}"
                    
                doc = db.query(VectorDocument).filter(
                    VectorDocument.vector_id == parent_vector_id
                ).first()
                
                if doc:
                    scene_data = doc.metadata_json
                    scene_data['matched_chunk'] = match.metadata.get('text', '')
                    scene_data['similarity'] = match.score
                    
                    hits.append({
                        'document': scene_data,
                        'chapter_id': match_chapter_id,
                        'similarity': match.score,
                        'vector_id': match.id
                    })
                
                if len(hits) >= top_k:
                    break
        finally:
            db.close()
        
        return hits

    def _merge_results(
        self, 
        dense_matches: Dict[str, Any], 
        sparse_scores_dict: Dict[str, float],
        dense_weight: float = 0.7,
        sparse_weight: float = 0.3
    ) -> List[Any]:
        """
        ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì™€ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ê³  ê°€ì¤‘ì¹˜ì— ë”°ë¼ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        combined = []
        for c_id, match in dense_matches.items():
            parent_id = c_id.rsplit('_chunk_', 1)[0]
            dense_score = match.score
            sparse_score = sparse_scores_dict.get(parent_id, 0.0)
            
            # ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            match.score = (dense_weight * dense_score) + (sparse_weight * sparse_score)
            combined.append(match)
            
        # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        combined.sort(key=lambda x: x.score, reverse=True)
        return combined
