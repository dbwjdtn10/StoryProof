"""
ì„ë² ë”© ë° ê²€ìƒ‰ ì—”ì§„ ëª¨ë“ˆ
BAAI/bge-m3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± ë° Pinecone ë²¡í„° ê²€ìƒ‰
"""


import re
from typing import List, Dict, Any, Optional
import numpy as np
from rank_bm25 import BM25Okapi

from backend.core.config import settings
from backend.db.session import SessionLocal
from backend.db.models import VectorDocument

# ì „ì—­ ëª¨ë¸ ë° ì¸ë±ìŠ¤ ìºì‹œ (ì‹±ê¸€í†¤)
_global_model = None
_global_reranker = None
_global_kiwi = None
_global_bm25 = None
_global_corpus_indices = None


class EmbeddingSearchEngine:
    """ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ (Dual Model + Parent-Child Indexing) + Hybrid Search"""
    
    def __init__(self):
        """
        ì´ˆê¸°í™”: ëª¨ë¸ì€ ì§€ì—° ë¡œë”© ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•  ë•Œ ë¡œë“œí•©ë‹ˆë‹¤.
        ë‹¨ì¼ ëª¨ë¸(dragonkue/multilingual-e5-small-ko)ë¡œ í†µí•© ìš´ì˜í•©ë‹ˆë‹¤.
        """
        self.model = None
        self.reranker = None
        self.kiwi = None
        self.pc = None
        self.index = None
        self.bm25 = None
        self.corpus_indices = None # BM25ìš© ì¸ë±ìŠ¤ ë§¤í•‘ (index -> doc_id)
        
        # ëª¨ë¸ ì´ë¦„ ì„¤ì • (í•œêµ­ì–´/ë‹¤êµ­ì–´ í†µí•© ì²˜ë¦¬)
        self.model_name = settings.KOREAN_EMBEDDING_MODEL
        
        # Pinecone ì„¤ì •
        self.pinecone_api_key = settings.PINECONE_API_KEY
        self.index_name = settings.PINECONE_INDEX_NAME
        
        # ì²­í‚¹ ì„¤ì •
        self.child_chunk_size = settings.CHILD_CHUNK_SIZE
        self.child_chunk_overlap = settings.CHILD_CHUNK_OVERLAP

        # ì´ˆê¸° Pinecone ì—°ê²° (ì¸ë±ìŠ¤ í™•ì¸ìš©)
        self._init_pinecone()
        
        # BM25 ì´ˆê¸°í™” (In-Memory)
        self._init_bm25()

    def _init_pinecone(self):
        """Pinecone í´ë¼ì´ì–¸íŠ¸ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        import sys
        import os
        try:
            # ëŸ°íƒ€ì„ ì§„ë‹¨ ì •ë³´ ì¶œë ¥ (ê°œë°œ ì‹œì—ë§Œ ìœ ìš©)
            try:
                import pinecone
                # print(f"[DEBUG] Pinecone Module: {getattr(pinecone, '__file__', 'Unknown')}")
                # print(f"[DEBUG] Pinecone Version: {getattr(pinecone, '__version__', 'Unknown')}")
            except:
                pass

            from pinecone import Pinecone
            self.pc = Pinecone(api_key=self.pinecone_api_key)
            
            # ì¸ë±ìŠ¤ í™•ì¸
            available_indexes = []
            try:
                # v3.0.0+ ë°©ì‹
                available_indexes = [idx.name for idx in self.pc.list_indexes()]
            except AttributeError:
                # v2.x í•˜ìœ„ í˜¸í™˜ì„± (list_indexesê°€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
                available_indexes = self.pc.list_indexes()

            if self.index_name not in available_indexes:
                print(f"âš ï¸ Pinecone ì¸ë±ìŠ¤ '{self.index_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤: {available_indexes}")
            else:
                self.index = self.pc.Index(self.index_name)
                print(f"âœ… Pinecone ì¸ë±ìŠ¤ ì—°ê²°: {self.index_name}")
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg}")
            # ë§Œì•½ íŒ¨í‚¤ì§€ ëª…ì¹­ ë³€ê²½ ê´€ë ¨ ì˜¤ë¥˜ë¼ë©´ ë” ëª…í™•í•œ í•´ê²° ê°€ì´ë“œ ì¶œë ¥
            if "renamed" in error_msg.lower():
                print("ğŸ’¡ í•´ê²° ë°©ë²•: í„°ë¯¸ë„ì—ì„œ 'pip uninstall pinecone-client pinecone' í›„ 'pip install pinecone'ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                print(f"í˜„ì¬ Python: {sys.executable}")
    
    def _init_bm25(self):
        """
        BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” (Global Singleton ì‚¬ìš©)
        ì£¼ì˜: ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” Elasticsearch ë“±ì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, 
        í˜„ì¬ ê·œëª¨ì—ì„œëŠ” DBì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ì—¬ In-Memory BM25ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
        """
        global _global_bm25, _global_corpus_indices
        
        if _global_bm25 is not None:
            self.bm25 = _global_bm25
            self.corpus_indices = _global_corpus_indices
            print("âœ… BM25 Index loaded from cache")
            return

        print("ğŸ”„ Building BM25 Index from DB (with Kiwi)...")
        kiwi = self._get_kiwi()
        db = SessionLocal()
        try:
            # ëª¨ë“  Parent Sceneì˜ í…ìŠ¤íŠ¸ ë¡œë“œ (ChildëŠ” Parentì— í¬í•¨ë˜ë¯€ë¡œ Parent ê¸°ì¤€)
            # ë˜ëŠ” ê²€ìƒ‰ ì •í™•ë„ë¥¼ ìœ„í•´ Child ë‹¨ìœ„ë¡œ í•  ìˆ˜ë„ ìˆìŒ.
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ VectorDocument ì „ì²´ ë¡œë“œ
            docs = db.query(VectorDocument).all()
            
            corpus = []
            self.corpus_indices = []
            
            for doc in docs:
                text = doc.chunk_text
                if not text: continue
                
                # Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì ìš©
                tokens = [t.form for t in kiwi.tokenize(text)]
                corpus.append(tokens)
                self.corpus_indices.append(doc.vector_id)
            
            if corpus:
                self.bm25 = BM25Okapi(corpus)
                _global_bm25 = self.bm25
                _global_corpus_indices = self.corpus_indices
                print(f"âœ… BM25 Index built with {len(corpus)} documents")
            else:
                print("âš ï¸ No documents found for BM25")
                
        except Exception as e:
            print(f"âŒ Failed to build BM25 Index: {e}")
        finally:
            db.close()

    def _get_model(self):
        """ëª¨ë¸ ë¡œë“œ ë° ë°˜í™˜ (Lazy Loading & Singleton)"""
        from sentence_transformers import SentenceTransformer
        global _global_model
        
        if _global_model is None:
            print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            _global_model = SentenceTransformer(self.model_name)
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name}")
            
        self.model = _global_model
        return self.model

    def _get_reranker(self):
        """Reranker ë¡œë“œ ë° ë°˜í™˜ (Lazy Loading & Singleton)"""
        from sentence_transformers import CrossEncoder
        global _global_reranker
        
        # ì„¤ì •ëœ Reranker ëª¨ë¸ (ì—†ìœ¼ë©´ BAAI/bge-reranker-v2-m3 ì‚¬ìš©)
        reranker_name = getattr(settings, 'RERANKER_MODEL', "BAAI/bge-reranker-v2-m3")

        if _global_reranker is None:
            print(f"ğŸ”„ Reranker ë¡œë”© ì‹œì‘: {reranker_name}")
            _global_reranker = CrossEncoder(reranker_name, max_length=512)
            print(f"âœ… Reranker ë¡œë”© ì™„ë£Œ: {reranker_name}")
            
        self.reranker = _global_reranker
        return self.reranker

    def _get_kiwi(self):
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ë° ë°˜í™˜ (Lazy Loading & Singleton)"""
        from kiwipiepy import Kiwi
        global _global_kiwi
        
        if _global_kiwi is None:
            print(f"ğŸ”„ Kiwi Tokenizer ë¡œë”© ì‹œì‘...")
            _global_kiwi = Kiwi()
            print(f"âœ… Kiwi Tokenizer ë¡œë”© ì™„ë£Œ")
            
        self.kiwi = _global_kiwi
        return self.kiwi

    def _split_into_child_chunks(self, text: str) -> List[str]:
        """Parent Sceneì„ ì§€ì •ëœ í¬ê¸°ì˜ Child Chunkë¡œ ë¶„í•  (Sliding Window)"""
        chunks = []
        if not text:
            return chunks
            
        step = self.child_chunk_size - self.child_chunk_overlap
        if step <= 0:
            step = 1
            
        for i in range(0, len(text), step):
            chunk = text[i:i + self.child_chunk_size]
            if len(chunk) < 50: # ë„ˆë¬´ ì§§ì€ ìíˆ¬ë¦¬ëŠ” ì œì™¸ (ì˜µì…˜)
                continue
            chunks.append(chunk)
            
        # í…ìŠ¤íŠ¸ê°€ ì§§ì•„ì„œ ì²­í¬ê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ì¶”ê°€
        if not chunks and text:
            chunks.append(text)
            
        return chunks

    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        model = self._get_model()
        embedding = model.encode(text, normalize_embeddings=True)
        return embedding.tolist()

    def add_documents(self, documents: List[Dict], novel_id: int, chapter_id: int):
        """
        Parent-Child Indexing ì „ëµ:
        1. DBì—ëŠ” Parent Scene ì „ì²´ ì €ì¥ (Bible/Viewìš©)
        2. Pineconeì—ëŠ” Child Chunk ì €ì¥ (Searchìš©)
        """
        print(f"\nğŸ“¥ {len(documents)}ê°œ ì”¬(Parent) ì²˜ë¦¬ ì¤‘... (Parent-Child Strategy)")
        
        db = SessionLocal()
        vectors_to_upsert = []
        
        try:
            # 0. í•´ë‹¹ ì±•í„°ì˜ ê¸°ì¡´ VectorDocument(Parent) ë° Child ë°ì´í„° ì‚­ì œ (ì´ˆê¸°í™”)
            # Pineconeì—ì„œë„ ì‚­ì œí•´ì•¼ í•˜ì§€ë§Œ, ID ê¸°ë°˜ ë®ì–´ì“°ê¸°ê°€ ìš°ì„ ì´ë¯€ë¡œ ì¼ë‹¨ DBë¶€í„° ì •ë¦¬
            # ë§Œì•½ ì”¬ ê°œìˆ˜ê°€ ì¤„ì–´ë“¤ ê²½ìš°ë¥¼ ìœ„í•´ ê¸°ì¡´ ì±•í„° ë°ì´í„° ì‚­ì œ
            db.query(VectorDocument).filter(
                VectorDocument.novel_id == novel_id,
                VectorDocument.chapter_id == chapter_id
            ).delete()
            db.commit()

            for doc in documents:
                scene_index = doc['scene_index']
                original_text = doc.get('original_text', '')
                summary = doc.get('summary', '') 
                
                # 1. DBì— Parent Scene ì €ì¥
                # ê³ ìœ  ID ìƒì„± (Parentìš©) - Chapter ID í¬í•¨í•˜ì—¬ ì¶©ëŒ ë°©ì§€
                parent_vector_id = f"novel_{novel_id}_chap_{chapter_id}_scene_{scene_index}"
                
                new_doc = VectorDocument(
                    novel_id=novel_id,
                    chapter_id=chapter_id,
                    vector_id=parent_vector_id,
                    chunk_index=scene_index,
                    chunk_text=original_text,
                    metadata_json=doc
                )
                db.add(new_doc)

                # 2. Child Chunk ìƒì„± (Pinecone ìš©)
                # ... (rest of the logic remains similar but uses new parent_vector_id)
                
                # 2. Child Chunk ìƒì„± ë° ì„ë² ë”© ì¤€ë¹„
                # [ê°œì„ ] ìš”ì•½ + ë³¸ë¬¸ ê²°í•©ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
                # ìš”ì•½ì—ëŠ” í•µì‹¬ í‚¤ì›Œë“œê°€ ì••ì¶•ë˜ì–´ ìˆì–´ ì§ˆë¬¸ê³¼ì˜ ì–´íœ˜ ë§¤ì¹­ í™•ë¥  ì¦ê°€
                # í…ŒìŠ¤íŠ¸ ê²°ê³¼: í‰ê·  ìœ ì‚¬ë„ +2.8~5.9% í–¥ìƒ
                
                # ìš”ì•½ì´ ìˆìœ¼ë©´ ìš”ì•½ì„ ì•ì— ì¶”ê°€ (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)
                if summary:
                    combined_text = f"[ìš”ì•½] {summary}\n\n{original_text}"
                else:
                    combined_text = original_text
                
                child_chunks = self._split_into_child_chunks(combined_text)
                
                for i, chunk_text in enumerate(child_chunks):
                    # Child Chunk ì„ë² ë”©
                    embedding = self.embed_text(chunk_text)
                    
                    # Child Vector ID
                    child_id = f"{parent_vector_id}_chunk_{i}"
                    
                    # Metadata (Parent ì¶”ì ìš©)
                    metadata = {
                        'novel_id': novel_id,
                        'chapter_id': chapter_id,
                        'scene_index': scene_index,
                        'type': 'child', # êµ¬ë¶„ì
                        'text': chunk_text, # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•˜ì´ë¼ì´íŠ¸ ë§¤ì¹­ìš©ìœ¼ë¡œ ì €ì¥
                        'chunk_index': i
                    }
                    
                    vectors_to_upsert.append({
                        'id': child_id,
                        'values': embedding,
                        'metadata': metadata
                    })

                if (scene_index + 1) % 5 == 0:
                    print(f"  Parent ì”¬ ì²˜ë¦¬ ì¤‘: {scene_index + 1}/{len(documents)}")
            
            # Pinecone ì—…ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬)
            if vectors_to_upsert:
                # ì¸ë±ìŠ¤ ì—°ê²° í™•ì¸ ë° ì¬ì‹œë„
                if self.index is None:
                    print("âš ï¸ Pinecone ì¸ë±ìŠ¤ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¬ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                    self._init_pinecone()
                    
                if self.index is None:
                    raise RuntimeError(f"Pinecone ì¸ë±ìŠ¤ '{self.index_name}'ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

                batch_size = 100
                print(f"ğŸš€ ì´ {len(vectors_to_upsert)}ê°œì˜ Child Chunkë¥¼ Pineconeì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
                
                for i in range(0, len(vectors_to_upsert), batch_size):
                    batch = vectors_to_upsert[i:i + batch_size]
                    self.index.upsert(vectors=batch)
            
            db.commit()
            print("âœ… Pinecone ì—…ë¡œë“œ ë° DB ì €ì¥ ì™„ë£Œ")
            
            # BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ë¬¸ì„œ ì¶”ê°€ ì‹œ)
            self._init_bm25()
            
        except Exception as e:
            db.rollback()
            print(f"âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e
        finally:
            db.close()
    
    def search(
        self, 
        query: str, 
        novel_id: Optional[int] = None, 
        chapter_id: Optional[int] = None, 
        exclude_chapter_id: Optional[int] = None, 
        top_k: int = 5,
        alpha: float = 0.825 # ìµœì í™”ëœ ê¸°ë³¸ê°’ ì ìš©
    ):
        """
        Hybrid Search (Dense + Sparse)
        
        Args:
            query (str): ê²€ìƒ‰ ì§ˆë¬¸
            novel_id (int): í•„í„°ë§í•  ì†Œì„¤ ID
            chapter_id (int): í•„í„°ë§í•  íšŒì°¨ ID (ì„ íƒ - í¬í•¨ í•„í„°)
            exclude_chapter_id (int): ì œì™¸í•  íšŒì°¨ ID (ì„ íƒ - ì„¤ì • íŒŒê´´ ë¶„ì„ìš©)
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            alpha (float): ë°€ì§‘ ê²€ìƒ‰(Vector) ê°€ì¤‘ì¹˜ (0.0 ~ 1.0)
                           1.0 = Pure Vector, 0.0 = Pure Keyword
        """
        # 1. Dense Search (Pinecone)
        query_embedding = self.embed_text(query)
        
        # Pinecone í•„í„°
        filter_dict = {}
        if novel_id:
            filter_dict['novel_id'] = novel_id
        if chapter_id:
            filter_dict['chapter_id'] = chapter_id
        elif exclude_chapter_id:
            filter_dict['chapter_id'] = {"$ne": exclude_chapter_id}
        
        # í›„ë³´êµ° ê²€ìƒ‰ (Top-Kë³´ë‹¤ ë„‰ë„‰í•˜ê²Œ)
        candidate_k = top_k * 5
        
        dense_results = self.index.query(
            vector=query_embedding,
            top_k=candidate_k,
            include_metadata=True,
            filter=filter_dict if filter_dict else None
        )
        
        # 2. Sparse Search (BM25)
        # í˜„ì¬ëŠ” BM25ê°€ ì „ì²´ ë¬¸ì„œì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë§¤ê¸°ì§€ë§Œ, 
        # ì„±ëŠ¥ì„ ìœ„í•´ Dense í›„ë³´êµ°ì— ëŒ€í•´ì„œë§Œ Re-rankingí•˜ê±°ë‚˜,
        # ì „ì²´ì—ì„œ Top-Kë¥¼ ë½‘ì•„ êµì§‘í•©ì„ ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ: ì „ì²´ BM25 ì ìˆ˜ë¥¼ êµ¬í•˜ê³  Normalize
        
        sparse_scores_dict = {}
        if self.bm25:
            # Queryë„ ë™ì¼í•˜ê²Œ Kiwi í† í°í™”
            kiwi = self._get_kiwi()
            tokenized_query = [t.form for t in kiwi.tokenize(query)]
            sparse_scores = self.bm25.get_scores(tokenized_query)
            
            # ì •ê·œí™” (Min-Max)
            if len(sparse_scores) > 0:
                max_score = np.max(sparse_scores)
                min_score = np.min(sparse_scores)
                if max_score > min_score:
                    sparse_scores = (sparse_scores - min_score) / (max_score - min_score)
                else:
                    sparse_scores = np.zeros_like(sparse_scores)
            
            # ID ë§¤í•‘
            for idx, score in enumerate(sparse_scores):
                vector_id = self.corpus_indices[idx]
                sparse_scores_dict[vector_id] = score
        
        # 3. Score Fusion (Hybrid) -> Get Top-50 Candidates
        
        candidates = []
        for match in dense_results.matches:
            child_id = match.id
            parent_id = child_id.rsplit('_chunk_', 1)[0]
            
            dense_score = match.score
            sparse_score = sparse_scores_dict.get(parent_id, 0.0) 
            
            # Hybrid Score
            hybrid_score = (alpha * dense_score) + ((1 - alpha) * sparse_score)
            
            match.score = hybrid_score
            candidates.append(match)
            
        # ì¬ì •ë ¬ (Hybrid Score ê¸°ì¤€)
        candidates.sort(key=lambda x: x.score, reverse=True)
        
        # 4. Reranking (Cross-Encoder)
        # ìƒìœ„ 50ê°œ(ë˜ëŠ” top_k * 10)ë§Œ Rerankerì— íƒœì›€
        
        rerank_candidates = candidates[:top_k * 10]
        
        final_results = []
        
        # Rerankerê°€ ìˆìœ¼ë©´ ìˆ˜í–‰, ì—†ìœ¼ë©´ ìƒëµ
        try:
            reranker = self._get_reranker()
            
            # ì…ë ¥ ìŒ ìƒì„±: [[Query, Candidate_Text], ...]
            # ì£¼ì˜: Candidate Textê°€ ê¸¸ë©´ ì˜ë¦´ ìˆ˜ ìˆìŒ (Max 512 token)
            pairs = []
            for match in rerank_candidates:
                candidate_text = match.metadata.get('text', '')
                pairs.append([query, candidate_text])
                
            if pairs:
                scores = reranker.predict(pairs)
                
                # ì ìˆ˜ ì—…ë°ì´íŠ¸
                for i, match in enumerate(rerank_candidates):
                    match.score = float(scores[i]) # numpy float -> python float
                    final_results.append(match)
                    
                # Reranker ì ìˆ˜ ê¸°ì¤€ ì¬ì •ë ¬
                final_results.sort(key=lambda x: x.score, reverse=True)
            else:
                final_results = rerank_candidates
                
        except Exception as e:
            print(f"âš ï¸ Reranker failed: {e}. Fallback to Hybrid scores.")
            final_results = rerank_candidates

        # 5. Result Formatting & Parent Aggregation
        # Hybrid Searchë§Œ í–ˆì„ ë•ŒëŠ” candidates ì „ì²´ë¥¼ ì¼ì§€ë§Œ,
        # Reranking í›„ì—ëŠ” final_results (Top 50)ë§Œ ì‚¬ìš©
        
        seen_keys = set()
        hits = []
        db = SessionLocal()
        
        try:
            for match in final_results:
                scene_index = int(match.metadata.get('scene_index'))
                match_chapter_id = match.metadata.get('chapter_id') or chapter_id
                
                key = (match_chapter_id, scene_index)
                if key in seen_keys: continue
                seen_keys.add(key)
                
                if match_chapter_id:
                    parent_vector_id = f"novel_{novel_id}_chap_{match_chapter_id}_scene_{scene_index}"
                else:
                    parent_vector_id = f"novel_{novel_id}_scene_{scene_index}"
                    
                doc = db.query(VectorDocument).filter(
                    VectorDocument.vector_id == parent_vector_id
                ).first()
                
                if doc:
                    scene_data = doc.metadata_json
                    scene_data['matched_chunk'] = match.metadata.get('text', '')
                    scene_data['similarity'] = match.score # Reranker Score
                    
                    hits.append({
                        'document': scene_data,
                        'chapter_id': chapter_id,
                        'similarity': match.score,
                        'vector_id': match.id
                    })
                
                if len(hits) >= top_k:
                    break
        finally:
            db.close()
        
        return hits
