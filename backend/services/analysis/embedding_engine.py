"""
ì„ë² ë”© ë° ê²€ìƒ‰ ì—”ì§„ ëª¨ë“ˆ
BAAI/bge-m3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± ë° Pinecone ë²¡í„° ê²€ìƒ‰
"""

import re
from typing import List, Dict, Any, Optional

from backend.core.config import settings
from backend.db.session import SessionLocal
from backend.db.models import VectorDocument

# ì „ì—­ ëª¨ë¸ ìºì‹œ (ì‹±ê¸€í†¤)
_global_model = None


class EmbeddingSearchEngine:
    """ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ (Dual Model + Parent-Child Indexing)"""
    
    def __init__(self):
        """
        ì´ˆê¸°í™”: ëª¨ë¸ì€ ì§€ì—° ë¡œë”© ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•  ë•Œ ë¡œë“œí•©ë‹ˆë‹¤.
        ë‹¨ì¼ ëª¨ë¸(dragonkue/multilingual-e5-small-ko)ë¡œ í†µí•© ìš´ì˜í•©ë‹ˆë‹¤.
        """
        self.model = None
        self.pc = None
        self.index = None
        
        # ëª¨ë¸ ì´ë¦„ ì„¤ì • (í•œêµ­ì–´/ë‹¤êµ­ì–´ í†µí•© ì²˜ë¦¬)
        self.model_name = settings.KOREAN_EMBEDDING_MODEL
        
        # Pinecone ì„¤ì •
        self.pinecone_api_key = settings.PINECONE_API_KEY
        self.index_name = settings.PINECONE_INDEX_NAME
        
        # ì²­í‚¹ ì„¤ì •
        self.child_chunk_size = settings.CHILD_CHUNK_SIZE
        self.child_chunk_overlap = settings.CHILD_CHUNK_OVERLAP

        # ì´ˆê¸° Pinecone ì—°ê²° (ì¸ë±ìŠ¤ í™•ì¸ìš©)
        self._init_pinecone()

    def _init_pinecone(self):
        """Pinecone í´ë¼ì´ì–¸íŠ¸ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        import sys
        import os
        try:
            # ëŸ°íƒ€ì„ ì§„ë‹¨ ì •ë³´ ì¶œë ¥ (ê°œë°œ ì‹œì—ë§Œ ìœ ìš©)
            try:
                import pinecone
                # print(f"[DEBUG] Pinecone Module: {getattr(pinecone, '__file__', 'Unknown')}")
                # print(f"[DEBUG] Pinecone Version: {getattr(pinecone, '__version__', 'Unknown')}")
            except:
                pass

            from pinecone import Pinecone
            self.pc = Pinecone(api_key=self.pinecone_api_key)
            
            # ì¸ë±ìŠ¤ í™•ì¸
            if self.index_name not in [idx.name for idx in self.pc.list_indexes()]:
                print(f"âš ï¸ Pinecone ì¸ë±ìŠ¤ '{self.index_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                self.index = self.pc.Index(self.index_name)
                print(f"âœ… Pinecone ì¸ë±ìŠ¤ ì—°ê²°: {self.index_name}")
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg}")
            # ë§Œì•½ íŒ¨í‚¤ì§€ ëª…ì¹­ ë³€ê²½ ê´€ë ¨ ì˜¤ë¥˜ë¼ë©´ ë” ëª…í™•í•œ í•´ê²° ê°€ì´ë“œ ì¶œë ¥
            if "renamed" in error_msg.lower():
                print("ğŸ’¡ í•´ê²° ë°©ë²•: í„°ë¯¸ë„ì—ì„œ 'pip uninstall pinecone-client pinecone' í›„ 'pip install pinecone'ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                print(f"í˜„ì¬ Python: {sys.executable}")

    def _get_model(self):
        """ëª¨ë¸ ë¡œë“œ ë° ë°˜í™˜ (Lazy Loading & Singleton)"""
        from sentence_transformers import SentenceTransformer
        global _global_model
        
        if _global_model is None:
            print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            _global_model = SentenceTransformer(self.model_name)
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name}")
            
        self.model = _global_model
        return self.model

    def _split_into_child_chunks(self, text: str) -> List[str]:
        """Parent Sceneì„ ì§€ì •ëœ í¬ê¸°ì˜ Child Chunkë¡œ ë¶„í•  (Sliding Window)"""
        chunks = []
        if not text:
            return chunks
            
        step = self.child_chunk_size - self.child_chunk_overlap
        if step <= 0:
            step = 1
            
        for i in range(0, len(text), step):
            chunk = text[i:i + self.child_chunk_size]
            if len(chunk) < 50: # ë„ˆë¬´ ì§§ì€ ìíˆ¬ë¦¬ëŠ” ì œì™¸ (ì˜µì…˜)
                continue
            chunks.append(chunk)
            
        # í…ìŠ¤íŠ¸ê°€ ì§§ì•„ì„œ ì²­í¬ê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ì¶”ê°€
        if not chunks and text:
            chunks.append(text)
            
        return chunks

    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        model = self._get_model()
        embedding = model.encode(text, normalize_embeddings=True)
        return embedding.tolist()

    def add_documents(self, documents: List[Dict], novel_id: int, chapter_id: int):
        """
        Parent-Child Indexing ì „ëµ:
        1. DBì—ëŠ” Parent Scene ì „ì²´ ì €ì¥ (Bible/Viewìš©)
        2. Pineconeì—ëŠ” Child Chunk ì €ì¥ (Searchìš©)
        """
        print(f"\nğŸ“¥ {len(documents)}ê°œ ì”¬(Parent) ì²˜ë¦¬ ì¤‘... (Parent-Child Strategy)")
        
        db = SessionLocal()
        vectors_to_upsert = []
        
        try:
            # 0. í•´ë‹¹ ì±•í„°ì˜ ê¸°ì¡´ VectorDocument(Parent) ë° Child ë°ì´í„° ì‚­ì œ (ì´ˆê¸°í™”)
            # Pineconeì—ì„œë„ ì‚­ì œí•´ì•¼ í•˜ì§€ë§Œ, ID ê¸°ë°˜ ë®ì–´ì“°ê¸°ê°€ ìš°ì„ ì´ë¯€ë¡œ ì¼ë‹¨ DBë¶€í„° ì •ë¦¬
            # ë§Œì•½ ì”¬ ê°œìˆ˜ê°€ ì¤„ì–´ë“¤ ê²½ìš°ë¥¼ ìœ„í•´ ê¸°ì¡´ ì±•í„° ë°ì´í„° ì‚­ì œ
            db.query(VectorDocument).filter(
                VectorDocument.novel_id == novel_id,
                VectorDocument.chapter_id == chapter_id
            ).delete()
            db.commit()

            for doc in documents:
                scene_index = doc['scene_index']
                original_text = doc.get('original_text', '')
                summary = doc.get('summary', '') 
                
                # 1. DBì— Parent Scene ì €ì¥
                # ê³ ìœ  ID ìƒì„± (Parentìš©) - Chapter ID í¬í•¨í•˜ì—¬ ì¶©ëŒ ë°©ì§€
                parent_vector_id = f"novel_{novel_id}_chap_{chapter_id}_scene_{scene_index}"
                
                new_doc = VectorDocument(
                    novel_id=novel_id,
                    chapter_id=chapter_id,
                    vector_id=parent_vector_id,
                    chunk_index=scene_index,
                    chunk_text=original_text,
                    metadata_json=doc
                )
                db.add(new_doc)

                # 2. Child Chunk ìƒì„± (Pinecone ìš©)
                # ... (rest of the logic remains similar but uses new parent_vector_id)
                
                # 2. Child Chunk ìƒì„± ë° ì„ë² ë”© ì¤€ë¹„
                # í…ìŠ¤íŠ¸ = ìš”ì•½ + ë³¸ë¬¸ (ê²€ìƒ‰ ì •í™•ë„ë¥¼ ìœ„í•´ ìš”ì•½ë„ ì•ë‹¨ì— ë°°ì¹˜)
                # í•˜ì§€ë§Œ ì •í™•í•œ ìœ„ì¹˜ ê²€ìƒ‰ì„ ì›í•œë‹¤ë©´ ë³¸ë¬¸ë§Œ ìë¥´ëŠ”ê²Œ ë‚˜ì„ ìˆ˜ ìˆìŒ.
                # ì—¬ê¸°ì„œëŠ” ë³¸ë¬¸ ìœ„ì£¼ë¡œ ì²­í‚¹.
                
                child_chunks = self._split_into_child_chunks(original_text)
                
                for i, chunk_text in enumerate(child_chunks):
                    # Child Chunk ì„ë² ë”©
                    embedding = self.embed_text(chunk_text)
                    
                    # Child Vector ID
                    child_id = f"{parent_vector_id}_chunk_{i}"
                    
                    # Metadata (Parent ì¶”ì ìš©)
                    metadata = {
                        'novel_id': novel_id,
                        'chapter_id': chapter_id,
                        'scene_index': scene_index,
                        'type': 'child', # êµ¬ë¶„ì
                        'text': chunk_text, # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•˜ì´ë¼ì´íŠ¸ ë§¤ì¹­ìš©ìœ¼ë¡œ ì €ì¥
                        'chunk_index': i
                    }
                    
                    vectors_to_upsert.append({
                        'id': child_id,
                        'values': embedding,
                        'metadata': metadata
                    })

                if (scene_index + 1) % 5 == 0:
                    print(f"  Parent ì”¬ ì²˜ë¦¬ ì¤‘: {scene_index + 1}/{len(documents)}")
            
            # Pinecone ì—…ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬)
            if vectors_to_upsert:
                batch_size = 100
                print(f"ğŸš€ ì´ {len(vectors_to_upsert)}ê°œì˜ Child Chunkë¥¼ Pineconeì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
                
                for i in range(0, len(vectors_to_upsert), batch_size):
                    batch = vectors_to_upsert[i:i + batch_size]
                    self.index.upsert(vectors=batch)
            
            db.commit()
            print("âœ… Pinecone ì—…ë¡œë“œ ë° DB ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            db.rollback()
            print(f"âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e
        finally:
            db.close()
    
    def search(self, query: str, novel_id: Optional[int] = None, chapter_id: Optional[int] = None, top_k: int = 5):
        """
        ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  Parent Scene ì •ë³´ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤.
        
        Args:
            query (str): ê²€ìƒ‰ ì§ˆë¬¸
            novel_id (int): í•„í„°ë§í•  ì†Œì„¤ ID
            chapter_id (int): í•„í„°ë§í•  íšŒì°¨ ID (ì„ íƒ)
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
        """
        query_embedding = self.embed_text(query)
        
        # Pinecone í•„í„°
        filter_dict = {}
        if novel_id:
            filter_dict['novel_id'] = novel_id
        
        # ê²€ìƒ‰ (Child Chunkë¥¼ ì°¾ìŒ)
        # top_kë¥¼ ì¡°ê¸ˆ ë„‰ë„‰í•˜ê²Œ ì¡ìŒ (ê°™ì€ ì”¬ì˜ ì—¬ëŸ¬ ì²­í¬ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
        search_limit = top_k * 3 
        
        results = self.index.query(
            vector=query_embedding,
            top_k=search_limit,
            include_metadata=True,
            filter=filter_dict if filter_dict else None
        )
        
        # Parent Scene ì§‘ê³„ (ì¤‘ë³µ ì œê±°)
        seen_keys = set() # (chapter_id, scene_index) ìŒìœ¼ë¡œ ê´€ë¦¬
        hits = []
        db = SessionLocal()
        
        try:
            for match in results.matches:
                scene_index = int(match.metadata.get('scene_index'))
                match_chapter_id = match.metadata.get('chapter_id') or chapter_id
                
                # ì¤‘ë³µ ì²´í¬ í‚¤ ìƒì„±
                key = (match_chapter_id, scene_index)
                
                if key in seen_keys:
                    continue
                
                seen_keys.add(key)
                
                # DBì—ì„œ Parent Scene ì¡°íšŒ
                if match_chapter_id:
                    parent_vector_id = f"novel_{novel_id}_chap_{match_chapter_id}_scene_{scene_index}"
                else:
                    parent_vector_id = f"novel_{novel_id}_scene_{scene_index}"
                    
                doc = db.query(VectorDocument).filter(
                    VectorDocument.vector_id == parent_vector_id
                ).first()
                
                if doc:
                    scene_data = doc.metadata_json
                    
                    # ë§¤ì¹˜ëœ Child Text ì •ë³´ë¥¼ ì¶”ê°€ë¡œ ì œê³µ (í•˜ì´ë¼ì´íŠ¸ íŒíŠ¸ìš©)
                    scene_data['matched_chunk'] = match.metadata.get('text', '')
                    scene_data['similarity'] = match.score
                    
                    hits.append({
                        'document': scene_data,
                        'chapter_id': chapter_id,
                        'similarity': match.score,
                        'vector_id': match.id
                    })
                
                if len(hits) >= top_k:
                    break
                    
        finally:
            db.close()
        
        return hits
