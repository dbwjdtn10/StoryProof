
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy import desc
from sqlalchemy.orm import Session
from sqlalchemy import func
from typing import List, Optional
import json
import logging
from pydantic import BaseModel
from datetime import datetime

from backend.db.session import get_db
from backend.db.models import Novel, Analysis, AnalysisType, CharacterChatRoom, CharacterChatMessage
from backend.core.config import settings
from backend.services.chatbot_service import get_chatbot_service

# Initialize Gemini Client
try:
    from google import genai
    from google.genai import types
    client = genai.Client(api_key=settings.GOOGLE_API_KEY)
except ImportError:
    client = None
    print("Warning: google-genai not installed or configured.")

# Setup logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# --- Schemas ---
class CharacterChatRoomCreate(BaseModel):
    novel_id: int
    chapter_id: Optional[int] = None
    character_name: str
    persona_prompt: str

class CharacterChatRoomUpdate(BaseModel):
    persona_prompt: Optional[str] = None

class CharacterChatRoomResponse(BaseModel):
    id: int
    user_id: int
    novel_id: int
    chapter_id: Optional[int]
    character_name: str
    persona_prompt: str
    created_at: datetime
    updated_at: Optional[datetime]

    class Config:
        from_attributes = True

class CharacterChatMessageCreate(BaseModel):
    content: str

class CharacterChatMessageResponse(BaseModel):
    id: int
    room_id: int
    role: str
    content: str
    created_at: datetime

    class Config:
        from_attributes = True

class PersonaGenerationRequest(BaseModel):
    novel_id: int
    chapter_id: Optional[int] = None
    character_name: str

class PersonaGenerationResponse(BaseModel):
    character_name: str
    persona_prompt: str

# --- Helper Functions ---
def extract_character_dialogues(analysis_data: dict, character_name: str, max_dialogues: int = 50) -> list:
    """
    Extract actual dialogue lines spoken by the character from scene data.
    Returns list of dialogue strings (up to max_dialogues).
    """
    dialogues = []
    scenes = analysis_data.get("scenes", [])
    
    logger.info(f"[DIALOGUE EXTRACTION] Extracting dialogues for character: {character_name}")
    logger.info(f"[DIALOGUE EXTRACTION] Total scenes to check: {len(scenes)}")
    
    for scene_idx, scene in enumerate(scenes):
        # Check if character appears in this scene
        characters_in_scene = scene.get("characters", [])
        if character_name not in characters_in_scene:
            continue
            
        logger.debug(f"[DIALOGUE EXTRACTION] Scene {scene_idx}: Character found, extracting...")
        
        # Extract dialogue from original_text
        text = scene.get("original_text", "")
        if not text:
            continue
            
        lines = text.split('\n')
        
        for line in lines:
            stripped = line.strip()
            
            # Improved dialogue detection:
            # 1. Lines starting with quotes (English or Korean)
            # 2. Lines with em-dash dialogue format (â€” dialogue)
            # 3. Lines with ã€Œã€brackets (Japanese-style quotes)
            is_dialogue = (
                stripped.startswith('"') or 
                stripped.startswith("'") or 
                stripped.startswith('â€œ') or  # Korean opening quote
                stripped.startswith('ã€Œ') or
                stripped.startswith('â€”')  # Em-dash dialogue
            )
            
            if is_dialogue and len(stripped) > 3:  # Filter out very short lines
                # Clean up the dialogue
                cleaned = stripped.strip('â€”').strip()
                dialogues.append(cleaned)
                logger.debug(f"[DIALOGUE EXTRACTION] Found: {cleaned[:50]}...")
                
                if len(dialogues) >= max_dialogues:
                    logger.info(f"[DIALOGUE EXTRACTION] Reached max dialogues ({max_dialogues})")
                    return dialogues
    
    logger.info(f"[DIALOGUE EXTRACTION] Extracted {len(dialogues)} dialogues for {character_name}")
    return dialogues

# --- Router ---
router = APIRouter()

@router.post("/generate-persona", response_model=PersonaGenerationResponse)
async def generate_persona(
    request: PersonaGenerationRequest,
    db: Session = Depends(get_db)
):
    """
    Generate a persona system prompt for a character using analysis data.
    """
    if not client:
        raise HTTPException(
            status_code=500,
            detail="LLM client not initialized. Check GOOGLE_API_KEY."
        )

    # Debug logging - Track incoming request
    print(f"\n[DEBUG] ===== GENERATE PERSONA REQUEST =====")
    print(f"[DEBUG] Requested novel_id: {request.novel_id}")
    print(f"[DEBUG] Requested character_name: {request.character_name}")

    # Fetch Analysis
    # Prioritize Chapter-specific analysis if chapter_id is provided
    query = db.query(Analysis).filter(
        Analysis.novel_id == request.novel_id,
        Analysis.status == "completed"
    )
    
    if request.chapter_id:
        # Try to find analysis specifically for this chapter (Character or Overall)
        # We might need to be careful if AnalysisType.CHARACTER is used for single chapter
        # For now, let's filter by chapter_id if it exists in the Analysis table
        query = query.filter(Analysis.chapter_id == request.chapter_id)
    else:
        # Fallback to Global Analysis (Chapter ID is NULL)
        # query = query.filter(Analysis.chapter_id.is_(None)) 
        # But maybe we want ANY analysis if global is missing?
        # Let's stick to novel_id filter for now if no chapter_id providing "Global" context
        pass 
        
    analysis = query.order_by(desc(Analysis.created_at)).first()
    
    # If no specific analysis found for chapter, fall back to global?
    # User requested strict isolation: "Make it readable from current novel/file only"
    # So if chapter analysis is missing, we might NOT want global.
    # However, if the user just uploaded, maybe there is NO analysis yet?
    # In that case, we should probably return a default or error, or try to use VectorDocument metadata.
    
    if not analysis and request.chapter_id:
        print(f"[Persona] No Analysis found for chapter {request.chapter_id}. Aggregating from VectorDocument metadata...")
        # Fallback: Aggregate character data from VectorDocument metadata for this chapter
        # This is useful when the file is uploaded and processed (vectors exist) but explicit "Analysis" step hasn't run or failed.
        # This ensures we still have valid character options from the file itself.
        
        from backend.db.models import VectorDocument
        
        # Fetch parent vectors (chunk_index is what we need, but metadata contains character info)
        # We need to scan metadata_json of parent documents in this chapter.
        vectors = db.query(VectorDocument).filter(
            VectorDocument.novel_id == request.novel_id,
            VectorDocument.chapter_id == request.chapter_id
        ).all()
        
        if vectors:
            from backend.services.analysis.gemini_structurer import GeminiStructurer
            # We can use the aggregation logic from GeminiStructurer!
            # It expects StructuredScene objects, but we can construct dicts or mock objects?
            # Actually extract_global_entities expects list of StructuredScene.
            # Let's manually aggregate simple traits here to check if the requested character exists.
            
            aggregated_char = None
            
            for vec in vectors:
                meta = vec.metadata_json or {}
                chars = meta.get('characters', [])
                for char in chars:
                    c_name = char.get('name') if isinstance(char, dict) else char
                    if c_name == request.character_name:
                        # Found match!
                        if not aggregated_char:
                             aggregated_char = {
                                 "name": c_name,
                                 "description": char.get('description', '') if isinstance(char, dict) else '',
                                 "traits": char.get('traits', []) if isinstance(char, dict) else []
                             }
                        else:
                            # Merge traits
                            new_traits = char.get('traits', []) if isinstance(char, dict) else []
                            for t in new_traits:
                                if t not in aggregated_char['traits']:
                                    aggregated_char['traits'].append(t)
                            
                            # Update desc if longer
                            new_desc = char.get('description', '') if isinstance(char, dict) else ''
                            if len(new_desc) > len(aggregated_char['description']):
                                aggregated_char['description'] = new_desc
            
            if aggregated_char:
                # Construct a fake Analysis result object/dict to pass to persona generation
                # We need a proper object structure that has .result attribute
                class MockAnalysis:
                    def __init__(self, data):
                        self.result = data
                        self.id = "mock_vector_aggregation"
                        self.novel_id = request.novel_id
                        self.analysis_type = "vector_aggregation"
                
                analysis = MockAnalysis({
                    "characters": [aggregated_char],
                    "summary": "Generated from Vector Metadata",
                    "mood": "Unknown"
                })
                print(f"[Persona] Successfully aggregated data for '{request.character_name}' from {len(vectors)} scenes.")
            else:
                print(f"[Persona] Character '{request.character_name}' not found in chapter {request.chapter_id} vectors.")

    if not analysis or (not hasattr(analysis, 'result')) or (hasattr(analysis, 'result') and not analysis.result):
        # Fallback to character specific analysis if overall is missing (Legacy logic)
        # But if we are in chapter_id mode and failed, we should probably stop or risk global pollution.
        if request.chapter_id:
             # Just return a basic persona if strictly scoped but nothing found?
             # Or raise 404? 
             # Let's try to proceed with basic info if aggregated_char failed.
             # But if we proceed, we fall into legacy logic which queries GLOBAL analysis.
             # We must STOP here if chapter_id is set.
             pass # Will fall through to 404 check below or legacy loop
             
        if request.chapter_id:
             print(f"[Persona] Strict mode: No analysis found for chapter {request.chapter_id}. Aborting to prevent cross-contamination.")
             raise HTTPException(status_code=404, detail=f"No analysis or character data found for '{request.character_name}' in this chapter.")

        # Legacy fallback (Global)
        analysis = db.query(Analysis).filter(
            Analysis.novel_id == request.novel_id,
            Analysis.analysis_type == AnalysisType.CHARACTER,
            Analysis.status == "completed"
        ).order_by(desc(Analysis.created_at)).first()
    
    # Debug: Show what we found
    if analysis:
        print(f"[DEBUG] Found analysis ID: {analysis.id}")
        print(f"[DEBUG] Analysis novel_id: {analysis.novel_id}")
        print(f"[DEBUG] Analysis type: {analysis.analysis_type}")
    else:
        print(f"[DEBUG] NO ANALYSIS FOUND for novel_id={request.novel_id}")
    
    if not analysis or not analysis.result:
         raise HTTPException(
            status_code=404,
            detail="Analysis data not found for this novel. Please run analysis first."
        )

    # Find character data
    character_data = None
    data = analysis.result
    
    # Debug logging
    print(f"[DEBUG] Analysis type: {analysis.analysis_type}")
    print(f"[DEBUG] Data type: {type(data)}")
    print(f"[DEBUG] Data keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
    
    # Handle both structure types (Overall vs Character specific)
    characters_list = []
    
    if isinstance(data, dict):
        # Try different possible keys for characters
        characters_list = (
            data.get("characters", []) or 
            data.get("character_analysis", []) or 
            []
        )
        print(f"[DEBUG] Characters list length: {len(characters_list)}")
        if characters_list:
            print(f"[DEBUG] First character sample: {characters_list[0]}")
    elif isinstance(data, list):
        characters_list = data
        
    # Normalize function to remove whitespace
    def normalize_name(name: str) -> str:
        return name.replace(" ", "").lower() if name else ""

    target_name_norm = normalize_name(request.character_name)
    print(f"[DEBUG] Looking for normalized name: '{target_name_norm}'")

    # First pass: Exact match
    for char in characters_list:
        if not isinstance(char, dict):
            continue
        char_name = char.get("name") or char.get("character_name")
        char_name_norm = normalize_name(char_name)
        print(f"[DEBUG] Checking character: '{char_name}' (normalized: '{char_name_norm}')")
        if char_name_norm == target_name_norm:
            character_data = char
            break
            
    # Second pass: Partial match
    if not character_data:
        for char in characters_list:
            if not isinstance(char, dict):
                continue
            char_name = char.get("name") or char.get("character_name")
            char_name_norm = normalize_name(char_name)
            if target_name_norm in char_name_norm or char_name_norm in target_name_norm:
                character_data = char
                print(f"[DEBUG] Found partial match: '{char_name}'")
                break

    
    if not character_data:
        # Provide helpful debug info in error
        available_names = []
        for char in characters_list[:10]:  # Show first 10
            if isinstance(char, dict):
                name = char.get("name") or char.get("character_name")
                if name:
                    available_names.append(name)
        
        error_detail = f"Character '{request.character_name}' not found in analysis."
        if available_names:
            error_detail += f" Available characters: {', '.join(available_names)}"
        else:
            error_detail += " No characters found in analysis data."
            
        raise HTTPException(
            status_code=404,
            detail=error_detail
        )
         
    # [Context Enhancement] Add relations if available
    relations_text = ""
    # Try to find relationships in overall analysis
    if "relationships" in analysis.result:
        rels = analysis.result["relationships"]
        # Filter for this character
        char_rels = [r for r in rels if request.character_name in r.get("source", "") or request.character_name in r.get("target", "")]
        if char_rels:
             relations_text = "\n" + "\n".join([f"- {r.get('target' if r.get('source') == request.character_name else 'source')}: {r.get('relation')} ({r.get('description')})" for r in char_rels])

    # [Enhancement] Extract character dialogues for speech pattern analysis
    dialogues = extract_character_dialogues(data, request.character_name, max_dialogues=50)
    dialogue_examples = ""
    has_dialogues = False
    
    if dialogues:
        has_dialogues = True
        dialogue_count = len(dialogues)
        dialogue_examples = f"\n\n[ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ (ì´ {dialogue_count}ê°œ)]"
        dialogue_examples += "\n" + "\n".join([f"{i+1}. {d}" for i, d in enumerate(dialogues)])
        dialogue_examples += f"\n\nâš ï¸ ì¤‘ìš”: ìœ„ {dialogue_count}ê°œì˜ ì‹¤ì œ ëŒ€ì‚¬ë¥¼ ë°˜ë“œì‹œ ë¶„ì„í•˜ë¼. ë§íˆ¬ íŒ¨í„´, ì–´ë¯¸ ì‚¬ìš© ë¹ˆë„, ë¬¸ì¥ ê¸¸ì´, ë°˜ë³µ í‘œí˜„ì„ í†µê³„ì ìœ¼ë¡œ ì¶”ì¶œí•˜ë¼."
    else:
        logger.warning(f"[PERSONA] No dialogues found for {request.character_name}. Using general analysis only.")
        dialogue_examples = "\n\n[ì•Œë¦¼: ì´ ìºë¦­í„°ì˜ ì§ì ‘ì ì¸ ëŒ€ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„±ê²© ì„¤ëª…ê³¼ íŠ¹ì§•ë§Œìœ¼ë¡œ ë§íˆ¬ë¥¼ ì¶”ë¡ í•˜ì„¸ìš”.]"
    
    # Generate Prompt using Gemini
    meta_prompt = f"""
ë„ˆëŠ” ì„¸ê³„ ìµœê³ ì˜ ìºë¦­í„° í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì´ë‹¤. 
ì•„ë˜ ì›¹ì†Œì„¤ ìºë¦­í„°ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, ì´ ìºë¦­í„°ê°€ ë˜ì–´ ì‹¤ì‹œê°„ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•  AIì˜ **'ì‹œìŠ¤í…œ ì§€ì¹¨(System Instruction)'**ì„ ì‘ì„±í•˜ë¼.

[ë°ì´í„° ë² ì´ìŠ¤]
- ì´ë¦„: {character_data.get('name')}
- í•µì‹¬ ì„±ê²©: {character_data.get('description')}
- ì£¼ìš” íŠ¹ì§•/íƒœê·¸: {', '.join(character_data.get('traits', []))}{relations_text}{dialogue_examples}

[ì‹œìŠ¤í…œ ì§€ì¹¨ ì‘ì„± ê°€ì´ë“œë¼ì¸]
1. **1ì¸ì¹­ ì •ì²´ì„±**: "ë„ˆëŠ” ~ì´ë‹¤"ë¼ê³  ì •ì˜í•˜ë¼. ìºë¦­í„°ì˜ ë‚´ë©´ ì‹¬ë¦¬ì™€ ê°€ì¹˜ê´€ì„ í¬í•¨í•˜ë¼.

2. **ë§íˆ¬ì˜ ë¬¼ë¦¬ì  ê·œì¹™**:
   {'   - ğŸ¯ ìœ„ì— ì œê³µëœ [ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ]ë¥¼ í•œ ì¤„ í•œ ì¤„ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ë¼' if has_dialogues else '   - ì„±ê²© ì„¤ëª…ê³¼ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ë§íˆ¬ë¥¼ ì¶”ë¡ í•˜ë¼'}
   {'   - ëŒ€ì‚¬ì—ì„œ ê°€ì¥ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì–´ë¯¸ë¥¼ ì¶”ì¶œí•˜ê³  ë¹ˆë„ë¥¼ ê³„ì‚° (ì˜ˆ: ~ì–´ìš” 30%, ~ë‹¤ 20%)' if has_dialogues else '   - ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ì–´ë¯¸ë¥¼ ì„¤ì • (ì˜ˆ: ê¶Œìœ„ì ì´ë©´ ~ë¼/~ë‹¤)'}
   {'   - ì‹¤ì œ ëŒ€ì‚¬ì˜ í‰ê·  ë¬¸ì¥ ê¸¸ì´ë¥¼ ì¸¡ì • (ì§§ìŒ/ì¤‘ê°„/ê¸º)' if has_dialogues else '   - ì„±ê²©ì— ë§ëŠ” ë¬¸ì¥ ê¸¸ì´ ê²°ì •'}
   {'   - ëŒ€ì‚¬ì—ì„œ ë°˜ë³µë˜ëŠ” ê°íƒ„ì‚¬, ì ‘ì†ì‚¬, ë§ë²„ë¦‡ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¶”ì¶œ (ì˜ˆ: "ê·¸ë˜", "ìŒ...")' if has_dialogues else '   - ìºë¦­í„° íŠ¹ì„±ì— ë§ëŠ” ê°íƒ„ì‚¬/ë²„ë¦‡ ì„¤ì •'}
   {'   - ëŒ€ì‚¬ì—ì„œ ë†’ì„ë§/ë°˜ë§ ë¹„ìœ¨ì„ ì •í™•íˆ ê³„ì‚°í•˜ë¼' if has_dialogues else '   - ì„±ê²©ê³¼ ì„¤ì •ì— ë§ëŠ” ë†’ì„ë§/ë°˜ë§ ë¹„ìœ¨ ì„¤ì •'}
   - íŠ¹ì • ë¬¸ì¥ êµ¬ì¡°ë‚˜ íŒ¨í„´ì´ ìˆìœ¼ë©´ ê¸°ë¡í•˜ë¼

3. **ëŒ€í™” íƒœë„**:
   {'   - ì‹¤ì œ ëŒ€ì‚¬ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í‰ê·  ê°ì • í†¤ ë¶„ì„ (ê³µê²©ì /ì¹œê·¼í•¨/ë¬´ëšëší•¨/ì¥ë‚œê¸°/ê¶Œìœ„ì )' if has_dialogues else '   - ì„±ê²© ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ê°ì • í†¤ ì„¤ì •'}
   {'   - ëŒ€ì‚¬ì˜ ì§ˆë¬¸ vs ì§„ìˆ  ë¹„ìœ¨ì„ ê³„ì‚°í•˜ë¼' if has_dialogues else '   - ìºë¦­í„° íŠ¹ì„±ì— ë§ëŠ” ì§ˆë¬¸/ì§„ìˆ  ë¹„ìœ¨ ì„¤ì •'}
   - ê´€ê³„ ë°ì´í„°ì™€ {'ëŒ€ì‚¬ í†¤' if has_dialogues else 'ì„±ê²© íŠ¹ì§•'}ì„ ì¢…í•©í•´ ì‚¬ìš©ì ëŒ€í•˜ëŠ” íƒœë„ë¥¼ ì„¤ì •í•˜ë¼

4. **êµ¬ì–´ì²´ ë³€í™˜**: ì†Œì„¤ ì† ë”±ë”±í•œ ë¬¸ì–´ì²´ê°€ ì•„ë‹Œ, ì‹¤ì œ ëŒ€í™”ë‚˜ ë©”ì‹ ì €ì—ì„œ ì“¸ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì–´íˆ¬ë¥¼ ëª…ë ¹í•˜ë¼. ì‹¤ì œ ëŒ€ì‚¬ì˜ ìŠ¤íƒ€ì¼ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ë¼.

5. **ê¸ˆê¸° ì‚¬í•­**: ì ˆëŒ€ ê¸°ê³„ì ì¸ "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?" ì‹ì˜ ë‹µë³€ì„ í•˜ì§€ ë§ê³ , ìºë¦­í„°ë‹µì§€ ì•Šì€ ì¹œì ˆí•¨ì´ë‚˜ ë¹„ì†ì–´ë¥¼ ì œí•œí•˜ë¼.

[ì¶œë ¥ í˜•ì‹]
ì§€ì¹¨ ë‚´ìš©ë§Œ ì¶œë ¥í•  ê²ƒ.
    """
        
    # Add dialogue inclusion guideline if dialogues exist
    if has_dialogues:
        meta_prompt += """
    
6. **ğŸ¯ ì¤‘ìš”: ì‹¤ì œ ëŒ€ì‚¬ í¬í•¨ í•„ìˆ˜**:
   - ìœ„ [ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ] ì¤‘ì—ì„œ ìºë¦­í„°ì˜ ë§íˆ¬ë¥¼ ê°€ì¥ ì˜ ë³´ì—¬ì£¼ëŠ” **10-15ê°œë¥¼ ì„ ë³„**í•˜ì—¬ ìƒì„±í•  ì‹œìŠ¤í…œ ì§€ì¹¨ì— í¬í•¨ì‹œì¼œë¼
   - ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œ ì§€ì¹¨ì˜ ë§íˆ¬ ê·œì¹™ ì„¤ëª… í›„ì— ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶”ê°€í•˜ë¼:
   
   ì˜ˆì‹œ í˜•ì‹:
   [ì°¸ê³ í•  ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ]
   1. "ëŒ€ì‚¬ ë‚´ìš©..."
   2. "ëŒ€ì‚¬ ë‚´ìš©..."
   ...(10-15ê°œ)
   
   ìœ„ ëŒ€ì‚¬ë¥¼ ì°¸ê³ í•˜ì—¬ ëŒ€í™”í•  ë•Œ ìœ ì‚¬í•œ ì–´íˆ¬ì™€ íŒ¨í„´ì„ ì‚¬ìš©í•˜ë¼.
    """


    # [Strict Grounding Instruction Injection]
    meta_prompt += """
    
[í•„ìˆ˜ í¬í•¨ ê·œì¹™]
ìƒì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ê·œì¹™ë“¤ì„ í¬í•¨í•´ì•¼ í•œë‹¤:
"1. ë„ˆëŠ” ì†Œì„¤ ì„¤ì •ì— ê¸°ë°˜í•œ ìºë¦­í„° ì—­í• ê·¹ì„ ìˆ˜í–‰í•œë‹¤.
2. ì‚¬ìš©ìê°€ ë„¤ ê¸°ì–µì´ë‚˜ ì œê³µëœ ë§¥ë½ì— ì—†ëŠ” ê²ƒì„ ë¬¼ìœ¼ë©´, ëª¨ë¥¸ë‹¤ê³  í•˜ê±°ë‚˜ ì• ë§¤í•˜ê²Œ í”¼í•˜ë¼. ì ˆëŒ€ ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ˆë¼.
3. ì‚¬ìš©ìê°€ ì†Œì„¤ì˜ ì‚¬ì‹¤ê³¼ ëª¨ìˆœë˜ëŠ” ë§ì„ í•˜ë©´, ë„¤ ê¸°ì–µì„ ë°”íƒ•ìœ¼ë¡œ ìºë¦­í„°ë‹µê²Œ êµì •í•˜ë¼.
4. ë‹µë³€ì€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ë¼. íŠ¹ë³„íˆ ê¸´ ì„¤ëª…ì„ ìš”ì²­ë°›ì§€ ì•ŠëŠ” í•œ, ì¥í™©í•˜ê²Œ ì„¤ëª…í•˜ê±°ë‚˜ ì •ë³´ë¥¼ ëŠ˜ì–´ë†“ì§€ ë§ˆë¼. ì¼ë°˜ì ì¸ ë‹µë³€ì€ 1~3ë¬¸ì¥ì„ ëª©í‘œë¡œ í•˜ë¼."
    """
    
    try:
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=meta_prompt
        )
        persona_prompt = response.text.strip()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate persona: {str(e)}")

    return PersonaGenerationResponse(
        character_name=request.character_name,
        persona_prompt=persona_prompt
    )

@router.put("/rooms/{room_id}", response_model=CharacterChatRoomResponse)
async def update_room(
    room_id: int,
    room_update: CharacterChatRoomUpdate,
    db: Session = Depends(get_db)
):
    """
    Update a character chat room (e.g. persona prompt).
    """
    room = db.query(CharacterChatRoom).filter(CharacterChatRoom.id == room_id).first()
    if not room:
        raise HTTPException(status_code=404, detail="Chat room not found")
        
    if room_update.persona_prompt is not None:
        room.persona_prompt = room_update.persona_prompt
        
    room.updated_at = func.now()
    db.commit()
    db.refresh(room)
    return room

@router.delete("/rooms/{room_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_room(
    room_id: int,
    db: Session = Depends(get_db)
):
    """
    Delete a character chat room.
    """
    room = db.query(CharacterChatRoom).filter(CharacterChatRoom.id == room_id).first()
    if not room:
        raise HTTPException(status_code=404, detail="Chat room not found")
        
    db.delete(room)
    db.commit()
    return None

@router.post("/rooms", response_model=CharacterChatRoomResponse)
async def create_room(
    room_data: CharacterChatRoomCreate,
    db: Session = Depends(get_db)
):
    """
    Create a new character chat room.
    """
    # Verify novel exists
    novel = db.query(Novel).filter(Novel.id == room_data.novel_id).first()
    if not novel:
        raise HTTPException(status_code=404, detail="Novel not found")
        
    # Create room
    # Get actual user_id from auth if possible, otherwise use novel author or 1
    # For now, we trust the frontend to handle auth or just use 1 as fallback/demo
    user_id = novel.author_id 

    new_room = CharacterChatRoom(
        user_id=user_id,
        novel_id=room_data.novel_id,
        chapter_id=room_data.chapter_id,
        character_name=room_data.character_name,
        persona_prompt=room_data.persona_prompt
    )
    
    db.add(new_room)
    db.commit()
    db.refresh(new_room)
    
    return new_room

@router.get("/rooms", response_model=List[CharacterChatRoomResponse])
async def list_rooms(
    novel_id: int,
    chapter_id: Optional[int] = None,
    db: Session = Depends(get_db)
):
    """
    List chat rooms for a novel.
    If chapter_id is provided, filter by that chapter (file-scoped).
    If not provided, return all (or global ones? For now, we return those matching the query).
    """
    query = db.query(CharacterChatRoom).filter(
        CharacterChatRoom.novel_id == novel_id
    )
    
    if chapter_id:
        query = query.filter(CharacterChatRoom.chapter_id == chapter_id)
    
    rooms = query.order_by(desc(CharacterChatRoom.updated_at)).all()
    
    return rooms

@router.post("/rooms/{room_id}/messages", response_model=List[CharacterChatMessageResponse])
async def send_message(
    room_id: int,
    message: CharacterChatMessageCreate,
    db: Session = Depends(get_db)
):
    """
    Send a message to the character and get a response.
    """
    room = db.query(CharacterChatRoom).filter(CharacterChatRoom.id == room_id).first()
    if not room:
        raise HTTPException(status_code=404, detail="Chat room not found")
        
    if not client:
        raise HTTPException(status_code=500, detail="LLM client not initialized")

    # 1. Save User Message
    user_msg = CharacterChatMessage(
        room_id=room.id,
        role="user",
        content=message.content
    )
    db.add(user_msg)
    db.commit() # Commit to save ID and order
    
    # 2. Fetch recent history for context (last 10 messages)
    history_records = db.query(CharacterChatMessage).filter(
        CharacterChatMessage.room_id == room.id
    ).order_by(CharacterChatMessage.created_at).all()
    
    # 3. RAG: Retrieve Context
    chatbot_service = get_chatbot_service()
    rag_context = ""
    source_info = None
    
    if chatbot_service and chatbot_service.engine:
        # Find similar chunks using the user's message
        # We need the novel_id. The room is linked to a novel.
        novel = db.query(Novel).filter(Novel.id == room.novel_id).first()
        novel_filter = None
        if novel:
            # Construct filter like "alice" from "alice.txt" roughly, or better use ID if service supports it
            # Currently chatbot service uses filename string matching. 
            # Let's try to match by title if possible, or we need to ensure filename logic matches.
            # Assuming title + extension or similar. 
            # For robustness, we might skip novel_filter if uncertain, but that mixes novels.
            # TODO: Improve ChatbotService to filter by novel_id directly. 
            # For now, let's rely on the global search or simple title match fallback.
            novel_filter = novel.title # Basic attempt
            
        try:
            chunks = chatbot_service.find_similar_chunks(
                question=message.content,
                top_k=3,
                novel_filter=novel_filter,
                chapter_id=room.chapter_id
            )
            if chunks:
                rag_context = "\n\n[Reference Scenes from Novel]:\n"
                for i, chunk in enumerate(chunks):
                    rag_context += f"Scene {chunk.get('scene_index', '?')}: {chunk['text'][:500]}...\n"
                
                # Metadata for frontend (optional)
                source_info = [{
                    "scene": c.get('scene_index'), 
                    "similarity": c.get('similarity')
                } for c in chunks]
                
        except Exception as e:
            print(f"RAG Search failed: {e}")

    # 4. Generate Response
    input_text = message.content
    if rag_context:
        # Inject RAG context with clear separation
        input_text = f"""
### [ê¸°ì–µ ë°ì´í„°] ###
{rag_context}

### [ì‚¬ìš©ìì˜ ë©”ì‹œì§€] ###
{message.content}
"""
    
    # Format history for Gemini
    contents = []
    
    # Add System Instruction logic (Gemini 2.5 supports system_instruction param) 
    # Combine Persona + Messenger Protocol + Self-Checking
    system_instruction = room.persona_prompt + """

[ì±„íŒ… í”„ë¡œí† ì½œ: ì‹¤ì‹œê°„ ë©”ì‹ ì € ëª¨ë“œ]
1. **ê°„ê²°ì„±**: í•œ ë²ˆì˜ ë‹µì¥ì€ 1~3ë¬¸ì¥ ì´ë‚´ë¡œ ì œí•œí•œë‹¤.
2. **ê¸°ì–µ í™œìš©**: [ê¸°ì–µ ë°ì´í„°]ëŠ” ë„ˆì˜ ì‹¤ì œ ê¸°ì–µì´ë‹¤. "ì†Œì„¤ì— ë”°ë¥´ë©´" ê°™ì€ í‘œí˜„ì„ ì ˆëŒ€ ì“°ì§€ ë§ê³ , ìì‹ ì˜ ê²½í—˜ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ë¼.
3. **ëª°ì… ìœ ì§€**: ì‚¬ìš©ìê°€ ì†Œì„¤ ì„¤ì •ê³¼ ë§ì§€ ì•ŠëŠ” ë§ì„ í•˜ë©´, ìºë¦­í„°ë‹µê²Œ ë°˜ì‘í•˜ë¼.
4. **í˜„ì¥ê°**: ì§ˆë¬¸ì—ë§Œ ë‹µí•˜ì§€ ë§ê³ , ê°€ë”ì€ ìºë¦­í„°ì˜ í˜„ì¬ ê°ì •ì´ë‚˜ ìƒí™©ì„ íˆ­ ë˜ì ¸ë¼.
5. **ê¸ˆì§€**: ì„¤ëª…ì¡°ì˜ ë§íˆ¬, ê¸´ ë¬¸ë‹¨, AIë‹¤ìš´ ì •ì¤‘í•¨ì„ ëª¨ë‘ ë²„ë ¤ë¼.

[í•„ìˆ˜ ì¶œë ¥ í˜•ì‹]
ëª¨ë“  ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ì„ EXACTLY ë”°ë¼ì•¼ í•œë‹¤:

<ë‹µë³€ ë‚´ìš©ì„ ê·¸ëƒ¥ ì—¬ê¸°ì— ì“´ë‹¤. íƒœê·¸ ì—†ì´ ê·¸ëƒ¥ ë¬¸ì¥ë§Œ.>

[SELF_CHECK]
Checklist: X/5 | Confidence: Y.Y/5.0 | Notes: ê°„ë‹¨ ë©”ëª¨

ì¤‘ìš”:
- ë‹µë³€ì— [CHARACTER_MESSAGE]ë‚˜ ë‹¤ë¥¸ íƒœê·¸ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆë¼
- ê·¸ëƒ¥ í‰ë²”í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ ë‹µë³€í•˜ë¼
- [SELF_CHECK] ì´ì „ì—ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ìˆì–´ì•¼ í•œë‹¤
- [SELF_CHECK] íƒœê·¸ëŠ” ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•œë‹¤

ì‹¤ì œ ì¶œë ¥ ì˜ˆì‹œ:
ê·¸ë˜, ë­”ê°€ í•„ìš”í•´?

[SELF_CHECK]
Checklist: 5/5 | Confidence: 4.5/5.0 | Notes: ì§§ê³  ìºë¦­í„°ë‹µê²Œ

ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©: (1)í˜ë¥´ì†Œë‚˜ ìœ ì§€ (2)ë§íˆ¬ ì¼ê´€ì„± (3)ì„¤ì • ì¤€ìˆ˜ (4)ê¸¸ì´ ì ì ˆ (5)RAG ìì—° í™œìš©
    """
    
    for msg in history_records:
        role = "user" if msg.role == "user" else "model"
        # We don't inject RAG into past history to save tokens and avoid confusion
        contents.append(types.Content(
            role=role,
            parts=[types.Part(text=msg.content)]
        ))
    
    # Add current message with RAG
    contents.append(types.Content(
        role="user",
        parts=[types.Part(text=input_text)]
    ))
        
    try:
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=contents,
            config=types.GenerateContentConfig(
                system_instruction=system_instruction
            )
        )
        ai_reply = response.text.strip()
        
        # Debug: Log full response to check if SELF_CHECK is present
        logger.info(f"\n{'='*70}")
        logger.info(f"[AI RESPONSE DEBUG] Room {room_id} | Character: {room.character_name}")
        logger.info(f"Full response length: {len(ai_reply)} chars")
        logger.info(f"Contains [SELF_CHECK]: {'[SELF_CHECK]' in ai_reply}")
        logger.info(f"{'='*70}")
        
        # Parse self-check from response
        user_message = ai_reply
        self_check_log = ""
        
        if "[SELF_CHECK]" in ai_reply:
            parts = ai_reply.split("[SELF_CHECK]", 1)
            user_message = parts[0].strip()
            self_check_log = parts[1].strip()
            
            # Log to uvicorn console (not saved to DB or shown to user)
            logger.info(f"\n{'='*70}")
            logger.info(f"[SELF-CHECK] Room {room_id} | Character: {room.character_name}")
            logger.info(f"[SELF-CHECK] {self_check_log}")
            logger.info(f"{'='*70}\n")
        else:
            # Just warn if self-check is missing - don't generate fake data
            logger.warning(f"\n[SELF-CHECK] âš ï¸ LLM did not include self-check for Room {room_id}")
            logger.warning(f"[SELF-CHECK] Response length: {len(ai_reply)} chars")
            logger.warning(f"[SELF-CHECK] First 200 chars: {ai_reply[:200]}...")
            logger.warning(f"[SELF-CHECK] Tip: Check prompt or try regenerating persona\n")
        
        ai_reply = user_message  # Use cleaned message for DB
        
    except Exception as e:
        # Fallback if generation fails
        ai_reply = "..."
        print(f"Error generating chat response: {e}")
        
    # 5. Save AI Message
    ai_msg = CharacterChatMessage(
        room_id=room.id,
        role="assistant",
        content=ai_reply
    )
    db.add(ai_msg)
    
    # Update room updated_at
    room.updated_at = ai_msg.created_at
    
    db.commit()
    db.refresh(user_msg)
    db.refresh(ai_msg)
    
    return [user_msg, ai_msg]

@router.get("/rooms/{room_id}/messages", response_model=List[CharacterChatMessageResponse])
async def get_messages(
    room_id: int,
    db: Session = Depends(get_db)
):
    """
    Get message history for a room.
    """
    messages = db.query(CharacterChatMessage).filter(
        CharacterChatMessage.room_id == room_id
    ).order_by(CharacterChatMessage.created_at).all()
    
    return messages
