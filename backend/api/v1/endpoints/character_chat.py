"""ìºë¦­í„° ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸"""

import json
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from sqlalchemy import desc
from sqlalchemy.orm import Session
from typing import List, Optional
import asyncio
import logging
from functools import partial
from datetime import datetime

from backend.db.session import get_db
from backend.db.models import Novel, Analysis, AnalysisType, CharacterChatRoom, CharacterChatMessage, VectorDocument
from backend.core.config import settings
from backend.services.chatbot_service import get_chatbot_service
from backend.services.character_chat_service import CharacterChatService
from backend.schemas.character_chat_schema import (
    CharacterChatRoomCreate, CharacterChatRoomUpdate, CharacterChatRoomResponse,
    CharacterChatMessageCreate, CharacterChatMessageResponse,
    PersonaGenerationRequest, PersonaGenerationResponse,
)

# Setup logger (must be before genai import attempt)
logger = logging.getLogger(__name__)

# Initialize Gemini Client
try:
    from google import genai
    from google.genai import types
    client = genai.Client(api_key=settings.GOOGLE_API_KEY)
except ImportError:
    client = None
    logger.warning("Warning: google-genai not installed or configured.")



# --- Helper Functions ---

def extract_character_dialogues(analysis_data: dict, character_name: str, max_dialogues: int = 50) -> list:
    """
    Extract actual dialogue lines spoken by the character from scene data.
    Returns list of dialogue strings (up to max_dialogues).
    """
    dialogues = []
    scenes = analysis_data.get("scenes", [])

    logger.info(f"[DIALOGUE EXTRACTION] Extracting dialogues for character: {character_name}")
    logger.info(f"[DIALOGUE EXTRACTION] Total scenes to check: {len(scenes)}")

    for scene_idx, scene in enumerate(scenes):
        characters_in_scene = scene.get("characters", [])
        if character_name not in characters_in_scene:
            continue

        logger.debug(f"[DIALOGUE EXTRACTION] Scene {scene_idx}: Character found, extracting...")

        text = scene.get("original_text", "")
        if not text:
            continue

        lines = text.split('\n')
        for line in lines:
            stripped = line.strip()
            is_dialogue = (
                stripped.startswith('"') or
                stripped.startswith("'") or
                stripped.startswith('\u201c') or  # Korean opening quote
                stripped.startswith('\u300c') or
                stripped.startswith('\u2014')  # Em-dash dialogue
            )
            if is_dialogue and len(stripped) > 3:
                cleaned = stripped.strip('\u2014').strip()
                dialogues.append(cleaned)
                logger.debug(f"[DIALOGUE EXTRACTION] Found: {cleaned[:50]}...")
                if len(dialogues) >= max_dialogues:
                    logger.info(f"[DIALOGUE EXTRACTION] Reached max dialogues ({max_dialogues})")
                    return dialogues

    logger.info(f"[DIALOGUE EXTRACTION] Extracted {len(dialogues)} dialogues for {character_name}")
    return dialogues


def _fetch_analysis_for_character(
    db: Session, novel_id: int, chapter_id: Optional[int], character_name: str
):
    """ë¶„ì„ ë°ì´í„° ì¡°íšŒ. ì±•í„°ë³„ ë¶„ì„ â†’ ë²¡í„° ë©”íƒ€ë°ì´í„° ì§‘ê³„ â†’ ê¸€ë¡œë²Œ ë¶„ì„ ìˆœìœ¼ë¡œ ì‹œë„."""
    query = db.query(Analysis).filter(
        Analysis.novel_id == novel_id,
        Analysis.status == "completed"
    )
    if chapter_id:
        query = query.filter(Analysis.chapter_id == chapter_id)

    analysis = query.order_by(desc(Analysis.created_at)).first()

    if not analysis and chapter_id:
        logger.info(f"[Persona] No Analysis found for chapter {chapter_id}. Aggregating from VectorDocument metadata...")
        vectors = db.query(VectorDocument).filter(
            VectorDocument.novel_id == novel_id,
            VectorDocument.chapter_id == chapter_id
        ).all()

        if vectors:
            aggregated_char = None
            for vec in vectors:
                meta = vec.metadata_json or {}
                for char in meta.get('characters', []):
                    c_name = char.get('name') if isinstance(char, dict) else char
                    if c_name != character_name:
                        continue
                    if not aggregated_char:
                        aggregated_char = {
                            "name": c_name,
                            "description": char.get('description', '') if isinstance(char, dict) else '',
                            "traits": char.get('traits', []) if isinstance(char, dict) else []
                        }
                    else:
                        new_traits = char.get('traits', []) if isinstance(char, dict) else []
                        for t in new_traits:
                            if t not in aggregated_char['traits']:
                                aggregated_char['traits'].append(t)
                        new_desc = char.get('description', '') if isinstance(char, dict) else ''
                        if len(new_desc) > len(aggregated_char['description']):
                            aggregated_char['description'] = new_desc

            if aggregated_char:
                class MockAnalysis:
                    def __init__(self, data, nid):
                        self.result = data
                        self.id = "mock_vector_aggregation"
                        self.novel_id = nid
                        self.analysis_type = "vector_aggregation"

                analysis = MockAnalysis({
                    "characters": [aggregated_char],
                    "summary": "Generated from Vector Metadata",
                    "mood": "Unknown"
                }, novel_id)
                logger.info(f"[Persona] Successfully aggregated data for '{character_name}' from {len(vectors)} scenes.")
            else:
                logger.warning(f"[Persona] Character '{character_name}' not found in chapter {chapter_id} vectors.")

    if not analysis or not getattr(analysis, 'result', None):
        if chapter_id:
            logger.warning(f"[Persona] Strict mode: No analysis found for chapter {chapter_id}. Aborting.")
            raise HTTPException(
                status_code=404,
                detail=f"No analysis or character data found for '{character_name}' in this chapter."
            )
        # Legacy fallback (Global)
        analysis = db.query(Analysis).filter(
            Analysis.novel_id == novel_id,
            Analysis.analysis_type == AnalysisType.CHARACTER,
            Analysis.status == "completed"
        ).order_by(desc(Analysis.created_at)).first()

    if analysis:
        logger.debug(f"[Persona] Found analysis ID: {analysis.id}, type: {analysis.analysis_type}")
    else:
        logger.warning(f"[Persona] NO ANALYSIS FOUND for novel_id={novel_id}")

    if not analysis or not analysis.result:
        raise HTTPException(
            status_code=404,
            detail="Analysis data not found for this novel. Please run analysis first."
        )

    return analysis


def _find_character_in_analysis(analysis, character_name: str) -> dict:
    """ë¶„ì„ ê²°ê³¼ì—ì„œ ìºë¦­í„° ë°ì´í„° ì°¾ê¸° (ì •í™• ë§¤ì¹­ â†’ ë¶€ë¶„ ë§¤ì¹­)."""
    data = analysis.result
    logger.debug(f"[Persona] Analysis type: {analysis.analysis_type}, data type: {type(data)}")

    characters_list = []
    if isinstance(data, dict):
        characters_list = data.get("characters", []) or data.get("character_analysis", []) or []
        logger.debug(f"[Persona] Characters list length: {len(characters_list)}")
    elif isinstance(data, list):
        characters_list = data

    def normalize_name(name: str) -> str:
        return name.replace(" ", "").lower() if name else ""

    target_norm = normalize_name(character_name)
    logger.debug(f"[Persona] Looking for normalized name: '{target_norm}'")

    # First pass: Exact match
    for char in characters_list:
        if not isinstance(char, dict):
            continue
        char_name = char.get("name") or char.get("character_name")
        if normalize_name(char_name) == target_norm:
            return char

    # Second pass: Partial match
    for char in characters_list:
        if not isinstance(char, dict):
            continue
        char_name = char.get("name") or char.get("character_name")
        char_name_norm = normalize_name(char_name)
        if target_norm in char_name_norm or char_name_norm in target_norm:
            logger.debug(f"[Persona] Found partial match: '{char_name}'")
            return char

    available_names = [
        char.get("name") or char.get("character_name")
        for char in characters_list[:10]
        if isinstance(char, dict) and (char.get("name") or char.get("character_name"))
    ]
    error_detail = f"Character '{character_name}' not found in analysis."
    if available_names:
        error_detail += f" Available characters: {', '.join(available_names)}"
    else:
        error_detail += " No characters found in analysis data."
    raise HTTPException(status_code=404, detail=error_detail)


def _build_persona_meta_prompt(
    character_data: dict, character_name: str, analysis_result: dict, dialogues: list
) -> str:
    """í˜ë¥´ì†Œë‚˜ ìƒì„±ì„ ìœ„í•œ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ êµ¬ì¶•."""
    relations_text = ""
    if "relationships" in analysis_result:
        rels = analysis_result["relationships"]
        char_rels = [
            r for r in rels
            if character_name in r.get("source", "") or character_name in r.get("target", "")
        ]
        if char_rels:
            relations_text = "\n" + "\n".join([
                f"- {r.get('target' if r.get('source') == character_name else 'source')}: "
                f"{r.get('relation')} ({r.get('description')})"
                for r in char_rels
            ])

    has_dialogues = bool(dialogues)
    if has_dialogues:
        dialogue_count = len(dialogues)
        dialogue_examples = f"\n\n[ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ (ì´ {dialogue_count}ê°œ)]"
        dialogue_examples += "\n" + "\n".join([f"{i+1}. {d}" for i, d in enumerate(dialogues)])
        dialogue_examples += (
            f"\n\nâš ï¸ ì¤‘ìš”: ìœ„ {dialogue_count}ê°œì˜ ì‹¤ì œ ëŒ€ì‚¬ë¥¼ ë°˜ë“œì‹œ ë¶„ì„í•˜ë¼. "
            f"ë§íˆ¬ íŒ¨í„´, ì–´ë¯¸ ì‚¬ìš© ë¹ˆë„, ë¬¸ì¥ ê¸¸ì´, ë°˜ë³µ í‘œí˜„ì„ í†µê³„ì ìœ¼ë¡œ ì¶”ì¶œí•˜ë¼."
        )
    else:
        logger.warning(f"[PERSONA] No dialogues found for {character_name}. Using general analysis only.")
        dialogue_examples = (
            "\n\n[ì•Œë¦¼: ì´ ìºë¦­í„°ì˜ ì§ì ‘ì ì¸ ëŒ€ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            "ì„±ê²© ì„¤ëª…ê³¼ íŠ¹ì§•ë§Œìœ¼ë¡œ ë§íˆ¬ë¥¼ ì¶”ë¡ í•˜ì„¸ìš”.]"
        )

    meta_prompt = f"""
ë„ˆëŠ” ì„¸ê³„ ìµœê³ ì˜ ìºë¦­í„° í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì´ë‹¤.
ì•„ë˜ ì›¹ì†Œì„¤ ìºë¦­í„°ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, ì´ ìºë¦­í„°ê°€ ë˜ì–´ ì‹¤ì‹œê°„ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•  AIì˜ **'ì‹œìŠ¤í…œ ì§€ì¹¨(System Instruction)'**ì„ ì‘ì„±í•˜ë¼.

[ë°ì´í„° ë² ì´ìŠ¤]
- ì´ë¦„: {character_data.get('name')}
- í•µì‹¬ ì„±ê²©: {character_data.get('description')}
- ì£¼ìš” íŠ¹ì§•/íƒœê·¸: {', '.join(character_data.get('traits', []))}{relations_text}{dialogue_examples}

[ì‹œìŠ¤í…œ ì§€ì¹¨ ì‘ì„± ê°€ì´ë“œë¼ì¸]
1. **1ì¸ì¹­ ì •ì²´ì„±**: "ë„ˆëŠ” ~ì´ë‹¤"ë¼ê³  ì •ì˜í•˜ë¼. ìºë¦­í„°ì˜ ë‚´ë©´ ì‹¬ë¦¬ì™€ ê°€ì¹˜ê´€ì„ í¬í•¨í•˜ë¼.

2. **ë§íˆ¬ì˜ ë¬¼ë¦¬ì  ê·œì¹™**:
   {'   - ğŸ¯ ìœ„ì— ì œê³µëœ [ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ]ë¥¼ í•œ ì¤„ í•œ ì¤„ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ë¼' if has_dialogues else '   - ì„±ê²© ì„¤ëª…ê³¼ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ë§íˆ¬ë¥¼ ì¶”ë¡ í•˜ë¼'}
   {'   - ëŒ€ì‚¬ì—ì„œ ê°€ì¥ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì–´ë¯¸ë¥¼ ì¶”ì¶œí•˜ê³  ë¹ˆë„ë¥¼ ê³„ì‚° (ì˜ˆ: ~ì–´ìš” 30%, ~ë‹¤ 20%)' if has_dialogues else '   - ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ì–´ë¯¸ë¥¼ ì„¤ì • (ì˜ˆ: ê¶Œìœ„ì ì´ë©´ ~ë¼/~ë‹¤)'}
   {'   - ì‹¤ì œ ëŒ€ì‚¬ì˜ í‰ê·  ë¬¸ì¥ ê¸¸ì´ë¥¼ ì¸¡ì • (ì§§ìŒ/ì¤‘ê°„/ê¸º)' if has_dialogues else '   - ì„±ê²©ì— ë§ëŠ” ë¬¸ì¥ ê¸¸ì´ ê²°ì •'}
   {'   - ëŒ€ì‚¬ì—ì„œ ë°˜ë³µë˜ëŠ” ê°íƒ„ì‚¬, ì ‘ì†ì‚¬, ë§ë²„ë¦‡ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¶”ì¶œ (ì˜ˆ: "ê·¸ë˜", "ìŒ...")' if has_dialogues else '   - ìºë¦­í„° íŠ¹ì„±ì— ë§ëŠ” ê°íƒ„ì‚¬/ë²„ë¦‡ ì„¤ì •'}
   {'   - ëŒ€ì‚¬ì—ì„œ ë†’ì„ë§/ë°˜ë§ ë¹„ìœ¨ì„ ì •í™•íˆ ê³„ì‚°í•˜ë¼' if has_dialogues else '   - ì„±ê²©ê³¼ ì„¤ì •ì— ë§ëŠ” ë†’ì„ë§/ë°˜ë§ ë¹„ìœ¨ ì„¤ì •'}
   - íŠ¹ì • ë¬¸ì¥ êµ¬ì¡°ë‚˜ íŒ¨í„´ì´ ìˆìœ¼ë©´ ê¸°ë¡í•˜ë¼

3. **ëŒ€í™” íƒœë„**:
   {'   - ì‹¤ì œ ëŒ€ì‚¬ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í‰ê·  ê°ì • í†¤ ë¶„ì„ (ê³µê²©ì /ì¹œê·¼í•¨/ë¬´ëšëší•¨/ì¥ë‚œê¸°/ê¶Œìœ„ì )' if has_dialogues else '   - ì„±ê²© ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ê°ì • í†¤ ì„¤ì •'}
   {'   - ëŒ€ì‚¬ì˜ ì§ˆë¬¸ vs ì§„ìˆ  ë¹„ìœ¨ì„ ê³„ì‚°í•˜ë¼' if has_dialogues else '   - ìºë¦­í„° íŠ¹ì„±ì— ë§ëŠ” ì§ˆë¬¸/ì§„ìˆ  ë¹„ìœ¨ ì„¤ì •'}
   - ê´€ê³„ ë°ì´í„°ì™€ {'ëŒ€ì‚¬ í†¤' if has_dialogues else 'ì„±ê²© íŠ¹ì§•'}ì„ ì¢…í•©í•´ ì‚¬ìš©ì ëŒ€í•˜ëŠ” íƒœë„ë¥¼ ì„¤ì •í•˜ë¼

4. **êµ¬ì–´ì²´ ë³€í™˜**: ì†Œì„¤ ì† ë”±ë”±í•œ ë¬¸ì–´ì²´ê°€ ì•„ë‹Œ, ì‹¤ì œ ëŒ€í™”ë‚˜ ë©”ì‹ ì €ì—ì„œ ì“¸ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì–´íˆ¬ë¥¼ ëª…ë ¹í•˜ë¼. ì‹¤ì œ ëŒ€ì‚¬ì˜ ìŠ¤íƒ€ì¼ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ë¼.

5. **ê¸ˆê¸° ì‚¬í•­**: ì ˆëŒ€ ê¸°ê³„ì ì¸ "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?" ì‹ì˜ ë‹µë³€ì„ í•˜ì§€ ë§ê³ , ìºë¦­í„°ë‹µì§€ ì•Šì€ ì¹œì ˆí•¨ì´ë‚˜ ë¹„ì†ì–´ë¥¼ ì œí•œí•˜ë¼.

[ì¶œë ¥ í˜•ì‹]
ì§€ì¹¨ ë‚´ìš©ë§Œ ì¶œë ¥í•  ê²ƒ.
    """

    if has_dialogues:
        meta_prompt += """

6. **ğŸ¯ ì¤‘ìš”: ì‹¤ì œ ëŒ€ì‚¬ í¬í•¨ í•„ìˆ˜**:
   - ìœ„ [ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ] ì¤‘ì—ì„œ ìºë¦­í„°ì˜ ë§íˆ¬ë¥¼ ê°€ì¥ ì˜ ë³´ì—¬ì£¼ëŠ” **10-15ê°œë¥¼ ì„ ë³„**í•˜ì—¬ ìƒì„±í•  ì‹œìŠ¤í…œ ì§€ì¹¨ì— í¬í•¨ì‹œì¼œë¼
   - ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œ ì§€ì¹¨ì˜ ë§íˆ¬ ê·œì¹™ ì„¤ëª… í›„ì— ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶”ê°€í•˜ë¼:

   ì˜ˆì‹œ í˜•ì‹:
   [ì°¸ê³ í•  ì‹¤ì œ ëŒ€ì‚¬ ì˜ˆì‹œ]
   1. "ëŒ€ì‚¬ ë‚´ìš©..."
   2. "ëŒ€ì‚¬ ë‚´ìš©..."
   ...(10-15ê°œ)

   ìœ„ ëŒ€ì‚¬ë¥¼ ì°¸ê³ í•˜ì—¬ ëŒ€í™”í•  ë•Œ ìœ ì‚¬í•œ ì–´íˆ¬ì™€ íŒ¨í„´ì„ ì‚¬ìš©í•˜ë¼.
    """

    meta_prompt += """

[í•„ìˆ˜ í¬í•¨ ê·œì¹™]
ìƒì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ê·œì¹™ë“¤ì„ í¬í•¨í•´ì•¼ í•œë‹¤:
"1. ë„ˆëŠ” ì†Œì„¤ ì„¤ì •ì— ê¸°ë°˜í•œ ìºë¦­í„° ì—­í• ê·¹ì„ ìˆ˜í–‰í•œë‹¤.
2. ì‚¬ìš©ìê°€ ë„¤ ê¸°ì–µì´ë‚˜ ì œê³µëœ ë§¥ë½ì— ì—†ëŠ” ê²ƒì„ ë¬¼ìœ¼ë©´, ëª¨ë¥¸ë‹¤ê³  í•˜ê±°ë‚˜ ì• ë§¤í•˜ê²Œ í”¼í•˜ë¼. ì ˆëŒ€ ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ˆë¼.
3. ì‚¬ìš©ìê°€ ì†Œì„¤ì˜ ì‚¬ì‹¤ê³¼ ëª¨ìˆœë˜ëŠ” ë§ì„ í•˜ë©´, ë„¤ ê¸°ì–µì„ ë°”íƒ•ìœ¼ë¡œ ìºë¦­í„°ë‹µê²Œ êµì •í•˜ë¼.
4. ë‹µë³€ì€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ë¼. íŠ¹ë³„íˆ ê¸´ ì„¤ëª…ì„ ìš”ì²­ë°›ì§€ ì•ŠëŠ” í•œ, ì¥í™©í•˜ê²Œ ì„¤ëª…í•˜ê±°ë‚˜ ì •ë³´ë¥¼ ëŠ˜ì–´ë†“ì§€ ë§ˆë¼. ì¼ë°˜ì ì¸ ë‹µë³€ì€ 1~3ë¬¸ì¥ì„ ëª©í‘œë¡œ í•˜ë¼."
    """

    return meta_prompt


def _fetch_character_bible(
    db: Session, novel_id: int, chapter_id: Optional[int], character_name: str
) -> str:
    """íŠ¹ì • ìºë¦­í„°ì˜ ì„¤ì • ì •ë³´(ê´€ê³„ í¬í•¨)ë¥¼ Analysis DBì—ì„œ ì¶”ì¶œ."""
    query = db.query(Analysis).filter(
        Analysis.novel_id == novel_id,
        Analysis.analysis_type == AnalysisType.CHARACTER
    )
    if chapter_id:
        query = query.filter(Analysis.chapter_id == chapter_id)
    analysis = query.order_by(Analysis.updated_at.desc()).first()

    if not analysis or not analysis.result:
        return ""

    result = analysis.result
    parts = []

    # í•´ë‹¹ ìºë¦­í„° ì •ë³´ ì¶”ì¶œ
    for c in result.get('characters', []):
        if c.get('name') == character_name:
            traits = ", ".join(c.get('traits', [])[:5])
            desc = c.get('description', '')[:150]
            parts.append(f"[{character_name} ì„¤ì •]\nì„±ê²©/íŠ¹ì§•: {desc}\nì£¼ìš” íŠ¹ì„±: {traits}")
            break

    # í•´ë‹¹ ìºë¦­í„° ê´€ë ¨ ê´€ê³„ ì¶”ì¶œ
    rel_lines = []
    for r in result.get('relationships', []):
        if character_name in (r.get('character1', ''), r.get('character2', '')):
            other = r.get('character2', '') if r.get('character1', '') == character_name else r.get('character1', '')
            rel_lines.append(f"- {other}: {r.get('description', '')[:80]}")
    if rel_lines:
        parts.append("[ê´€ê³„]\n" + "\n".join(rel_lines[:5]))

    return "\n\n".join(parts)[:400]


async def _perform_rag_search(
    db: Session, chatbot_service, room: CharacterChatRoom, message_content: str
) -> str:
    """RAG ê²€ìƒ‰ìœ¼ë¡œ ì†Œì„¤ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¡°íšŒ. ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜."""
    if not chatbot_service or not chatbot_service.engine:
        return ""

    try:
        loop = asyncio.get_running_loop()
        chunks = await loop.run_in_executor(
            None,
            partial(
                chatbot_service.find_similar_chunks,
                question=message_content,
                top_k=3,
                novel_id=room.novel_id,   # novel_idë¥¼ ì§ì ‘ ì „ë‹¬ (title ìš°íšŒ ì¡°íšŒ ë¶ˆí•„ìš”)
                chapter_id=room.chapter_id
            )
        )
        if not chunks:
            return ""
        rag_context = "\n\n[Reference Scenes from Novel]:\n"
        for chunk in chunks:
            rag_context += f"Scene {chunk.get('scene_index', '?')}: {chunk['text'][:500]}...\n"
        return rag_context
    except Exception as e:
        logger.error(f"RAG Search failed: {e}")
        return ""


def _parse_self_check_response(ai_reply: str, room_id: int, character_name: str) -> str:
    """AI ì‘ë‹µì—ì„œ SELF_CHECK íŒŒì‹± ë° ë¡œê¹… í›„ ì •ì œëœ ë©”ì‹œì§€ ë°˜í™˜."""
    logger.info(f"{'='*70}")
    logger.info(f"[AI RESPONSE DEBUG] Room {room_id} | Character: {character_name}")
    logger.info(f"Full response length: {len(ai_reply)} chars")
    logger.info(f"Contains [SELF_CHECK]: {'[SELF_CHECK]' in ai_reply}")
    logger.info(f"{'='*70}")

    if "[SELF_CHECK]" in ai_reply:
        parts = ai_reply.split("[SELF_CHECK]", 1)
        user_message = parts[0].strip()
        self_check_log = parts[1].strip()
        logger.info(f"[SELF-CHECK] Room {room_id} | Character: {character_name}")
        logger.info(f"[SELF-CHECK] {self_check_log}")
        return user_message

    logger.warning(f"[SELF-CHECK] âš ï¸ LLM did not include self-check for Room {room_id}")
    logger.warning(f"[SELF-CHECK] Response length: {len(ai_reply)} chars")
    logger.warning(f"[SELF-CHECK] First 200 chars: {ai_reply[:200]}...")
    logger.warning(f"[SELF-CHECK] Tip: Check prompt or try regenerating persona")
    return ai_reply


# ìºë¦­í„° ì±— ê³µí†µ í”„ë¡œí† ì½œ ì§€ì¹¨ (send_message + send_message_stream ê³µìœ )
_CHAT_PROTOCOL = """

[ì±„íŒ… í”„ë¡œí† ì½œ: ì‹¤ì‹œê°„ ë©”ì‹ ì € ëª¨ë“œ]
1. **ê°„ê²°ì„±**: í•œ ë²ˆì˜ ë‹µì¥ì€ 1~3ë¬¸ì¥ ì´ë‚´ë¡œ ì œí•œí•œë‹¤.
2. **ê¸°ì–µ í™œìš©**: [ê¸°ì–µ ë°ì´í„°]ëŠ” ë„ˆì˜ ì‹¤ì œ ê¸°ì–µì´ë‹¤. "ì†Œì„¤ì— ë”°ë¥´ë©´" ê°™ì€ í‘œí˜„ì„ ì ˆëŒ€ ì“°ì§€ ë§ê³ , ìì‹ ì˜ ê²½í—˜ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ë¼.
3. **ëª°ì… ìœ ì§€**: ì‚¬ìš©ìê°€ ì†Œì„¤ ì„¤ì •ê³¼ ë§ì§€ ì•ŠëŠ” ë§ì„ í•˜ë©´, ìºë¦­í„°ë‹µê²Œ ë°˜ì‘í•˜ë¼.
4. **í˜„ì¥ê°**: ì§ˆë¬¸ì—ë§Œ ë‹µí•˜ì§€ ë§ê³ , ê°€ë”ì€ ìºë¦­í„°ì˜ í˜„ì¬ ê°ì •ì´ë‚˜ ìƒí™©ì„ íˆ­ ë˜ì ¸ë¼.
5. **ê¸ˆì§€**: ì„¤ëª…ì¡°ì˜ ë§íˆ¬, ê¸´ ë¬¸ë‹¨, AIë‹¤ìš´ ì •ì¤‘í•¨ì„ ëª¨ë‘ ë²„ë ¤ë¼.
6. **ì ˆëŒ€ ê¸ˆì§€ â€” ì‚¬ì‹¤ ë‚ ì¡°**: ì†Œì„¤ ì† ì‚¬ê±´, ì¸ë¬¼ ê´€ê³„, ì¥ì†Œ, ëŒ€í™” ë‚´ìš© ë“± êµ¬ì²´ì ì¸ ì‚¬ì‹¤ì€ [ê¸°ì–µ ë°ì´í„°]ì— ìˆëŠ” ë‚´ìš©ë§Œ ë§í•˜ë¼. [ê¸°ì–µ ë°ì´í„°]ì— "ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"ë¼ê³  ë‚˜ì˜¤ê±°ë‚˜ ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´, ê·¸ ì‚¬ì‹¤ì— ëŒ€í•´ì„œëŠ” ë°˜ë“œì‹œ "ì˜ ê¸°ì–µì´ ì•ˆ ë‚˜" ë˜ëŠ” í™”ì œ ì „í™˜ìœ¼ë¡œ ëŒ€ì‘í•˜ë¼. ë„¤ í•™ìŠµ ë°ì´í„°ë‚˜ ìƒìƒìœ¼ë¡œ ì†Œì„¤ ë‚´ìš©ì„ ì§€ì–´ë‚´ëŠ” ê²ƒì€ ì ˆëŒ€ ê¸ˆì§€ë‹¤.

[í•„ìˆ˜ ì¶œë ¥ í˜•ì‹]
ëª¨ë“  ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ì„ EXACTLY ë”°ë¼ì•¼ í•œë‹¤:

<ë‹µë³€ ë‚´ìš©ì„ ê·¸ëƒ¥ ì—¬ê¸°ì— ì“´ë‹¤. íƒœê·¸ ì—†ì´ ê·¸ëƒ¥ ë¬¸ì¥ë§Œ.>

[SELF_CHECK]
Checklist: X/5 | Confidence: Y.Y/5.0 | Notes: ê°„ë‹¨ ë©”ëª¨

ì¤‘ìš”:
- ë‹µë³€ì— [CHARACTER_MESSAGE]ë‚˜ ë‹¤ë¥¸ íƒœê·¸ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆë¼
- ê·¸ëƒ¥ í‰ë²”í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ ë‹µë³€í•˜ë¼
- [SELF_CHECK] ì´ì „ì—ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ìˆì–´ì•¼ í•œë‹¤
- [SELF_CHECK] íƒœê·¸ëŠ” ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•œë‹¤

ì‹¤ì œ ì¶œë ¥ ì˜ˆì‹œ:
ê·¸ë˜, ë­”ê°€ í•„ìš”í•´?

[SELF_CHECK]
Checklist: 5/5 | Confidence: 4.5/5.0 | Notes: ì§§ê³  ìºë¦­í„°ë‹µê²Œ

ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©: (1)í˜ë¥´ì†Œë‚˜ ìœ ì§€ (2)ë§íˆ¬ ì¼ê´€ì„± (3)ì„¤ì • ì¤€ìˆ˜ (4)ê¸¸ì´ ì ì ˆ (5)RAG ìì—° í™œìš©
    """


# --- Router ---
router = APIRouter()


@router.post("/generate-persona", response_model=PersonaGenerationResponse)
async def generate_persona(
    request: PersonaGenerationRequest,
    db: Session = Depends(get_db)
):
    """Generate a persona system prompt for a character using analysis data."""
    if not client:
        raise HTTPException(
            status_code=500,
            detail="LLM client not initialized. Check GOOGLE_API_KEY."
        )

    logger.info(f"===== GENERATE PERSONA REQUEST =====")
    logger.info(f"novel_id: {request.novel_id}, character_name: {request.character_name}")

    analysis = _fetch_analysis_for_character(db, request.novel_id, request.chapter_id, request.character_name)
    character_data = _find_character_in_analysis(analysis, request.character_name)
    dialogues = extract_character_dialogues(analysis.result, request.character_name, max_dialogues=50)
    meta_prompt = _build_persona_meta_prompt(character_data, request.character_name, analysis.result, dialogues)

    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None,
            partial(client.models.generate_content, model='gemini-2.5-flash', contents=meta_prompt)
        )
        persona_prompt = response.text.strip()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate persona: {str(e)}")

    return PersonaGenerationResponse(
        character_name=request.character_name,
        persona_prompt=persona_prompt
    )


@router.put("/rooms/{room_id}", response_model=CharacterChatRoomResponse)
async def update_room(
    room_id: int,
    room_update: CharacterChatRoomUpdate,
    db: Session = Depends(get_db)
):
    """Update a character chat room (e.g. persona prompt)."""
    return CharacterChatService.update_room(db, room_id, room_update.persona_prompt)


@router.delete("/rooms/{room_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_room(
    room_id: int,
    db: Session = Depends(get_db)
):
    """Delete a character chat room."""
    CharacterChatService.delete_room(db, room_id)


@router.post("/rooms", response_model=CharacterChatRoomResponse)
async def create_room(
    room_data: CharacterChatRoomCreate,
    db: Session = Depends(get_db)
):
    """Create a new character chat room."""
    return CharacterChatService.create_room(
        db,
        novel_id=room_data.novel_id,
        chapter_id=room_data.chapter_id,
        character_name=room_data.character_name,
        persona_prompt=room_data.persona_prompt,
    )


@router.get("/rooms", response_model=List[CharacterChatRoomResponse])
async def list_rooms(
    novel_id: int,
    chapter_id: Optional[int] = None,
    db: Session = Depends(get_db)
):
    """List chat rooms for a novel."""
    return CharacterChatService.list_rooms(db, novel_id, chapter_id)


@router.post("/rooms/{room_id}/messages", response_model=List[CharacterChatMessageResponse])
async def send_message(
    room_id: int,
    message: CharacterChatMessageCreate,
    db: Session = Depends(get_db)
):
    """Send a message to the character and get a response."""
    room = CharacterChatService.get_room(db, room_id)

    if not client:
        raise HTTPException(status_code=500, detail="LLM client not initialized")

    # 1. Save User Message
    user_msg = CharacterChatMessage(room_id=room.id, role="user", content=message.content)
    db.add(user_msg)
    db.commit()

    # 2. Fetch chat history
    history_records = db.query(CharacterChatMessage).filter(
        CharacterChatMessage.room_id == room.id
    ).order_by(CharacterChatMessage.created_at).all()

    # 3. RAG: Retrieve Context
    chatbot_service = get_chatbot_service()
    rag_context = await _perform_rag_search(db, chatbot_service, room, message.content)

    # 4. Build input with optional RAG context
    # ìºë¦­í„° ì„¤ì • ë°”ì´ë¸” ì¡°íšŒ (Method C - ìºë¦­í„° ìŠ¬ë¼ì´ìŠ¤)
    char_bible = _fetch_character_bible(db, room.novel_id, room.chapter_id, room.character_name)
    bible_prefix = f"### [ìºë¦­í„° ì„¤ì •] ###\n{char_bible}\n\n" if char_bible else ""

    # RAG ê²°ê³¼ê°€ ì—†ì–´ë„ [ê¸°ì–µ ë°ì´í„°] ë¸”ë¡ì„ í•­ìƒ í¬í•¨ì‹œì¼œ,
    # LLMì´ ê´€ë ¨ ì†Œì„¤ ì¥ë©´ì´ ì—†ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì¸ì§€í•˜ë„ë¡ í•œë‹¤.
    memory_block = rag_context if rag_context else "[ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì†Œì„¤ ì¥ë©´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.]"
    input_text = f"""{bible_prefix}### [ê¸°ì–µ ë°ì´í„°] ###
{memory_block}

### [ì‚¬ìš©ìì˜ ë©”ì‹œì§€] ###
{message.content}"""

    # Build system instruction
    system_instruction = room.persona_prompt + _CHAT_PROTOCOL

    # Build conversation history
    contents = []
    for msg in history_records:
        if msg.id == user_msg.id:
            continue  # í˜„ì¬ ë©”ì‹œì§€ëŠ” RAG ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì•„ë˜ì„œ ì¶”ê°€
        role = "user" if msg.role == "user" else "model"
        contents.append(types.Content(role=role, parts=[types.Part(text=msg.content)]))
    contents.append(types.Content(role="user", parts=[types.Part(text=input_text)]))

    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None,
            partial(
                client.models.generate_content,
                model='gemini-2.5-flash',
                contents=contents,
                config=types.GenerateContentConfig(system_instruction=system_instruction)
            )
        )
        ai_reply = _parse_self_check_response(response.text.strip(), room_id, room.character_name)
    except Exception as e:
        ai_reply = "..."
        logger.error(f"Error generating chat response: {e}")

    # 5. Save AI Message
    ai_msg = CharacterChatMessage(room_id=room.id, role="assistant", content=ai_reply)
    db.add(ai_msg)
    room.updated_at = datetime.utcnow()
    db.commit()
    db.refresh(user_msg)
    db.refresh(ai_msg)

    return [user_msg, ai_msg]


@router.get("/rooms/{room_id}/messages", response_model=List[CharacterChatMessageResponse])
async def get_messages(
    room_id: int,
    db: Session = Depends(get_db)
):
    """Get message history for a room."""
    return CharacterChatService.get_messages(db, room_id)


@router.post("/rooms/{room_id}/messages/stream")
async def send_message_stream(
    room_id: int,
    message: CharacterChatMessageCreate,
    db: Session = Depends(get_db)
):
    """
    ìºë¦­í„° ì±— ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (SSE)

    ì´ë²¤íŠ¸ í˜•ì‹:
      data: {"type": "user_saved", "id": ..., "content": "...", "created_at": "..."}
      data: {"type": "token", "text": "..."}
      data: {"type": "done", "ai_id": ..., "ai_content": "...", "created_at": "..."}
    """
    room = CharacterChatService.get_room(db, room_id)

    if not client:
        raise HTTPException(status_code=500, detail="LLM client not initialized")

    # 1. Save User Message
    user_msg = CharacterChatMessage(room_id=room.id, role="user", content=message.content)
    db.add(user_msg)
    db.commit()
    db.refresh(user_msg)

    # 2. Fetch chat history
    history_records = db.query(CharacterChatMessage).filter(
        CharacterChatMessage.room_id == room.id
    ).order_by(CharacterChatMessage.created_at).all()

    # 3. RAG + ë°”ì´ë¸”
    chatbot_service = get_chatbot_service()
    rag_context = await _perform_rag_search(db, chatbot_service, room, message.content)
    char_bible = _fetch_character_bible(db, room.novel_id, room.chapter_id, room.character_name)
    bible_prefix = f"### [ìºë¦­í„° ì„¤ì •] ###\n{char_bible}\n\n" if char_bible else ""
    memory_block = rag_context if rag_context else "[ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì†Œì„¤ ì¥ë©´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.]"
    input_text = f"""{bible_prefix}### [ê¸°ì–µ ë°ì´í„°] ###
{memory_block}

### [ì‚¬ìš©ìì˜ ë©”ì‹œì§€] ###
{message.content}"""

    system_instruction = room.persona_prompt + _CHAT_PROTOCOL

    # 4. Build conversation history
    contents = []
    for msg in history_records:
        if msg.id == user_msg.id:
            continue
        role = "user" if msg.role == "user" else "model"
        contents.append(types.Content(role=role, parts=[types.Part(text=msg.content)]))
    contents.append(types.Content(role="user", parts=[types.Part(text=input_text)]))

    loop = asyncio.get_running_loop()

    async def generate():
        # ìœ ì € ë©”ì‹œì§€ ì €ì¥ ì™„ë£Œ ì•Œë¦¼
        yield f"data: {json.dumps({'type': 'user_saved', 'id': user_msg.id, 'content': user_msg.content, 'created_at': user_msg.created_at.isoformat()})}\n\n"

        # Gemini ìŠ¤íŠ¸ë¦¬ë°: ë™ê¸° â†’ ë¹„ë™ê¸° ë³€í™˜
        queue: asyncio.Queue = asyncio.Queue()

        def _run_stream():
            try:
                for chunk in client.models.generate_content_stream(
                    model='gemini-2.5-flash',
                    contents=contents,
                    config=types.GenerateContentConfig(system_instruction=system_instruction)
                ):
                    if chunk.text:
                        asyncio.run_coroutine_threadsafe(queue.put(chunk.text), loop)
            except Exception as e:
                asyncio.run_coroutine_threadsafe(queue.put(e), loop)
            finally:
                asyncio.run_coroutine_threadsafe(queue.put(None), loop)

        loop.run_in_executor(None, _run_stream)

        # [SELF_CHECK] í•„í„°ë§í•˜ë©° ìŠ¤íŠ¸ë¦¬ë°
        full_text = ""
        sent_chars = 0

        while True:
            item = await queue.get()
            if item is None:
                break
            if isinstance(item, Exception):
                yield f"data: {json.dumps({'type': 'error', 'text': str(item)})}\n\n"
                break

            full_text += item
            check_pos = full_text.find("[SELF_CHECK]")

            if check_pos >= 0:
                # [SELF_CHECK] ë°œê²¬: ê·¸ ì´ì „ í…ìŠ¤íŠ¸ë§Œ ì „ì†¡
                visible = full_text[:check_pos].rstrip()
                if len(visible) > sent_chars:
                    new_text = visible[sent_chars:]
                    if new_text:
                        yield f"data: {json.dumps({'type': 'token', 'text': new_text})}\n\n"
                break
            else:
                new_text = full_text[sent_chars:]
                if new_text:
                    yield f"data: {json.dumps({'type': 'token', 'text': new_text})}\n\n"
                sent_chars = len(full_text)

        # AI ë©”ì‹œì§€ ì €ì¥
        parsed_reply = _parse_self_check_response(full_text.strip(), room_id, room.character_name)
        ai_msg = CharacterChatMessage(room_id=room.id, role="assistant", content=parsed_reply)
        db.add(ai_msg)
        room.updated_at = datetime.utcnow()
        db.commit()
        db.refresh(ai_msg)

        yield f"data: {json.dumps({'type': 'done', 'ai_id': ai_msg.id, 'ai_content': ai_msg.content, 'created_at': ai_msg.created_at.isoformat()})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    )
