"""
ì†Œì„¤ ì„¤ì • ë° ê°œì—°ì„± ê²€ì‚¬ API ì—”ë“œí¬ì¸íŠ¸
- ê¸°ì¡´ check.pyì˜ ê¸°ëŠ¥ì„ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ì¶° ê³ ë„í™”
- Pinecone ê¸°ë°˜ ë§¥ë½ ê²€ìƒ‰ ë° DB ê¸°ë°˜ ë°”ì´ë¸” ì„¤ì • ë¹„êµ
"""

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional

from backend.core.config import settings
from backend.db.session import get_db
from backend.db.models import Analysis, AnalysisType
from backend.services.analysis.embedding_engine import EmbeddingSearchEngine

router = APIRouter()

class ConsistencyRequest(BaseModel):
    current_text: str
    novel_id: int

class StoryValidator:
    def __init__(self, db: Session, engine: EmbeddingSearchEngine):
        self.db = db
        self.engine = engine
        self.client = OpenAI(
            api_key=settings.GOOGLE_API_KEY,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai"
        )

    def run_analysis(self, current_text: str, novel_id: int):
        """
        'ì„¤ì •íŒŒê´´ë¶„ì„ê¸°' ë©”ì¸ ë¶„ì„ í”„ë¡œì„¸ìŠ¤
        """
        # STEP 1: Pinecone ë§¥ë½ ê²€ìƒ‰ (ìŠ¤í† ë¦¬ë³´ë“œ)
        search_res = self.engine.search(query=current_text, novel_id=novel_id, top_k=5)
        storyboard_context = [res['document'].get('summary', '') for res in search_res]
        
        if not storyboard_context:
            storyboard_context = ["ê³¼ê±° ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."]

        # STEP 2: PostgreSQL ë°”ì´ë¸” ì¡°íšŒ (JSONB)
        bible_record = self.db.query(Analysis).filter(
            Analysis.novel_id == novel_id,
            Analysis.analysis_type == AnalysisType.CHARACTER
        ).first()
        
        bible_settings = bible_record.result if bible_record else "ë“±ë¡ëœ ë°”ì´ë¸”/ìºë¦­í„° ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤."

        # STEP 3: LLM í†µí•© ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±
        return self.generate_llm_report(current_text, storyboard_context, bible_settings)

    def generate_llm_report(self, text, context, bible):
        prompt = f"""
ë‹¹ì‹ ì€ ì†Œì„¤ ì „ë¬¸ í¸ì§‘ìì…ë‹ˆë‹¤. ì‘ê°€ê°€ ì“´ [ìµœê·¼ ì‘ì„± ë‚´ìš©]ì´ [ë°”ì´ë¸”] ë° [ê³¼ê±° ì¤„ê±°ë¦¬]ì™€ ì¶©ëŒí•˜ëŠ”ì§€ í•µì‹¬ë§Œ ë¶„ì„í•˜ì„¸ìš”.

[ìµœê·¼ ì‘ì„± ë‚´ìš©]:
{text}

[ì„¤ì • ë°”ì´ë¸”]:
{bible}

[ê³¼ê±° ì¤„ê±°ë¦¬]:
{context}

---

### ì§€ì¹¨:
1. **ê°€ì¥ ì‹¬ê°í•œ ì˜¤ë¥˜ í•˜ë‚˜**ë§Œ ê³¨ë¼ì„œ ì•„ì£¼ ê°„ê²°í•˜ê²Œ ë¦¬í¬íŠ¸í•˜ì„¸ìš”. 
2. **ë°˜ë“œì‹œ** ë¬¸ì œê°€ ëœ êµ¬ì²´ì ì¸ ë¬¸ì¥ì„ [í˜„ì¬ ë¬¸ì¥] í•­ëª©ì— ì ì–´ì£¼ì„¸ìš”. 
3. ì „ì²´ ë¶„ëŸ‰ì€ 10~15ì¤„ ë‚´ì™¸ë¡œ ì§§ê²Œ ìœ ì§€í•˜ì„¸ìš”.

### ë¦¬í¬íŠ¸ ì–‘ì‹:
#### ### ì†Œì„¤ í¸ì§‘ ë¦¬í¬íŠ¸

**[í˜„ì¬ ë¬¸ì¥]:** "ì˜¤ë¥˜ê°€ ë°œê²¬ëœ ì‹¤ì œ ë¬¸ì¥ì„ ì—¬ê¸°ì— ë”°ì˜´í‘œë¡œ ì ìœ¼ì„¸ìš”"

1. âš ï¸ ì„¤ì • ì¶©ëŒ: (ê°„ê²°í•˜ê²Œ í•œ ì¤„)
2. âš™ï¸ ê°œì—°ì„± ê²½ê³ : (ê°„ê²°í•˜ê²Œ í•œ ì¤„)
3. ğŸ—£ï¸ ë³´ì´ìŠ¤ ë¶ˆì¼ì¹˜: (ê°„ê²°í•˜ê²Œ í•œ ì¤„)

**[ì¢…í•© ì˜ê²¬]:** í•œë‘ ë¬¸ì¥ìœ¼ë¡œ í•´ê²°ì±… ì œì‹œ.
"""
        
        response = self.client.chat.completions.create(
            model=settings.GEMINI_MODEL,
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” í•µì‹¬ë§Œ ì§šì–´ì£¼ëŠ” ìœ ëŠ¥í•œ ì†Œì„¤ í¸ì§‘ìì´ë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        report_content = response.choices[0].message.content
        
        # [í˜„ì¬ ë¬¸ì¥]: ë’¤ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ target_sentenceë¡œ í™œìš© (í”„ë¡ íŠ¸ì—”ë“œ ë„¤ë¹„ê²Œì´ì…˜ìš©)
        import re
        target_match = re.search(r'\*\*\[í˜„ì¬ ë¬¸ì¥\]:\*\* "(.*?)"', report_content)
        target_sentence = target_match.group(1) if target_match else ""

        return {
            "report": report_content,
            "target_sentence": target_sentence,
            "metadata": {
                "context_count": len(context) if isinstance(context, list) else 0,
                "has_bible": bible != "ë“±ë¡ëœ ë°”ì´ë¸”/ìºë¦­í„° ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        }

# ì „ì—­ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ (ìµœì´ˆ ìš”ì²­ ì‹œ ë¡œë“œ)
_search_engine = None

def get_search_engine() -> EmbeddingSearchEngine:
    global _search_engine
    if _search_engine is None:
        try:
            _search_engine = EmbeddingSearchEngine()
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="ê²€ìƒ‰ ì—”ì§„ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
    return _search_engine

@router.post("/check")
async def check_consistency(
    request_data: ConsistencyRequest,
    db: Session = Depends(get_db),
    engine: EmbeddingSearchEngine = Depends(get_search_engine)
):
    """
    í˜„ì¬ ì‘ì„± ì¤‘ì¸ ë¬¸ì¥ê³¼ ì†Œì„¤ì˜ ê¸°ì¡´ ì„¤ì •(ë°”ì´ë¸”) ë° ê³¼ê±° ì¤„ê±°ë¦¬ ê°„ì˜ ì¼ê´€ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
    """
    try:
        validator = StoryValidator(db, engine)
        result = validator.run_analysis(request_data.current_text, request_data.novel_id)
        return result

    except Exception as e:
        print(f"âŒ Consistency check failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¼ê´€ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
