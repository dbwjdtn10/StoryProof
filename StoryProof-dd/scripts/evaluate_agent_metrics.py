"""
Agent ì§€í‘œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (LLM-as-a-Judge)
==========================================
StoryConsistencyAgentì˜ ì„¤ì • ì¼ê´€ì„± ê²€ì‚¬ë¥¼ ì‹¤í–‰í•˜ê³ , Geminiê°€ 3ê°€ì§€ ì§€í‘œë¡œ ì±„ì í•©ë‹ˆë‹¤.

ì§€í‘œ:
1. Tool Use Accuracy (ë„êµ¬ í™œìš© ì •í™•ë„) â€” ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì˜¬ë°”ë¥´ê²Œ ê²€ìƒ‰í–ˆëŠ”ê°€
2. Reasoning Quality (ì¶”ë¡  í’ˆì§ˆ) â€” íŒë‹¨ ë¡œì§ì´ í•©ë¦¬ì ì¸ê°€
3. Output Completeness (ì¶œë ¥ ì™„ì „ì„±) â€” ì‘ë‹µì´ ì™„ì „í•œê°€ (êµ¬ì ˆ, ì„¤ëª…, ì œì•ˆ í¬í•¨)
+ ì •í™•ë„(Accuracy) â€” expected_statusì™€ ì‹¤ì œ status ì¼ì¹˜ ì—¬ë¶€

ì‚¬ìš©ë²•:
    python scripts/evaluate_agent_metrics.py --dataset eval_dataset.json
    python scripts/evaluate_agent_metrics.py --dataset eval_dataset.json --max-samples 5
"""

import os
import sys
import json
import time
import asyncio
import argparse
from typing import Dict, List
from tqdm import tqdm
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from google import genai
from dotenv import load_dotenv
from backend.core.config import settings

load_dotenv()

# ===== ì„¤ì • =====
OUTPUT_FILE = "agent_eval_results.json"


# ===== LLM-as-a-Judge í”„ë¡¬í”„íŠ¸ =====
TOOL_USE_ACCURACY_PROMPT = """ë‹¹ì‹ ì€ AI Agentì˜ ë„êµ¬ í™œìš© í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[í…ŒìŠ¤íŠ¸ ì…ë ¥ ë¬¸ì¥]:
{input_text}

[Agentê°€ ê²€ìƒ‰í•œ ì»¨í…ìŠ¤íŠ¸]:
{retrieved_context}

[ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…]:
{explanation}

Agentê°€ ì„¤ì • ì¼ê´€ì„± ê²€ì‚¬ë¥¼ ìœ„í•´ ì ì ˆí•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í–ˆëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- 1ì : ì™„ì „íˆ ë¬´ê´€í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰
- 2ì : ì•½ê°„ ê´€ë ¨ ìˆì§€ë§Œ íŒë‹¨ì— ë¶€ì¡±
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ ìˆëŠ” ì»¨í…ìŠ¤íŠ¸
- 4ì : ëŒ€ë¶€ë¶„ ê´€ë ¨ ìˆê³  íŒë‹¨ì— ì¶©ë¶„í•œ ì •ë³´
- 5ì : íŒë‹¨ì— ì™„ë²½íˆ ì í•©í•œ ë†’ì€ ê´€ë ¨ì„±ì˜ ì»¨í…ìŠ¤íŠ¸

JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

REASONING_QUALITY_PROMPT = """ë‹¹ì‹ ì€ AI Agentì˜ ì¶”ë¡  í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[í…ŒìŠ¤íŠ¸ ì…ë ¥ ë¬¸ì¥]:
{input_text}

[ì˜ˆìƒ ê²°ê³¼]: {expected_status}
[ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•]: {scenario_type}
[ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…]: {explanation}

[Agentì˜ ì‹¤ì œ ì‘ë‹µ]:
{agent_response}

Agentì˜ íŒë‹¨ ë¡œì§ì´ í•©ë¦¬ì ì¸ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
ì„¤ì • ì¶©ëŒ/ì¼ì¹˜ ì—¬ë¶€ë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒë‹¨í–ˆëŠ”ì§€, ê·¼ê±°ê°€ íƒ€ë‹¹í•œì§€ í‰ê°€í•©ë‹ˆë‹¤.

í‰ê°€ ê¸°ì¤€:
- 1ì : ì™„ì „íˆ ì˜ëª»ëœ íŒë‹¨, ë…¼ë¦¬ì  ì˜¤ë¥˜
- 2ì : íŒë‹¨ì´ ë¶€ì •í™•í•˜ê³  ê·¼ê±°ê°€ ì•½í•¨
- 3ì : ë¶€ë¶„ì ìœ¼ë¡œ ë§ëŠ” íŒë‹¨ì´ë‚˜ ë…¼ë¦¬ì— í—ˆì 
- 4ì : ëŒ€ì²´ë¡œ í•©ë¦¬ì  íŒë‹¨, ì‚¬ì†Œí•œ ê°œì„  ì—¬ì§€
- 5ì : ì™„ë²½íˆ í•©ë¦¬ì ì´ê³  ê·¼ê±°ê°€ ëª…í™•í•œ íŒë‹¨

JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""

OUTPUT_COMPLETENESS_PROMPT = """ë‹¹ì‹ ì€ AI Agentì˜ ì¶œë ¥ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[Agentì˜ ì‘ë‹µ]:
{agent_response}

[ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•]: {scenario_type}

Agentì˜ ì‘ë‹µì´ ì™„ì „í•œì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ì™„ì „í•œ ì‘ë‹µì˜ ì¡°ê±´ (ì„¤ì • íŒŒê´´ ê°ì§€ ì‹œ):
- status í•„ë“œ ì¡´ì¬
- ë¬¸ì œ êµ¬ì ˆ(quote) ì¸ìš©
- ë¬¸ì œ ì„¤ëª…(description)
- ìˆ˜ì • ì œì•ˆ(suggestion)
- results ë°°ì—´ì— ì„¸ë¶€ í•­ëª© í¬í•¨

ì™„ì „í•œ ì‘ë‹µì˜ ì¡°ê±´ (ì„¤ì • ì¼ì¹˜ ì‹œ):
- status í•„ë“œ ì¡´ì¬
- ì¼ì¹˜ë¥¼ í™•ì¸í•˜ëŠ” ê°„ë‹¨í•œ ì„¤ëª…

í‰ê°€ ê¸°ì¤€:
- 1ì : í•„ìˆ˜ í•„ë“œ ëŒ€ë¶€ë¶„ ëˆ„ë½, ë¶ˆì™„ì „í•œ JSON
- 2ì : ì¼ë¶€ í•„ë“œë§Œ ì¡´ì¬, í•µì‹¬ ì •ë³´ ëˆ„ë½
- 3ì : ê¸°ë³¸ êµ¬ì¡°ëŠ” ìˆìœ¼ë‚˜ ì„¸ë¶€ ì‚¬í•­ ë¶€ì¡±
- 4ì : ëŒ€ë¶€ë¶„ì˜ í•„ë“œ ì¡´ì¬, ì•½ê°„ì˜ ë³´ì™„ í•„ìš”
- 5ì : ëª¨ë“  í•„ìˆ˜ í•„ë“œê°€ ì™„ë²½íˆ í¬í•¨ëœ ì‘ë‹µ

JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"score": 1-5, "reason": "íŒì • ì‚¬ìœ "}}"""


def get_gemini_client():
    """Gemini API í´ë¼ì´ì–¸íŠ¸"""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return genai.Client(api_key=api_key)


def judge_metric(client, prompt: str, max_retries: int = 3) -> Dict:
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ì§€í‘œ ì±„ì """
    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config={
                    'response_mime_type': 'application/json',
                    'temperature': 0.1
                }
            )
            result = json.loads(response.text)
            score = int(result.get("score", 0))
            if 1 <= score <= 5:
                return {"score": score, "reason": result.get("reason", "")}
        except Exception as e:
            print(f"  âš ï¸ ì±„ì  ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
            time.sleep(1)
    
    return {"score": 0, "reason": "ì±„ì  ì‹¤íŒ¨"}


def resolve_novel_id(novel_filename: str):
    """
    novel_filename(ì˜ˆ: "KR_fantasy_alice.txt")ìœ¼ë¡œ DBì˜ novel_idë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ì¡°íšŒ ì „ëµ (ìˆœì°¨ ì‹œë„):
    1. ì˜ë¬¸ í‚¤ì›Œë“œ â†’ í•œêµ­ì–´ ë³€í™˜ í›„ Novel.title ë§¤ì¹­
    2. ì˜ë¬¸ í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ Novel.title ê²€ìƒ‰
    3. Novel.titleì— íŒŒì¼ëª… ì „ì²´ í¬í•¨ ê²€ìƒ‰
    """
    from backend.db.session import SessionLocal
    from backend.db.models import Novel
    
    # íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ì˜ë¬¸ í‚¤ì›Œë“œ â†’ í•œêµ­ì–´ ë§¤í•‘
    FILENAME_KR_MAP = {
        "alice": "ì•¨ë¦¬ìŠ¤",
        "jane": "ì œì¸",
        "sherlock": "ì…œë¡",
        "frankenstein": "í”„ë‘ì¼„",
        "jekyll": "ì§€í‚¬",
        "gatsby": "ê°œì¸ ë¹„",
        "pride": "ì˜¤ë§Œ",
        "dracula": "ë“œë¼í˜ë¼",
        "oz": "ì˜¤ì¦ˆ",
        "peterpan": "í”¼í„°íŒ¬",
        "treasure": "ë³´ë¬¼",
        "tomsawyer": "í†°",
        "mobydick": "ëª¨ë¹„ë”•",
        "timemachine": "íƒ€ì„ë¨¸ì‹ ",
        "warofworlds": "ìš°ì£¼ì „ìŸ",
        "karamazov": "ì¹´ë¼ë§ˆì¡°í”„",
    }
    
    db = SessionLocal()
    try:
        clean_name = novel_filename.replace('.txt', '')
        parts = clean_name.split('_')
        keywords = [p.lower() for p in parts if len(p) > 2 and p != "KR"]
        
        # ì „ëµ 1: ì˜ë¬¸ â†’ í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in keywords:
            kr_keyword = FILENAME_KR_MAP.get(keyword)
            if kr_keyword:
                novel = db.query(Novel).filter(
                    Novel.title.ilike(f"%{kr_keyword}%")
                ).first()
                if novel:
                    print(f"  [Novel ID] '{keyword}'â†’'{kr_keyword}' â†’ title='{novel.title}' â†’ id={novel.id}")
                    return novel.id
        
        # ì „ëµ 2: ì˜ë¬¸ í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ ê²€ìƒ‰
        for keyword in keywords:
            novel = db.query(Novel).filter(
                Novel.title.ilike(f"%{keyword}%")
            ).first()
            if novel:
                print(f"  [Novel ID] í‚¤ì›Œë“œ '{keyword}' â†’ title='{novel.title}' â†’ id={novel.id}")
                return novel.id
        
        # ì „ëµ 3: íŒŒì¼ëª… ì „ì²´ í¬í•¨ ê²€ìƒ‰
        novel = db.query(Novel).filter(Novel.title.contains(clean_name)).first()
        if novel:
            print(f"  [Novel ID] ì§ì ‘ ë§¤ì¹­ â†’ id={novel.id}")
            return novel.id
        
        # ì‹¤íŒ¨
        all_novels = db.query(Novel).all()
        print(f"  âš ï¸ [Novel ID] '{novel_filename}' ë§¤ì¹­ ì‹¤íŒ¨!")
        print(f"  ğŸ“‹ DB ì†Œì„¤ ëª©ë¡:")
        for n in all_novels[:10]:
            print(f"     - id={n.id}, title='{n.title}'")
        return None
        
    finally:
        db.close()


def run_agent_check(input_text: str, novel_filename: str) -> Dict:
    """
    StoryConsistencyAgent ì‹¤í–‰ (ë™ê¸° ë˜í¼)
    
    Returns:
        {"status": str, "results": list, "context_used": str, "raw_response": dict}
    """
    from backend.services.analysis.agent import StoryConsistencyAgent
    
    api_key = os.getenv("GOOGLE_API_KEY")
    agent = StoryConsistencyAgent(api_key=api_key)
    
    # novel_filenameìœ¼ë¡œ novel_id ì¡°íšŒ (ê°•í™”ëœ ë§¤ì¹­)
    novel_id = resolve_novel_id(novel_filename)
    
    if not novel_id:
        return {
            "status": "ì˜¤ë¥˜",
            "results": [],
            "context_used": "",
            "raw_response": {"error": f"ì†Œì„¤ '{novel_filename}'ì„ DBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        }
    
    print(f"  âœ… novel_id={novel_id} í™•ì¸ë¨")
    
    # Agentì˜ check_consistencyëŠ” asyncì´ë¯€ë¡œ asyncio.runìœ¼ë¡œ ì‹¤í–‰
    try:
        result = asyncio.run(agent.check_consistency(novel_id, input_text))
    except RuntimeError:
        # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(agent.check_consistency(novel_id, input_text))
        finally:
            loop.close()
    
    # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (Agent ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•œ ê²ƒ)
    search_results = agent.search_engine.search(
        input_text, novel_id=novel_id, top_k=5
    )
    context_texts = []
    for hit in search_results:
        doc = hit.get('document', {})
        text = doc.get('original_text', doc.get('text', ''))
        context_texts.append(text[:300])
    context_str = "\n---\n".join(context_texts)
    
    return {
        "status": result.get("status", "ì•Œ ìˆ˜ ì—†ìŒ"),
        "results": result.get("results", []),
        "context_used": context_str,
        "raw_response": result,
        "novel_id_resolved": novel_id
    }


def evaluate_single_scenario(client, scenario: Dict, agent_result: Dict) -> Dict:
    """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ 3ê°€ì§€ ì§€í‘œ + ì •í™•ë„ í‰ê°€"""
    input_text = scenario["input_text"]
    expected_status = scenario["expected_status"]
    scenario_type = scenario["scenario_type"]
    explanation = scenario.get("explanation", "ì„¤ëª… ì—†ìŒ")
    
    agent_response_str = json.dumps(agent_result["raw_response"], ensure_ascii=False, indent=2)
    
    metrics = {}
    
    # 1. Tool Use Accuracy
    prompt = TOOL_USE_ACCURACY_PROMPT.format(
        input_text=input_text,
        retrieved_context=agent_result["context_used"][:2000],
        explanation=explanation
    )
    metrics["tool_use_accuracy"] = judge_metric(client, prompt)
    
    # 2. Reasoning Quality
    prompt = REASONING_QUALITY_PROMPT.format(
        input_text=input_text,
        expected_status=expected_status,
        scenario_type=scenario_type,
        explanation=explanation,
        agent_response=agent_response_str[:2000]
    )
    metrics["reasoning_quality"] = judge_metric(client, prompt)
    
    # 3. Output Completeness
    prompt = OUTPUT_COMPLETENESS_PROMPT.format(
        agent_response=agent_response_str[:2000],
        scenario_type=scenario_type
    )
    metrics["output_completeness"] = judge_metric(client, prompt)
    
    # 4. Accuracy (ì§ì ‘ ë¹„êµ)
    actual_status = agent_result["status"]
    # ë¶€ë¶„ ë§¤ì¹­: "ì„¤ì • íŒŒê´´" ë˜ëŠ” "ì„¤ì • ì¼ì¹˜"ê°€ í¬í•¨ë˜ë©´ ë§¤ì¹­
    status_match = False
    if "íŒŒê´´" in expected_status and "íŒŒê´´" in actual_status:
        status_match = True
    elif "ì¼ì¹˜" in expected_status and ("ì¼ì¹˜" in actual_status or "ì¼ê´€" in actual_status):
        status_match = True
    elif expected_status == actual_status:
        status_match = True
    
    metrics["accuracy"] = {
        "correct": status_match,
        "expected": expected_status,
        "actual": actual_status
    }
    
    return metrics


def compute_summary(results: List[Dict]) -> Dict:
    """ê²°ê³¼ ìš”ì•½ í†µê³„ ê³„ì‚°"""
    llm_metrics = ["tool_use_accuracy", "reasoning_quality", "output_completeness"]
    summary = {}
    
    # LLM ì§€í‘œ í†µê³„
    for metric in llm_metrics:
        scores = [r["metrics"][metric]["score"] for r in results if r["metrics"][metric]["score"] > 0]
        if scores:
            summary[metric] = {
                "mean": round(sum(scores) / len(scores), 2),
                "min": min(scores),
                "max": max(scores),
                "count": len(scores)
            }
        else:
            summary[metric] = {"mean": 0, "min": 0, "max": 0, "count": 0}
    
    # ì •í™•ë„ í†µê³„
    accuracy_results = [r["metrics"]["accuracy"] for r in results]
    correct_count = sum(1 for a in accuracy_results if a["correct"])
    total = len(accuracy_results)
    summary["accuracy"] = {
        "correct": correct_count,
        "total": total,
        "rate": round(correct_count / total, 4) if total > 0 else 0
    }
    
    # ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•ë³„ í†µê³„
    type_stats = {}
    for r in results:
        stype = r.get("scenario_type", "unknown")
        if stype not in type_stats:
            type_stats[stype] = {"correct": 0, "total": 0, "scores": {m: [] for m in llm_metrics}}
        type_stats[stype]["total"] += 1
        if r["metrics"]["accuracy"]["correct"]:
            type_stats[stype]["correct"] += 1
        for m in llm_metrics:
            score = r["metrics"][m]["score"]
            if score > 0:
                type_stats[stype]["scores"][m].append(score)
    
    for stype in type_stats:
        total = type_stats[stype]["total"]
        correct = type_stats[stype]["correct"]
        type_stats[stype]["accuracy_rate"] = round(correct / total, 4) if total > 0 else 0
        for m in llm_metrics:
            scores = type_stats[stype]["scores"][m]
            type_stats[stype][f"{m}_mean"] = round(sum(scores) / len(scores), 2) if scores else 0
        del type_stats[stype]["scores"]  # ì›ë³¸ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ ì œê±°
    
    summary["by_scenario_type"] = type_stats
    
    # ì†Œì„¤ë³„ í†µê³„
    novel_stats = {}
    for r in results:
        novel = r.get("novel_filename", "unknown")
        if novel not in novel_stats:
            novel_stats[novel] = {"correct": 0, "total": 0}
        novel_stats[novel]["total"] += 1
        if r["metrics"]["accuracy"]["correct"]:
            novel_stats[novel]["correct"] += 1
    
    for novel in novel_stats:
        total = novel_stats[novel]["total"]
        correct = novel_stats[novel]["correct"]
        novel_stats[novel]["accuracy_rate"] = round(correct / total, 4) if total > 0 else 0
    
    summary["by_novel"] = novel_stats
    
    return summary


def main():
    parser = argparse.ArgumentParser(description="Agent ì§€í‘œ í‰ê°€ (LLM-as-a-Judge)")
    parser.add_argument("--dataset", type=str, default="eval_dataset.json", help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--max-samples", type=int, default=None, help="ìµœëŒ€ í‰ê°€ ìƒ˜í”Œ ìˆ˜")
    parser.add_argument("--output", type=str, default=OUTPUT_FILE, help="ê²°ê³¼ ì¶œë ¥ ê²½ë¡œ")
    args = parser.parse_args()
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    if not os.path.exists(args.dataset):
        print(f"âŒ ë°ì´í„°ì…‹ íŒŒì¼ ì—†ìŒ: {args.dataset}")
        print("   ë¨¼ì € python scripts/generate_eval_dataset.py ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    with open(args.dataset, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    agent_eval = dataset.get("agent_eval", [])
    if args.max_samples:
        agent_eval = agent_eval[:args.max_samples]
    
    print(f"ğŸ¤– Agent ì§€í‘œ í‰ê°€ (LLM-as-a-Judge)")
    print(f"   ë°ì´í„°ì…‹: {args.dataset}")
    print(f"   í‰ê°€ ìƒ˜í”Œ: {len(agent_eval)}ê°œ")
    print(f"   ì¶œë ¥: {args.output}")
    print()
    
    client = get_gemini_client()
    results = []
    
    for i, scenario in enumerate(tqdm(agent_eval, desc="Agent í‰ê°€ ì¤‘")):
        input_text = scenario["input_text"]
        print(f"\n[{i+1}/{len(agent_eval)}] ì‹œë‚˜ë¦¬ì˜¤: {input_text[:50]}...")
        print(f"  ìœ í˜•: {scenario['scenario_type']}, ì˜ˆìƒ: {scenario['expected_status']}")
        
        # 1. Agent ì‹¤í–‰
        agent_result = run_agent_check(input_text, scenario.get("novel_filename", ""))
        print(f"  â†’ Agent íŒë‹¨: {agent_result['status']}")
        
        # 2. LLM-as-a-Judge ì±„ì 
        metrics = evaluate_single_scenario(client, scenario, agent_result)
        
        result_entry = {
            "input_text": input_text,
            "novel_filename": scenario.get("novel_filename", ""),
            "scenario_type": scenario["scenario_type"],
            "expected_status": scenario["expected_status"],
            "actual_status": agent_result["status"],
            "agent_results_count": len(agent_result["results"]),
            "context_preview": agent_result["context_used"][:300],
            "metrics": metrics
        }
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        llm_scores = {k: v["score"] for k, v in metrics.items() if k != "accuracy"}
        print(f"  â†’ LLM ì ìˆ˜: {llm_scores}")
        print(f"  â†’ ì •í™•ë„: {'âœ…' if metrics['accuracy']['correct'] else 'âŒ'}")
        
        results.append(result_entry)
        
        # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
        time.sleep(0.5)
    
    # ìš”ì•½ í†µê³„ ê³„ì‚°
    summary = compute_summary(results)
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    output_data = {
        "metadata": {
            "evaluated_at": datetime.now().isoformat(),
            "total_samples": len(results),
            "dataset_source": args.dataset
        },
        "summary": summary,
        "details": results
    }
    
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ¤– Agent í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    
    for metric_name in ["tool_use_accuracy", "reasoning_quality", "output_completeness"]:
        stats = summary.get(metric_name, {})
        mean = stats.get("mean", 0)
        bar = "â–ˆ" * int(mean) + "â–‘" * (5 - int(mean))
        print(f"  {metric_name:25s}: {bar} {mean}/5.0")
    
    acc = summary.get("accuracy", {})
    acc_rate = acc.get("rate", 0) * 100
    print(f"  {'accuracy':25s}: {acc.get('correct', 0)}/{acc.get('total', 0)} ({acc_rate:.1f}%)")
    
    # ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•ë³„
    print(f"\n  ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•ë³„:")
    for stype, stats in summary.get("by_scenario_type", {}).items():
        print(f"    {stype}: ì •í™•ë„ {stats['accuracy_rate']*100:.0f}%, "
              f"ì¶”ë¡  {stats.get('reasoning_quality_mean', 0)}/5")
    
    print(f"\nâœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: {args.output}")


if __name__ == "__main__":
    main()
