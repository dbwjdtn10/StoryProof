import os
import sys
import json
import logging
from pathlib import Path
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(project_root)

from backend.services.analysis.gemini_structurer import GeminiStructurer
from backend.services.analysis.embedding_engine import EmbeddingSearchEngine
from backend.core.config import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def process_corpus(input_dir: str, output_file: str):
    """
    novel_corpus_kr í´ë”ì˜ ì†Œì„¤ë“¤ì„ ì½ì–´ ìƒˆë¡œìš´ ì²­í‚¹/ì„ë² ë”© ë°©ì‹ìœ¼ë¡œ ë³€í™˜ ë° ì €ì¥
    """
    input_path = Path(input_dir)
    if not input_path.exists():
        logger.error(f"ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        return

    # ì¶œë ¥ í´ë” ìƒì„±
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    # ì—”ì§„ ì´ˆê¸°í™”
    structurer = GeminiStructurer()
    search_engine = EmbeddingSearchEngine()

    processed_novels = []
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    txt_files = list(input_path.glob("*.txt"))
    logger.info(f"ğŸ“š ì´ {len(txt_files)}ê°œì˜ ì†Œì„¤ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    for txt_file in txt_files:
        logger.info(f"ğŸ“– ì²˜ë¦¬ ì¤‘: {txt_file.name}")
        
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # 1. LLM ì”¬ ë¶„í•  (Parent Chunks)
            logger.info(f"  âœ‚ï¸ AI ì”¬ ë¶„í•  ì‹œì‘...")
            scenes = structurer.split_scenes(content)
            logger.info(f"  âœ… {len(scenes)}ê°œ ì”¬ ë¶„í•  ì™„ë£Œ")
            
            novel_chunks = []
            
            # 2. ê° ì”¬ë³„ Child Chunk ìƒì„± ë° ì„ë² ë”©
            for i, scene_text in enumerate(tqdm(scenes, desc=f"  Embedding {txt_file.name}")):
                # Child Chunks ìƒì„± (200/50 ë°©ì‹)
                child_chunks = search_engine._split_into_child_chunks(scene_text)
                
                for j, chunk_text in enumerate(child_chunks):
                    # ì„ë² ë”© ìƒì„± (384ì°¨ì› e5-small)
                    embedding = search_engine.embed_text(chunk_text)
                    
                    novel_chunks.append({
                        "chunk_id": f"{i}_{j}",
                        "scene_index": i,
                        "text": chunk_text,
                        "embedding": embedding,
                        "metadata": {
                            "novel": txt_file.name,
                            "scene_idx": i,
                            "chunk_idx": j
                        }
                    })
            
            processed_novels.append({
                "filename": txt_file.name,
                "chunks": novel_chunks
            })
            
        except Exception as e:
            logger.error(f"âŒ {txt_file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    # 3. ê²°ê³¼ ì €ì¥
    output_data = {
        "model": settings.MULTILINGUAL_EMBEDDING_MODEL,
        "dimension": 384,
        "novels": {n["filename"]: {"chunks": n["chunks"]} for n in processed_novels}
    }
    
    # Legacy í˜•ì‹ í˜¸í™˜ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë„ ìƒì„±
    legacy_format = processed_novels

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(legacy_format, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ‰ ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê²°ê³¼ ì €ì¥: {output_file}")

if __name__ == "__main__":
    # ì‹¤í–‰ ê²½ë¡œ ì„¤ì •
    CORPUS_DIR = os.path.join(project_root, "novel_corpus_kr")
    OUTPUT_FILE = os.path.join(project_root, "processed_results_new", "novel_embeddings.json")
    
    process_corpus(CORPUS_DIR, OUTPUT_FILE)
