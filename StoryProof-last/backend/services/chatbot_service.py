"""
ì±—ë´‡ ì„œë¹„ìŠ¤ ëª¨ë“ˆ
==============
RAG (Retrieval-Augmented Generation) ê¸°ë°˜ ì†Œì„¤ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (Pinecone + BGE-M3 ì„ë² ë”©)
2. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„± (Google Gemini)
3. ì†Œì„¤ë³„ í•„í„°ë§ ì§€ì›

ë™ì‘ íë¦„:
ì‚¬ìš©ì ì§ˆë¬¸ â†’ ì„ë² ë”© ë³€í™˜ â†’ Pinecone ê²€ìƒ‰ â†’ ìœ ì‚¬ ì”¬ ì¶”ì¶œ â†’ Gemini ë‹µë³€ ìƒì„±
"""

from typing import Dict, List, Optional

from google import genai
from backend.core.config import settings
from backend.db.session import SessionLocal
from backend.db.models import Novel
from backend.services.analysis import EmbeddingSearchEngine


class ChatbotService:
    """
    RAG ê¸°ë°˜ ì±—ë´‡ ì„œë¹„ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì†Œì„¤ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ Pineconeì—ì„œ ìœ ì‚¬í•œ ì”¬ì„ ê²€ìƒ‰í•˜ê³ ,
    ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ Gemini LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Attributes:
        engine (EmbeddingSearchEngine): Pinecone ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ ì—”ì§„
        client (genai.Client): Google Gemini API í´ë¼ì´ì–¸íŠ¸
        DEFAULT_ALPHA (float): ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ë ˆê±°ì‹œ, í˜„ì¬ ë¯¸ì‚¬ìš©)
        DEFAULT_SIMILARITY_THRESHOLD (float): ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
    """
    
    # ê¸°ë³¸ ì„¤ì •ê°’ (í´ë˜ìŠ¤ ìƒìˆ˜)
    DEFAULT_ALPHA = 0.297  # ë ˆê±°ì‹œ íŒŒë¼ë¯¸í„° (Pineconeì—ì„œëŠ” ë¯¸ì‚¬ìš©)
    DEFAULT_SIMILARITY_THRESHOLD = 0.35  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì™„í™” (0.5 -> 0.35)

    def __init__(self):
        """
        ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        ì´ˆê¸°í™” ê³¼ì •:
        1. EmbeddingSearchEngine ë¡œë“œ (Pinecone ì—°ê²° + BGE-M3 ëª¨ë¸ ë¡œë“œ)
        2. Google Gemini API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        
        Note:
            - í™˜ê²½ ë³€ìˆ˜ GOOGLE_API_KEY, PINECONE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤
            - ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ì„œë¹„ìŠ¤ëŠ” ìƒì„±ë˜ì§€ë§Œ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤
        """
        # Step 1: ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        # EmbeddingSearchEngineì€ Pinecone ì—°ê²° ë° BGE-M3 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤
        if EmbeddingSearchEngine:
            try:
                self.engine = EmbeddingSearchEngine()
                print("âœ… ChatbotService: EmbeddingSearchEngine loaded")
            except Exception as e:
                print(f"âŒ ChatbotService: Failed to load EmbeddingSearchEngine: {e}")
                self.engine = None
        else:
            self.engine = None
        
        # Step 2: Google Gemini API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        # ë‹µë³€ ìƒì„±ì„ ìœ„í•œ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if settings.GOOGLE_API_KEY:
            self.client = genai.Client(api_key=settings.GOOGLE_API_KEY)
        else:
            self.client = None
            print("Warning: GOOGLE_API_KEY not set. LLM functionality will be disabled.")
    
    def find_similar_chunks(
        self,
        question: str,
        top_k: int = 5,
        alpha: float = DEFAULT_ALPHA,
        similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD,
        novel_id: Optional[int] = None,
        chapter_id: Optional[int] = None,
        novel_filter: Optional[str] = None
    ) -> List[Dict]:
        """
        ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì”¬(ì²­í¬)ì„ Pineconeì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        # ê²€ìƒ‰ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if not self.engine:
            return []
            
        # Step 1: novel_filterë¡œ ì†Œì„¤ ID ì¡°íšŒ (novel_idê°€ ì§ì ‘ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)
        if novel_id is None and novel_filter:
            db = SessionLocal()
            try:
                # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±° (ì˜ˆ: "alice.txt" â†’ "alice")
                search_term = novel_filter.replace('.txt', '')
                
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œëª©ìœ¼ë¡œ ì†Œì„¤ ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                novel = db.query(Novel).filter(Novel.title.ilike(f"%{search_term}%")).first()
                if novel:
                    novel_id = novel.id
                    print(f"ğŸ” Chatbot: Resolved novel_filter '{novel_filter}' to ID {novel_id} ({novel.title})")
                else:
                    print(f"âš ï¸ Chatbot: novel_filter '{novel_filter}' not found in DB")
            finally:
                db.close()
        elif novel_id:
            print(f"ğŸ” Chatbot: Using direct novel_id {novel_id}")
        
        # Step 2: Pinecone ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        try:
            results = self.engine.search(query=question, novel_id=novel_id, chapter_id=chapter_id, top_k=top_k)
            print(f"ğŸ” Chatbot: Found {len(results)} results (Novel: {novel_id}, Chapter Context: {chapter_id})")
            
            # Step 3: ê²°ê³¼ í¬ë§· ë³€í™˜ ë° í•„í„°ë§
            formatted_results = []
            for res in results:
                similarity = res['similarity']
                doc = res['document']
                scene_idx = doc.get('scene_index', '?')
                
                # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ì œì™¸
                if similarity < similarity_threshold:
                    print(f"  - [DROP] Scene {scene_idx}: similarity {similarity:.4f} < {similarity_threshold}")
                    continue
                
                print(f"  - [KEEP] Scene {scene_idx}: similarity {similarity:.4f}")
                    
                formatted_results.append({
                    'text': doc.get('original_text', ''),
                    'scene_index': doc.get('scene_index'),
                    'chapter_id': res.get('chapter_id'),
                    'summary': doc.get('summary'),
                    'novel_id': novel_id,
                    'similarity': similarity,
                    'original_similarity': similarity
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"Error during search: {e}")
            return []
    
    def generate_answer(self, question: str, context: str) -> str:
        """
        Google Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        RAG (Retrieval-Augmented Generation) ë°©ì‹:
        1. ê²€ìƒ‰ëœ ì”¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
        2. Geminiê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ ìƒì„±
        3. ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ LLMì˜ ì§€ì‹ í™œìš©
        
        í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:
        - ì—­í•  ì •ì˜: ì†Œì„¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸
        - ë‹µë³€ í˜•ì‹: [í•µì‹¬ ìš”ì•½] + [ìƒì„¸ ì„¤ëª…] 2ë‹¨ êµ¬ì¡°
        - ì»¨í…ìŠ¤íŠ¸: ê²€ìƒ‰ëœ ì”¬ë“¤ (ìµœëŒ€ 3,500ì)
        
        Args:
            question (str): ì‚¬ìš©ì ì§ˆë¬¸
            context (str): ê²€ìƒ‰ëœ ì”¬ë“¤ì˜ í…ìŠ¤íŠ¸ (ì—¬ëŸ¬ ì”¬ì´ ê²°í•©ëœ í˜•íƒœ)
            
        Returns:
            str: ìƒì„±ëœ ë‹µë³€ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
                 í˜•ì‹: [í•µì‹¬ ìš”ì•½]\n...\n\n[ìƒì„¸ ì„¤ëª…]\n...
                 
        Example:
            >>> service = ChatbotService()
            >>> context = "ì•¨ë¦¬ìŠ¤ëŠ” í† ë¼ë¥¼ ë”°ë¼ êµ¬ë©ìœ¼ë¡œ ë–¨ì–´ì¡Œë‹¤..."
            >>> answer = service.generate_answer(
            ...     question="ì•¨ë¦¬ìŠ¤ëŠ” ì–´ë””ë¡œ ë–¨ì–´ì¡Œë‚˜ìš”?",
            ...     context=context
            ... )
            >>> print(answer)
        """
        # Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not self.client:
            return "LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        # - ì»¨í…ìŠ¤íŠ¸ëŠ” 3,500ìë¡œ ì œí•œ (Gemini í† í° ì œí•œ ê³ ë ¤)
        # - ë‹µë³€ í˜•ì‹ì„ ëª…í™•íˆ ì§€ì •í•˜ì—¬ ì¼ê´€ëœ ì¶œë ¥ ìœ ë„
        prompt = f"""ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ë§¥ì€ ì†Œì„¤ì˜ ì—¬ëŸ¬ ë¶€ë¶„ì—ì„œ ë°œì·Œëœ ë‚´ìš©ì…ë‹ˆë‹¤. ë¬¸ë§¥ì— ì •ë‹µì´ ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë‹¤ë©´, ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì†Œì„¤ì˜ ì§€ì‹ì„ ë™ì›í•˜ì—¬ êµ¬ì²´ì ì´ê³  í’ë¶€í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ë‹µë³€ í˜•ì‹]
ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”. ë‘ ì„¹ì…˜ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ì„ ë‘ì„¸ìš”.

[í•µì‹¬ ìš”ì•½]
(ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)

[ìƒì„¸ ì„¤ëª…]
(ì°¾ì€ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ê·¼ê±°)

ë¬¸ë§¥:
{context[:3500]}
        
ì§ˆë¬¸: {question}
        
ë‹µë³€:"""
        
        # Gemini API í˜¸ì¶œ
        try:
            response = self.client.models.generate_content(
                model=settings.GEMINI_CHAT_MODEL,  # ì˜ˆ: "gemini-2.5-flash"
                contents=prompt
            )
            return response.text
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def augment_query(self, question: str) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ í™•ì¥í•©ë‹ˆë‹¤.
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ í‚¤ì›Œë“œ, ë™ì˜ì–´, êµ¬ì²´ì ì¸ í‘œí˜„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        if not self.client:
            return question

        prompt = f"""Role: ì „ë¬¸ ê²€ìƒ‰ ì¦ê°• ì–´ì‹œìŠ¤í„´íŠ¸
Task: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬, ì†Œì„¤ ë‚´ìš© ê²€ìƒ‰ì— ë„ì›€ì´ ë  'ê²€ìƒ‰ í‚¤ì›Œë“œ'ì™€ 'í™•ì¥ ì¿¼ë¦¬'ë¥¼ ì œì•ˆí•˜ì„¸ìš”.
Goal: ì‚¬ìš©ìê°€ ëª¨í˜¸í•˜ê²Œ ì§ˆë¬¸í•˜ë”ë¼ë„, ì •í™•í•œ ì”¬ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì¸ ë‹¨ì–´ë“¤ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”.

User Question: "{question}"

Rules:
1. Return ONLY the augmented search query string. No explanations.
2. Include original entities (names, places) exactly.
3. Add synonyms or related context keywords.
4. Format: "Original Question keyword1 keyword2 related_context"

Example:
Q: "ì•¨ë¦¬ìŠ¤ê°€ ë–¨ì–´ì§„ ê³³"
A: "ì•¨ë¦¬ìŠ¤ê°€ ë–¨ì–´ì§„ ê³³ í† ë¼ êµ´ ë‚™í•˜ ì´ìƒí•œ ë‚˜ë¼ ê¹Šì€ êµ¬ë©"

Output:"""
        try:
            response = self.client.models.generate_content(
                model=settings.GEMINI_CHAT_MODEL,
                contents=prompt
            )
            augmented = response.text.strip()
            print(f"ğŸ§¬ Query Augmented: '{question}' -> '{augmented}'")
            return augmented
        except Exception as e:
            print(f"âš ï¸ Query Augmentation Failed: {e}")
            return question

    def ask(
        self,
        question: str,
        alpha: float = DEFAULT_ALPHA,
        similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD,
        novel_id: Optional[int] = None,
        chapter_id: Optional[int] = None,
        novel_filter: Optional[str] = None
    ) -> Dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        """
        # 1. 1ì°¨ ê²€ìƒ‰ (ì›ë³¸ ì¿¼ë¦¬)
        top_chunks = self.find_similar_chunks(
            question=question,
            top_k=5,
            alpha=alpha,
            similarity_threshold=similarity_threshold,
            novel_id=novel_id,
            chapter_id=chapter_id,
            novel_filter=novel_filter
        )
        
        # 2. ê²°ê³¼ê°€ ì—†ìœ¼ë©´ 2ì°¨ ê²€ìƒ‰ (ì¿¼ë¦¬ í™•ì¥)
        if not top_chunks:
            print("ğŸ•µï¸ 1ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨. ì¿¼ë¦¬ í™•ì¥ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            augmented_query = self.augment_query(question)
            
            # í™•ì¥ì´ ì‹¤ì œë¡œ ì¼ì–´ë‚¬ì„ ë•Œë§Œ ì¬ê²€ìƒ‰
            if augmented_query != question:
                top_chunks = self.find_similar_chunks(
                    question=augmented_query,
                    top_k=5,
                    alpha=alpha,
                    similarity_threshold=similarity_threshold, # ë™ì¼ ì„ê³„ê°’ ì‚¬ìš© (ë˜ëŠ” ì•½ê°„ ë‚®ì¶œ ìˆ˜ ìˆìŒ)
                    novel_id=novel_id,
                    chapter_id=chapter_id,
                    novel_filter=novel_filter
                )

        # 3. ì—¬ì „íˆ ìœ ì‚¬í•œ ìŠ¤í† ë¦¬ë³´ë“œê°€ ì—†ëŠ” ê²½ìš°
        if not top_chunks:
            error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            if not self.engine:
                error_msg += " (ê²€ìƒ‰ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"
            
            return {
                "answer": error_msg,
                "source": None,
                "similarity": 0.0,
                "found_context": False
            }
        
        # 4. ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ìƒìœ„ ì²­í¬ í…ìŠ¤íŠ¸ ê²°í•©)
        context_texts = []
        for i, chunk in enumerate(top_chunks):
            # ì”¬ ë²ˆí˜¸ë‚˜ ìš”ì•½ì´ ìˆìœ¼ë©´ í¬í•¨
            header = f"[Context {i+1}]"
            if chunk.get('scene_index') is not None:
                header += f" Scene {chunk['scene_index']}"
            if chunk.get('summary'):
                header += f" (Summary: {chunk['summary']})"
                
            context_texts.append(f"{header}\n{chunk['text']}")
        
        context = "\n\n".join(context_texts)
        
        # 4. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer = self.generate_answer(question, context)
        
        # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì •ë³´
        best_chunk = top_chunks[0]
        
        # novel title ê°€ì ¸ì˜¤ê¸°
        novel_title = "Unknown Novel"
        if best_chunk.get('novel_id'):
            db = SessionLocal()
            try:
                novel = db.query(Novel).filter(Novel.id == best_chunk['novel_id']).first()
                if novel:
                    novel_title = novel.title
            finally:
                db.close()

        return {
            "answer": answer,
            "source": {
                "filename": novel_title,
                "chapter_id": best_chunk.get('chapter_id'),
                "scene_index": best_chunk.get('scene_index'),
                "summary": best_chunk.get('summary'),
                "total_scenes": len(top_chunks)
            },
            "similarity": best_chunk.get('similarity', 0.0), # similarity might be missing in some cases if not careful
            "found_context": True
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_chatbot_service = None


def get_chatbot_service() -> ChatbotService:
    """
    ì±—ë´‡ ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Returns:
        ChatbotService: ì±—ë´‡ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    """
    global _chatbot_service
    if _chatbot_service is None:
        _chatbot_service = ChatbotService()
    return _chatbot_service

